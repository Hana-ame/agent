#!/usr/bin/env python3
"""
æ‰“å­—æœºæ•ˆæœçš„HTTPå®¢æˆ·ç«¯ - æ”¯æŒOpenAIæ ¼å¼JSONé…ç½®ã€æ–‡ä»¶ä¸Šä¸‹æ–‡ã€å†å²å¯¹è¯å’ŒTeeè¾“å‡º
"""

import json
import time
import sys
import io
import os
import re
import argparse
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
import requests
from dotenv import load_dotenv

THREE_DOTS = "`" * 3

# ============ UTF-8 ç¼–ç è®¾ç½® ============


def setup_utf8():
    """è®¾ç½®UTF-8ç¼–ç ç¯å¢ƒ"""
    if sys.platform == "win32":
        import ctypes

        try:
            kernel32 = ctypes.windll.kernel32
            kernel32.SetConsoleCP(65001)
            kernel32.SetConsoleOutputCP(65001)
        except:
            pass

    sys.stdout = io.TextIOWrapper(
        sys.stdout.buffer, encoding="utf-8", errors="replace", line_buffering=True
    )


setup_utf8()

# ============ é…ç½®åŠ è½½ ============


def load_config(args):
    """ä».envå’ŒprofileåŠ è½½é…ç½®"""
    # 1. åŠ è½½åŸºç¡€ .env é…ç½®
    env_paths = [".env", "../.env", os.path.expanduser("~/.ai_chat.env")]
    base_config = {
        "endpoint": "",
        "api_key": "",
        "model": "Pro/moonshotai/Kimi-K2.5",
    }
    
    for env_path in env_paths:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            print(f"âœ“ å·²åŠ è½½ç¯å¢ƒé…ç½®: {env_path}", file=sys.stderr)
            break
    
    base_config.update({
        "endpoint": os.getenv("ENDPOINT", ""),
        "api_key": os.getenv("API_KEY", ""),
        "model": os.getenv("MODEL", base_config["model"]),
    })

    # 2. åŠ è½½ Profile é…ç½®ï¼ˆä» ~/.ai_chat_profiles.jsonï¼‰
    profile_config = load_profile(args.profile)
    
    # 3. åˆå¹¶ï¼šProfile > .env > é»˜è®¤
    final_config = {**base_config, **profile_config}
    
    return final_config


def load_profile(profile_name: str) -> Dict:
    """åŠ è½½æŒ‡å®š profile çš„é…ç½®"""
    profile_paths = [
        Path("profiles.json"),
        Path("../profiles.json"),
        Path.home() / ".ai_chat_profiles.json",
    ]
    
    for path in profile_paths:
        if path.exists():
            try:
                with open(path, "r", encoding="utf-8") as f:
                    profiles = json.load(f)
                
                if profile_name in profiles:
                    print(f"âœ“ å·²åŠ è½½é¢„è®¾: {profile_name} ({path})", file=sys.stderr)
                    return profiles[profile_name]
                elif profile_name != "default":
                    print(f"âš ï¸  æœªæ‰¾åˆ°é¢„è®¾ '{profile_name}'ï¼Œä½¿ç”¨é»˜è®¤é…ç½®", file=sys.stderr)
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ profile å¤±è´¥: {e}", file=sys.stderr)
    
    return {}


# ============ OpenAIæ ¼å¼é…ç½®æ„å»º ============


def build_request_body(args, config: Dict, messages: List[Dict]) -> Dict[str, Any]:
    """
    æ„å»ºOpenAIæ ¼å¼çš„è¯·æ±‚ä½“
    ä¼˜å…ˆçº§: å‘½ä»¤è¡Œå‚æ•° > JSONé…ç½®æ–‡ä»¶ > .env > é»˜è®¤å€¼
    """
    # 1. ä»JSONæ–‡ä»¶åŠ è½½åŸºç¡€é…ç½®ï¼ˆå¦‚æœæä¾›ï¼‰
    request_body = {}
    if args.config:
        try:
            config_path = Path(args.config).expanduser().resolve()
            with open(config_path, "r", encoding="utf-8") as f:
                request_body = json.load(f)
            print(f"âœ“ å·²åŠ è½½JSONé…ç½®: {config_path}", file=sys.stderr)
        except Exception as e:
            print(f"âš ï¸  åŠ è½½JSONé…ç½®å¤±è´¥: {e}", file=sys.stderr)
            raise

    # 2. å¤„ç†messagesï¼ˆåˆå¹¶å†å²+ç”¨æˆ·è¾“å…¥ï¼‰
    final_messages = request_body.get("messages", []).copy()

    # åŠ è½½å†å²å¯¹è¯ï¼ˆ--contextå‚æ•°ï¼Œè¿½åŠ åˆ°messagesï¼‰
    if args.context:
        try:
            with open(args.context, "r", encoding="utf-8") as f:
                history_data = json.load(f)
                if isinstance(history_data, list):
                    final_messages.extend(history_data)
                elif isinstance(history_data, dict) and "messages" in history_data:
                    final_messages.extend(history_data["messages"])
            print(f"âœ“ å·²åŠ è½½å†å²å¯¹è¯: {args.context}", file=sys.stderr)
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å†å²å¯¹è¯å¤±è´¥: {e}", file=sys.stderr)

    # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
    if messages:
        final_messages.extend(messages)

    if final_messages:
        request_body["messages"] = final_messages

    # 3. å‘½ä»¤è¡Œå‚æ•°è¦†ç›–JSONé…ç½®ï¼ˆOpenAIæ ‡å‡†å‚æ•°ï¼‰
    if args.model:
        request_body["model"] = args.model
    elif "model" not in request_body:
        request_body["model"] = config["model"]

    if args.temperature is not None:
        request_body["temperature"] = args.temperature
    elif "temperature" not in request_body:
        request_body["temperature"] = 0.7

    if args.max_tokens is not None:
        request_body["max_tokens"] = args.max_tokens
    elif "max_tokens" not in request_body:
        request_body["max_tokens"] = 8192

    # æµå¼è¾“å‡ºè®¾ç½®
    if args.no_stream:
        request_body["stream"] = False
    elif "stream" not in request_body:
        request_body["stream"] = True

    # ç‰¹å®šAPIæ‰©å±•å‚æ•°ï¼ˆå¦‚enable_thinkingï¼‰
    if hasattr(args, "enable_thinking") and args.enable_thinking is not None:
        request_body["enable_thinking"] = args.enable_thinking
    elif "enable_thinking" not in request_body:
        # request_body["enable_thinking"] = True
        pass # deepseek does not support this flag?

    # å…¶ä»–OpenAIæ ‡å‡†å‚æ•°ï¼ˆå¦‚æœJSONä¸­æœ‰ï¼Œä¿ç•™ï¼‰
    # top_p, presence_penalty, frequency_penalty, stop, seed ç­‰

    return request_body


# ============ å‚æ•°è§£æ ============


def parse_arguments():
    """è§£æå‚æ•°ï¼Œæ”¯æŒ@æ–‡ä»¶åè¯­æ³•å’ŒOpenAIæ ¼å¼JSONé…ç½®"""
    parser = argparse.ArgumentParser(
        description="AI Chat CLI - æ”¯æŒOpenAIæ ¼å¼JSONé…ç½®ã€æ–‡ä»¶ä¸Šä¸‹æ–‡å’ŒTeeè¾“å‡º",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨JSONé…ç½®ï¼ˆOpenAIæ ¼å¼ï¼‰
  python chat.py --config request.json
  
  # JSONé…ç½® + å‘½ä»¤è¡Œè¦†ç›–
  python chat.py --config request.json --model "gpt-4" --temperature 0.5
  
  # ä¼ ç»Ÿç”¨æ³•ï¼šç›´æ¥è¾“å…¥æç¤º
  python chat.py "ä½ å¥½"
  python chat.py @document.txt "æ€»ç»“è¿™ä¸ªæ–‡ä»¶"
  
  # å®Œæ•´ç¤ºä¾‹
  python chat.py --config base.json --history chat.json @code.py "è§£é‡Šä»£ç " -o result.md
  
OpenAI JSONæ ¼å¼ç¤ºä¾‹:
  {
    "model": "gpt-4",
    "temperature": 0.7,
    "max_tokens": 2000,
    "top_p": 1.0,
    "frequency_penalty": 0,
    "presence_penalty": 0,
    "stream": true,
    "enable_thinking": true,
    "messages": [
      {"role": "system", "content": "You are a helpful assistant"}
    ]
  }
        """,
    )
    
    parser.add_argument("--profile", "-p", default="default", 
                   help="ä½¿ç”¨é¢„è®¾é…ç½® (å¦‚: kimi, gpt4, local)")
    
    # OpenAIæ ¼å¼JSONé…ç½®
    parser.add_argument(
        "--config", "-f", default="config.json", help="OpenAIæ ¼å¼çš„JSONé…ç½®æ–‡ä»¶è·¯å¾„"
    )

    # è¾“å…¥æç¤ºï¼ˆæ”¯æŒ@æ–‡ä»¶åï¼‰
    parser.add_argument("prompt", nargs="*", help="è¾“å…¥æç¤ºï¼ˆæ”¯æŒ@æ–‡ä»¶ååŠ è½½æ–‡ä»¶å†…å®¹ï¼‰")

    # å†å²å¯¹è¯
    parser.add_argument(
        "--context", "-c", help="åŠ è½½å†å²å¯¹è¯JSONæ–‡ä»¶ï¼ˆè¿½åŠ åˆ°messagesï¼‰"
    )

    # è¾“å‡ºè®¾ç½®
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰")

    # APIé…ç½®ï¼ˆè¦†ç›–JSONå’Œ.envï¼‰
    parser.add_argument("--endpoint", "-e", help="APIç«¯ç‚¹ï¼ˆè¦†ç›–.envé…ç½®ï¼‰")
    parser.add_argument("--api-key", "-k", help="APIå¯†é’¥ï¼ˆè¦†ç›–.envé…ç½®ï¼‰")
    parser.add_argument("--model", "-m", help="æ¨¡å‹åç§°ï¼ˆè¦†ç›–JSONå’Œ.envé…ç½®ï¼‰")

    # OpenAIæ ‡å‡†å‚æ•°ï¼ˆè¦†ç›–JSONï¼‰
    parser.add_argument("--temperature", "-t", type=float, help="æ¸©åº¦å‚æ•°(0-2)")
    parser.add_argument("--max-tokens", type=int, help="æœ€å¤§tokenæ•°")
    parser.add_argument("--top-p", type=float, help="æ ¸é‡‡æ ·æ¦‚ç‡")
    parser.add_argument("--presence-penalty", type=float, help="å­˜åœ¨æƒ©ç½š")
    parser.add_argument("--frequency-penalty", type=float, help="é¢‘ç‡æƒ©ç½š")
    parser.add_argument("--seed", type=int, help="éšæœºç§å­")

    # æµå¼ä¸æ€è€ƒé€‰é¡¹
    parser.add_argument("--no-stream", action="store_true", help="ç¦ç”¨æµå¼è¾“å‡º")
    parser.add_argument(
        "--enable-thinking",
        action="store_true",
        default=None,
        help="å¯ç”¨æ€è€ƒè¿‡ç¨‹ï¼ˆç‰¹å®šAPIæ”¯æŒï¼‰",
    )
    parser.add_argument(
        "--no-thinking",
        dest="enable_thinking",
        action="store_false",
        help="ç¦ç”¨æ€è€ƒè¿‡ç¨‹",
    )

    return parser.parse_args()


# ============ æ–‡ä»¶ä¸Šä¸‹æ–‡å¤„ç† ============


def load_file_content(filepath: str):
    """åŠ è½½æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„"""
    try:
        if filepath.startswith("@"):
            filepath = filepath[1:]

        path = Path(filepath).expanduser().resolve()
        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")

        suffix = path.suffix.lower()

        # å›¾ç‰‡æ–‡ä»¶å¤„ç†
        if suffix in [".png", ".jpg", ".jpeg", ".gif", ".webp", ".bmp"]:
            import base64

            with open(path, "rb") as f:
                encoded = base64.b64encode(f.read()).decode("utf-8")
            mime_type = f"image/{suffix[1:]}" if suffix != ".jpg" else "image/jpeg"
            # OpenAIå¤šæ¨¡æ€æ ¼å¼
            return {
                "type": "image_url",
                "image_url": {"url": f"data:{mime_type};base64,{encoded}"},
            }

        # æ–‡æœ¬æ–‡ä»¶
        with open(path, "r", encoding="utf-8", errors="replace") as f:
            content = f.read()

        # æ ¹æ®æ–‡ä»¶ç±»å‹æ·»åŠ ä»£ç å—æ ‡è®°
        if suffix in [
            ".py",
            ".js",
            ".ts",
            ".java",
            ".cpp",
            ".c",
            ".h",
            ".go",
            ".rs",
            ".rb",
            ".php",
        ]:
            return f"{THREE_DOTS}{suffix[1:]}\n{content}\n{THREE_DOTS}\n[æ–‡ä»¶: {path.name}]"
        elif suffix in [".md", ".txt", ".rst"]:
            return f"{content}\n\n[æ–‡ä»¶: {path.name}]"
        elif suffix in [".json", ".yaml", ".yml", ".xml"]:
            return f"{THREE_DOTS}yaml\n{content}\n{THREE_DOTS}\n[æ–‡ä»¶: {path.name}]"
        else:
            return f"{THREE_DOTS}\n{content}\n{THREE_DOTS}\n[æ–‡ä»¶: {path.name}]"

    except Exception as e:
        print(f"âš ï¸  åŠ è½½æ–‡ä»¶å¤±è´¥ {filepath}: {e}", file=sys.stderr)
        return f"[æ— æ³•åŠ è½½æ–‡ä»¶: {filepath}]"


def build_user_messages(args) -> Tuple[List[Dict], str]:
    """æ„å»ºç”¨æˆ·è¾“å…¥çš„messagesï¼ˆæ”¯æŒ@æ–‡ä»¶è¯­æ³•ï¼‰"""
    if not args.prompt:
        return [], "chat"

    content_parts = []
    has_image = False

    for part in args.prompt:
        if part.startswith("@"):
            file_content = load_file_content(part)
            if isinstance(file_content, dict):  # å›¾ç‰‡ï¼ˆOpenAIå¤šæ¨¡æ€æ ¼å¼ï¼‰
                content_parts.append(file_content)
                has_image = True
            else:
                content_parts.append(file_content)
        else:
            content_parts.append(part)

    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œä½¿ç”¨OpenAIå¤šæ¨¡æ€contentæ ¼å¼ï¼ˆæ•°ç»„ï¼‰
    if has_image:
        # å°†çº¯æ–‡æœ¬éƒ¨åˆ†åˆå¹¶ï¼Œä¿æŒé¡ºåº
        multimodal_content = []
        current_text = []

        for part in content_parts:
            if isinstance(part, dict):  # å›¾ç‰‡
                if current_text:
                    multimodal_content.append(
                        {"type": "text", "text": "\n".join(current_text)}
                    )
                    current_text = []
                multimodal_content.append(part)
            else:
                current_text.append(part)

        if current_text:
            multimodal_content.append(
                {"type": "text", "text": "\n\n".join(current_text)}
            )

        return [{"role": "user", "content": multimodal_content}], args.prompt[0]
    else:
        # çº¯æ–‡æœ¬ï¼Œä¼ ç»Ÿæ ¼å¼
        user_content = "\n\n".join(content_parts)
        return [{"role": "user", "content": user_content}], args.prompt[0]


# ============ æ ¸å¿ƒï¼šå®‰å…¨çš„æ‰“å­—æœºæ‰“å° ============


class SafePrinter:
    """å®‰å…¨çš„æ‰“å­—æœºæ‰“å°å™¨ï¼Œæ­£ç¡®å¤„ç†ä¸­æ–‡ï¼ŒåŒæ—¶æ”¯æŒTeeè¾“å‡º"""

    def __init__(self, tee_file: Optional[io.TextIOWrapper] = None):
        self.tee_file = tee_file
        self.reasoning_printed_chars = 0
        self.content_printed_chars = 0
        self.in_reasoning_phase = True
        self.full_reasoning = ""
        self.full_content = ""

    def write_to_file(self, text: str, is_reasoning: bool = False):
        """å†™å…¥åˆ°æ–‡ä»¶ï¼ˆä¸æ‰“å°ï¼‰"""
        if self.tee_file:
            try:
                self.tee_file.write(text)
                self.tee_file.flush()
            except Exception as e:
                print(f"\n[æ–‡ä»¶å†™å…¥é”™è¯¯: {e}]", file=sys.stderr)

    def print_reasoning(self, full_reasoning: str, delay: float = 0.01) -> None:
        """æ‰“å°thinkingè¿‡ç¨‹ï¼Œåªæ‰“å°æ–°å¢éƒ¨åˆ†"""
        if not full_reasoning:
            return

        if isinstance(full_reasoning, bytes):
            full_reasoning = full_reasoning.decode("utf-8", errors="replace")

        self.full_reasoning = full_reasoning
        new_part = full_reasoning[self.reasoning_printed_chars :]

        for char in new_part:
            print(char, end="", flush=True)
            if self.tee_file:
                self.write_to_file(char, is_reasoning=True)
            time.sleep(delay)

        self.reasoning_printed_chars = len(full_reasoning)

    def print_content(self, full_content: str, delay: float = 0.03) -> None:
        """æ‰“å°æ­£å¼å›å¤ï¼Œåªæ‰“å°æ–°å¢éƒ¨åˆ†"""
        if not full_content:
            return

        if isinstance(full_content, bytes):
            full_content = full_content.decode("utf-8", errors="replace")

        self.full_content = full_content
        new_part = full_content[self.content_printed_chars :]

        for char in new_part:
            print(char, end="", flush=True)
            if self.tee_file:
                self.write_to_file(char, is_reasoning=False)
            time.sleep(delay)

        self.content_printed_chars = len(full_content)

    def switch_to_content(self) -> None:
        """ä»thinkingåˆ‡æ¢åˆ°contenté˜¶æ®µ"""
        if self.in_reasoning_phase:
            print()
            print("\n" + "-" * 50)
            print("âœ¨ æ­£å¼å›å¤ï¼š")
            print("-" * 50)
            if self.tee_file:
                self.write_to_file("\n\n---\nâœ¨ æ­£å¼å›å¤ï¼š\n---\n\n")
            self.in_reasoning_phase = False

    def finalize(self):
        """å®Œæˆè¾“å‡ºï¼Œç¡®ä¿æ–‡ä»¶å†™å…¥"""
        if self.tee_file:
            self.tee_file.flush()


# ============ HTTPå®¢æˆ·ç«¯ ============


class TypewriterHTTPClient:
    """æ‰“å­—æœºæ•ˆæœçš„HTTPå®¢æˆ·ç«¯"""

    def __init__(self, endpoint: str, api_key: str):
        self.endpoint = endpoint
        self.api_key = api_key
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream",
        }

    def stream_request(
        self, request_body: Dict[str, Any], printer: SafePrinter
    ) -> Tuple[str, str, Dict]:
        """
        æµå¼è¯·æ±‚ï¼Œæ‰“å­—æœºæ•ˆæœæ˜¾ç¤ºthinkingå’Œcontent
        request_body: OpenAIæ ¼å¼çš„å®Œæ•´è¯·æ±‚ä½“
        è¿”å›: (reasoning, content, metadata)
        """
        print("=" * 70)
        print("æ­£åœ¨å‘é€è¯·æ±‚...")
        print(f"Endpoint: {self.endpoint}")
        print(f"Model: {request_body.get('model', 'unknown')}")
        print(f"Stream: {request_body.get('stream', True)}")
        print(f"Messages: {len(request_body.get('messages', []))} è½®å¯¹è¯")

        # æ˜¾ç¤ºå…¶ä»–OpenAIå‚æ•°
        params = {
            k: v
            for k, v in request_body.items()
            if k not in ["messages", "model", "stream"] and v is not None
        }
        if params:
            print(f"Parameters: {json.dumps(params, ensure_ascii=False)}")

        full_reasoning = ""
        full_content = ""
        metadata = {
            "start_time": datetime.now().isoformat(),
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "finish_reason": None,
            "request_body": request_body,  # è®°å½•å®Œæ•´è¯·æ±‚
        }

        try:
            response = requests.post(
                self.endpoint,
                headers=self.headers,
                json=request_body,
                stream=request_body.get("stream", True),
                timeout=120000,
            )
            response.raise_for_status()

            print(f"\nè¿æ¥æˆåŠŸ (Status: {response.status_code})")
            print("-" * 70)

            if not request_body.get("stream", True):
                # éæµå¼å¤„ç†
                data = response.json()
                choice = data.get("choices", [{}])[0]
                message = choice.get("message", {})
                full_content = message.get("content", "")
                full_reasoning = message.get("reasoning_content", "")

                # æ‰“å°ç»“æœ
                if full_reasoning:
                    print("\nğŸ’­ Thinking è¿‡ç¨‹ï¼š")
                    print("-" * 50)
                    print(full_reasoning)
                    printer.write_to_file(full_reasoning)

                print("\nâœ¨ æ­£å¼å›å¤ï¼š")
                print("-" * 50)
                print(full_content)
                printer.write_to_file(full_content)

                # æ›´æ–°metadata
                if "usage" in data:
                    metadata.update(
                        {
                            "prompt_tokens": data["usage"].get("prompt_tokens", 0),
                            "completion_tokens": data["usage"].get(
                                "completion_tokens", 0
                            ),
                            "total_tokens": data["usage"].get("total_tokens", 0),
                        }
                    )
                metadata["finish_reason"] = choice.get("finish_reason")

            else:
                # æµå¼å¤„ç†
                header_printed = False
                buffer_bytes = b""

                for chunk in response.iter_content(chunk_size=128):
                    if not chunk:
                        continue

                    buffer_bytes += chunk

                    while b"\n" in buffer_bytes:
                        line_bytes, buffer_bytes = buffer_bytes.split(b"\n", 1)
                        line = line_bytes.decode("utf-8", errors="replace").strip()

                        if line.startswith("data: "):
                            data_str = line[6:]

                            if data_str == "[DONE]":
                                break

                            try:
                                data = json.loads(data_str)

                                # æ›´æ–°usageä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€æ¡ï¼‰
                                if "usage" in data and data["usage"]:
                                    metadata["prompt_tokens"] = data["usage"].get(
                                        "prompt_tokens", 0
                                    )
                                    metadata["completion_tokens"] = data["usage"].get(
                                        "completion_tokens", 0
                                    )
                                    metadata["total_tokens"] = data["usage"].get(
                                        "total_tokens", 0
                                    )

                                choices = data.get("choices", [])
                                if not choices:
                                    continue

                                delta = choices[0].get("delta", {})
                                finish_reason = choices[0].get("finish_reason")
                                if finish_reason:
                                    metadata["finish_reason"] = finish_reason

                                reasoning_delta = delta.get("reasoning_content") or delta.get("reasoning") or ""
                                content_delta = delta.get("content")

                                # æ‰“å°headerï¼ˆç¬¬ä¸€æ¬¡æ”¶åˆ°æ•°æ®ï¼‰
                                if not header_printed and (
                                    reasoning_delta or content_delta
                                ):
                                    header_printed = True
                                    if reasoning_delta or request_body.get(
                                        "enable_thinking"
                                    ):
                                        print("\nğŸ’­ Thinking è¿‡ç¨‹ï¼š")
                                        print("-" * 50)
                                        if printer.tee_file:
                                            printer.write_to_file(
                                                "\nğŸ’­ Thinking è¿‡ç¨‹ï¼š\n"
                                                + "-" * 50
                                                + "\n"
                                            )

                                # ç´¯ç§¯å¹¶æ‰“å°reasoning
                                if reasoning_delta and isinstance(reasoning_delta, str):
                                    full_reasoning += reasoning_delta
                                    printer.print_reasoning(full_reasoning)

                                # åˆ‡æ¢åˆ°contenté˜¶æ®µ
                                if content_delta and printer.in_reasoning_phase:
                                    printer.switch_to_content()

                                # ç´¯ç§¯å¹¶æ‰“å°content
                                if content_delta and isinstance(content_delta, str):
                                    full_content += content_delta
                                    printer.print_content(full_content)

                            except json.JSONDecodeError:
                                pass

                # å¤„ç†å‰©ä½™buffer
                if buffer_bytes:
                    try:
                        line = buffer_bytes.decode("utf-8", errors="replace").strip()
                        if line.startswith("data: "):
                            data_str = line[6:]
                            if data_str and data_str != "[DONE]":
                                data = json.loads(data_str)
                                # å¤„ç†æœ€åçš„æ•°æ®...
                    except:
                        pass

            print("\n" + "=" * 70)
            print("âœ… è¯·æ±‚å®Œæˆ")

            if full_reasoning:
                print(f"\nğŸ“Š Thinking: {len(full_reasoning)} å­—ç¬¦")
            print(f"ğŸ“Š Content: {len(full_content)} å­—ç¬¦")
            if metadata["total_tokens"]:
                print(
                    f"ğŸ“Š Tokens: {metadata['total_tokens']} (Prompt: {metadata['prompt_tokens']}, Completion: {metadata['completion_tokens']})"
                )
            if metadata["finish_reason"]:
                print(f"ğŸ“Š Finish reason: {metadata['finish_reason']}")

            metadata["end_time"] = datetime.now().isoformat()
            metadata["reasoning_chars"] = len(full_reasoning)
            metadata["content_chars"] = len(full_content)

            return full_reasoning, full_content, metadata

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback

            traceback.print_exc()
            raise


# ============ æ–‡ä»¶ä¿å­˜ ============


def save_conversation(
    output_path: Path,
    reasoning: str,
    content: str,
    metadata: Dict,
    messages: List[Dict],
):
    """ä¿å­˜å¯¹è¯åˆ°æ–‡ä»¶ï¼ˆMarkdownæ ¼å¼ï¼‰"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    request_body = metadata.get("request_body", {})

    md_content = f"""# AI å¯¹è¯è®°å½•

**æ—¶é—´**: {timestamp}  
**æ¨¡å‹**: {request_body.get('model', metadata.get('model', 'unknown'))}  
**Token æ¶ˆè€—**: {metadata.get('total_tokens', 'N/A')} (Prompt: {metadata.get('prompt_tokens', 'N/A')}, Completion: {metadata.get('completion_tokens', 'N/A')})  
**ç»“æŸåŸå› **: {metadata.get('finish_reason', 'N/A')}

## è¯·æ±‚å‚æ•°

{THREE_DOTS}json
{json.dumps({k: v for k, v in request_body.items() if k != 'messages'}, indent=2, ensure_ascii=False)}
{THREE_DOTS}

---

## å¯¹è¯å†å²

"""

    # æ·»åŠ å¯¹è¯å†å²
    for msg in messages:
        role = msg.get("role", "unknown")
        content_text = msg.get("content", "")

        # å¤„ç†å¤šæ¨¡æ€contentï¼ˆæ•°ç»„æ ¼å¼ï¼‰
        if isinstance(content_text, list):
            texts = []
            for item in content_text:
                if isinstance(item, dict) and item.get("type") == "text":
                    texts.append(item.get("text", ""))
                elif isinstance(item, dict) and item.get("type") == "image_url":
                    texts.append("[å›¾ç‰‡]")
            content_text = "\n".join(texts)

        md_content += f"### {role.upper()}\n\n{content_text}\n\n"

    # æ·»åŠ å½“å‰å›å¤
    md_content += f"""---

## å½“å‰å›å¤

"""

    if reasoning:
        md_content += f"""<details>
<summary>ğŸ’­ Thinking è¿‡ç¨‹ ({metadata.get('reasoning_chars', len(reasoning))} å­—ç¬¦)</summary>

{reasoning}
</details>

"""

    md_content += f"""### âœ¨ æ­£å¼å›å¤

{content}

---

## å…ƒæ•°æ®

{THREE_DOTS}json
{json.dumps(metadata, indent=2, ensure_ascii=False, default=str)}
{THREE_DOTS}
"""

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(md_content)

    return output_path


def save_json_history(
    output_path: Path,
    messages: List[Dict],
    reasoning: str,
    content: str,
    metadata: Dict,
):
    """ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆOpenAIå…¼å®¹ï¼Œä¾¿äºåç»­åŠ è½½ç»§ç»­å¯¹è¯ï¼‰"""
    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°æ¶ˆæ¯åˆ—è¡¨
    assistant_message = {"role": "assistant", "content": content}
    if reasoning:
        assistant_message["reasoning_content"] = reasoning

    full_messages = messages.copy()
    full_messages.append(assistant_message)

    data = {
        "metadata": {
            "export_time": datetime.now().isoformat(),
            "total_tokens": metadata.get("total_tokens"),
            "model": metadata.get("request_body", {}).get("model"),
        },
        "messages": full_messages,
    }

    json_path = output_path.with_suffix(".json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return json_path


# ============ ä¸»ç¨‹åº ============


def main():
    # åŠ è½½ç¯å¢ƒé…ç½®
    # åŸæ¥æ˜¯ï¼šenv_config = load_config()
    # æ”¹ä¸ºï¼š
    args = parse_arguments()
    env_config = load_config(args)

    # ç¡®å®šAPIç«¯ç‚¹å’Œå¯†é’¥
    endpoint = args.endpoint or env_config["endpoint"]
    api_key = args.api_key or env_config["api_key"]

    if not endpoint:
        print(
            "âŒ é”™è¯¯: æœªè®¾ç½®APIç«¯ç‚¹ã€‚è¯·ä½¿ç”¨--endpointå‚æ•°ã€-f JSONé…ç½®æˆ–è®¾ç½®.envæ–‡ä»¶",
            file=sys.stderr,
        )
        sys.exit(1)

    if not api_key:
        print(
            "âŒ é”™è¯¯: æœªè®¾ç½®APIå¯†é’¥ã€‚è¯·ä½¿ç”¨--api-keyå‚æ•°ã€-f JSONé…ç½®æˆ–è®¾ç½®.envæ–‡ä»¶",
            file=sys.stderr,
        )
        sys.exit(1)

    # æ„å»ºç”¨æˆ·è¾“å…¥çš„messagesï¼ˆå¤„ç†@æ–‡ä»¶ï¼‰
    user_messages, prompt_hint = build_user_messages(args)

    # æ„å»ºå®Œæ•´çš„OpenAIæ ¼å¼è¯·æ±‚ä½“
    try:
        request_body = build_request_body(args, env_config, user_messages)
    except Exception as e:
        print(f"âŒ æ„å»ºè¯·æ±‚å¤±è´¥: {e}", file=sys.stderr)
        sys.exit(1)

    # æ£€æŸ¥æ˜¯å¦æœ‰messages
    if not request_body.get("messages"):
        print(
            "âŒ é”™è¯¯: æ²¡æœ‰è¾“å…¥å†…å®¹ã€‚è¯·æä¾›æç¤ºæ–‡æœ¬ã€ä½¿ç”¨@æ–‡ä»¶åŠ è½½ï¼Œæˆ–åœ¨JSONé…ç½®ä¸­æä¾›messagesã€‚",
            file=sys.stderr,
        )
        sys.exit(1)

    # ç¡®å®šè¾“å‡ºæ–‡ä»¶
    if args.output:
        output_path = Path(args.output)
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_hint = re.sub(r"[^\w\s-]", "", prompt_hint)[:20].strip() or "chat"
        output_path = Path(f"chat_{safe_hint}_{timestamp}.md")

    # æ‰“å¼€è¾“å‡ºæ–‡ä»¶ï¼ˆTeeæ¨¡å¼ï¼‰
    tee_file = open(output_path, "w", encoding="utf-8")

    try:
        # åˆå§‹åŒ–æ‰“å°å™¨ï¼ˆå¸¦Teeï¼‰
        printer = SafePrinter(tee_file=tee_file)

        # å†™å…¥æ–‡ä»¶å¤´
        tee_file.write(
            f"# AIå¯¹è¯è®°å½• - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        )
        tee_file.write("## è¯·æ±‚é…ç½®\n\n")
        tee_file.write(
            f"{THREE_DOTS}json\n{json.dumps({k: v for k, v in request_body.items() if k != 'messages'}, indent=2, ensure_ascii=False)}\n{THREE_DOTS}\n\n"
        )
        tee_file.write("## å¯¹è¯å†…å®¹\n\n")

        # åˆ›å»ºå®¢æˆ·ç«¯å¹¶å‘é€è¯·æ±‚
        client = TypewriterHTTPClient(endpoint=endpoint, api_key=api_key)

        reasoning, content, metadata = client.stream_request(request_body, printer)

        # å®Œæˆæ–‡ä»¶å†™å…¥
        printer.finalize()

        # ä¿å­˜å®Œæ•´å¯¹è¯è®°å½•ï¼ˆMarkdownï¼‰
        save_conversation(
            output_path, reasoning, content, metadata, request_body["messages"]
        )

        # åŒæ—¶ä¿å­˜JSONå†å²ï¼ˆä¾¿äºç»§ç»­å¯¹è¯ï¼‰
        json_path = save_json_history(
            output_path, request_body["messages"], reasoning, content, metadata
        )

        print(f"\nğŸ’¾ å¯¹è¯å·²ä¿å­˜:")
        print(f"   Markdown: {output_path.absolute()}")
        print(f"   JSONå†å²: {json_path.absolute()}")
        print(f"\nğŸ’¡ æç¤º: ä½¿ç”¨ --context {json_path.name} ç»§ç»­æ­¤å¯¹è¯")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­", file=sys.stderr)
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºé”™è¯¯: {e}", file=sys.stderr)
        import traceback

        traceback.print_exc()
        sys.exit(1)
    finally:
        if tee_file:
            tee_file.close()


if __name__ == "__main__":
    main()
