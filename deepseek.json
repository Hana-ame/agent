{
  "max_tokens": 163840,
  "temperature": 0.7,
  "top_p": 0.7,
  "top_k": 50,
  "frequency_penalty": 0,
  "enable_thinking": true,
  "thinking_budget": 81920,
  "model": "deepseek-ai/DeepSeek-V3.2",
  "messages": [
    {
      "role": "user",
      "content": "```py\n#!/usr/bin/env python3\n\"\"\"\nAdvanced Typewriter Effect HTTP Client for OpenAI-compatible APIs (Enhanced Version).\nZero-default policy with graceful fallback - avoids crashes when encountering unsupported parameters.\n\"\"\"\n\nimport argparse\nimport base64\nimport importlib.util\nimport inspect\nimport io\nimport json\nimport logging\nimport mimetypes\nimport os\nimport re\nimport signal\nimport sys\nimport time\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional, Dict, List, Any, Tuple, Union, Callable\n\nimport requests\nfrom dotenv import load_dotenv\n\n# ============ Setup & Constants ============\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    stream=sys.stderr\n)\nlogger = logging.getLogger(__name__)\n\nCODE_BLOCK = \"`\" * 3\n\n\nclass Constants:\n    \"\"\"Application constants\"\"\"\n    CHUNK_SIZE = 1024\n    STREAM_DELAY_REASONING = 0.01\n    STREAM_DELAY_CONTENT = 0.03\n    TIMEOUT = 60*60*6  # ‰øùÊåÅ 1 Â∞èÊó∂Ë∂ÖÊó∂\n\n\nclass Colors:\n    \"\"\"ANSI Color codes for terminal output.\"\"\"\n\n    GREEN = \"\\033[92m\"\n    YELLOW = \"\\033[93m\"\n    RED = \"\\033[91m\"\n    BLUE = \"\\033[94m\"\n    CYAN = \"\\033[96m\"\n    BOLD = \"\\033[1m\"\n    END = \"\\033[0m\"\n\n    @classmethod\n    def info(cls, text: str) -> str:\n        return f\"{cls.GREEN}{text}{cls.END}\"\n\n    @classmethod\n    def warn(cls, text: str) -> str:\n        return f\"{cls.YELLOW}{text}{cls.END}\"\n\n    @classmethod\n    def error(cls, text: str) -> str:\n        return f\"{cls.RED}{text}{cls.END}\"\n\n    @classmethod\n    def bold(cls, text: str) -> str:\n        return f\"{cls.BOLD}{text}{cls.END}\"\n\n\n# ============ Exceptions ============\n\nclass ChatError(Exception):\n    \"\"\"Base exception for chat errors\"\"\"\n    pass\n\n\nclass ConfigurationError(ChatError):\n    \"\"\"Missing required configuration\"\"\"\n    pass\n\n\nclass APIError(ChatError):\n    \"\"\"API communication error\"\"\"\n    pass\n\n\nclass FileProcessingError(ChatError):\n    \"\"\"File I/O error\"\"\"\n    pass\n\n\nclass ToolError(ChatError):\n    \"\"\"Tool execution error\"\"\"\n    pass\n\n\n# ============ UTF-8 Setup ============\n\ndef setup_utf8():\n    \"\"\"Force UTF-8 encoding for Windows and Unix\"\"\"\n    if sys.platform == \"win32\":\n        import ctypes\n        try:\n            kernel32 = ctypes.windll.kernel32\n            kernel32.SetConsoleCP(65001)\n            kernel32.SetConsoleOutputCP(65001)\n        except Exception:\n            pass\n    \n    if hasattr(sys.stdout, 'buffer'):\n        sys.stdout = io.TextIOWrapper(\n            sys.stdout.buffer,\n            encoding=\"utf-8\",\n            errors=\"replace\",\n            line_buffering=True\n        )\n\n\n# ============ Data Models ============\n\n@dataclass\nclass APIConfig:\n    \"\"\"API connection configuration - only endpoint and api_key\"\"\"\n\n    endpoint: str = \"\"\n    api_key: str = \"\"\n    timeout: int = Constants.TIMEOUT  # HTTP timeout, not API parameter\n\n    def validate(self) -> None:\n        if not self.endpoint:\n            raise ConfigurationError(\"API endpoint is required\")\n        if not self.api_key:\n            raise ConfigurationError(\"API key is required\")\n\n\n@dataclass\nclass RequestMetadata:\n    \"\"\"Metadata for request tracking and reporting\"\"\"\n\n    start_time: datetime = field(default_factory=datetime.now)\n    end_time: Optional[datetime] = None\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n    total_tokens: int = 0\n    finish_reason: Optional[str] = None\n    model: Optional[str] = None\n    duration: float = 0.0\n    tool_calls: List[Dict] = field(default_factory=list)\n    tool_results: List[Dict] = field(default_factory=list)\n\n    def finalize(self) -> None:\n        self.end_time = datetime.now()\n        self.duration = (self.end_time - self.start_time).total_seconds()\n\n    def get_token_rate(self) -> Optional[float]:\n        \"\"\"Calculate tokens per second\"\"\"\n        if self.duration > 0 and self.completion_tokens > 0:\n            return self.completion_tokens / self.duration\n        return None\n\n\n# ============ Tool System ============\n\nclass ToolManager:\n    \"\"\"Manages tool functions and execution\"\"\"\n    \n    def __init__(self, tools_module_path: str = \"tools.py\"):\n        self.tools_module_path = tools_module_path\n        self.available_functions: Dict[str, Callable] = {}\n        self.tool_definitions: List[Dict] = []\n        self._load_tools()\n    \n    def _load_tools(self):\n        \"\"\"Load tool functions from tools.py module\"\"\"\n        try:\n            spec = importlib.util.spec_from_file_location(\"tools\", self.tools_module_path)\n            if spec and spec.loader:\n                tools_module = importlib.util.module_from_spec(spec)\n                spec.loader.exec_module(tools_module)\n                \n                # Find all functions in the module\n                for name, obj in inspect.getmembers(tools_module):\n                    if inspect.isfunction(obj) and not name.startswith('_'):\n                        # Check if function has proper annotations for OpenAI tools\n                        sig = inspect.signature(obj)\n                        params = {}\n                        for param_name, param in sig.parameters.items():\n                            if param_name != 'self':\n                                param_type = str(param.annotation) if param.annotation != inspect.Parameter.empty else \"string\"\n                                params[param_name] = {\n                                    \"type\": param_type.replace(\"<class '\", \"\").replace(\"'>\", \"\").lower(),\n                                    \"description\": f\"Parameter {param_name}\"\n                                }\n                        \n                        # Create tool definition\n                        tool_def = {\n                            \"type\": \"function\",\n                            \"function\": {\n                                \"name\": name,\n                                \"description\": obj.__doc__ or f\"Function {name}\",\n                                \"parameters\": {\n                                    \"type\": \"object\",\n                                    \"properties\": params,\n                                    \"required\": list(params.keys())\n                                }\n                            }\n                        }\n                        \n                        self.tool_definitions.append(tool_def)\n                        self.available_functions[name] = obj\n                        \n                        print(f\"{Colors.info('‚úì')} Loaded tool: {name}\", file=sys.stderr)\n                \n                if not self.tool_definitions:\n                    print(f\"{Colors.warn('‚ö†')} No tools found in {self.tools_module_path}\", file=sys.stderr)\n            else:\n                print(f\"{Colors.warn('‚ö†')} Tools module not found: {self.tools_module_path}\", file=sys.stderr)\n        except Exception as e:\n            print(f\"{Colors.warn('‚ö†')} Failed to load tools: {e}\", file=sys.stderr)\n    \n    def execute_tool(self, tool_call: Dict) -> Dict:\n        \"\"\"Execute a tool call and return result\"\"\"\n        try:\n            function_name = tool_call[\"function\"][\"name\"]\n            function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n            \n            if function_name not in self.available_functions:\n                raise ToolError(f\"Unknown function: {function_name}\")\n            \n            print(f\"\\n{Colors.CYAN}üîß Executing tool: {function_name}{Colors.END}\", file=sys.stderr)\n            print(f\"   Arguments: {function_args}\", file=sys.stderr)\n            \n            # Execute the function\n            result = self.available_functions[function_name](**function_args)\n            \n            print(f\"   Result: {result}\", file=sys.stderr)\n            \n            return {\n                \"tool_call_id\": tool_call[\"id\"],\n                \"role\": \"tool\",\n                \"name\": function_name,\n                \"content\": json.dumps(result, ensure_ascii=False)\n            }\n            \n        except Exception as e:\n            error_msg = f\"Tool execution failed: {str(e)}\"\n            print(f\"{Colors.error('‚ùå')} {error_msg}\", file=sys.stderr)\n            return {\n                \"tool_call_id\": tool_call[\"id\"],\n                \"role\": \"tool\",\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": json.dumps({\"error\": error_msg}, ensure_ascii=False)\n            }\n    \n    def get_tool_definitions(self) -> List[Dict]:\n        \"\"\"Get OpenAI-compatible tool definitions\"\"\"\n        return self.tool_definitions\n\n\n# ============ Configuration Management ============\n\nclass ConfigManager:\n    \"\"\"Manages configuration loading with graceful fallback\"\"\"\n\n    ENV_PATHS = [\".env\", \"../.env\", Path.home() / \".ai_chat.env\"]\n    PROFILE_PATHS = [\n        Path(\"profiles.json\"),\n        Path(\"../profiles.json\"),\n        Path.home() / \".ai_chat_profiles.json\",\n    ]\n\n    @staticmethod\n    def load_connection_config(args) -> APIConfig:\n        \"\"\"Load endpoint and api_key with priority: CLI > Profile > Env > Default\"\"\"\n        config = APIConfig()\n\n        # 1. Load .env files first\n        for env_path in ConfigManager.ENV_PATHS:\n            if Path(env_path).exists():\n                load_dotenv(env_path)\n                logger.info(f\"Loaded environment: {env_path}\")\n                break\n\n        # 2. From environment variables\n        config.endpoint = os.getenv(\"ENDPOINT\", \"\")\n        config.api_key = os.getenv(\"API_KEY\", \"\")\n\n        # 3. Try to load from profile\n        profile_config = ConfigManager._load_profile(args.profile)\n        if profile_config:\n            if \"endpoint\" in profile_config:\n                config.endpoint = profile_config[\"endpoint\"]\n            if \"api_key\" in profile_config:\n                config.api_key = profile_config[\"api_key\"]\n            if \"apiKey\" in profile_config:\n                config.api_key = profile_config[\"apiKey\"]\n\n        # 4. CLI arguments override everything\n        if hasattr(args, \"endpoint\") and args.endpoint:\n            config.endpoint = args.endpoint\n        if hasattr(args, \"api_key\") and args.api_key:\n            config.api_key = args.api_key\n\n        return config\n\n    @staticmethod\n    def _load_profile(profile_name: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Load profile from various locations\"\"\"\n        for path in ConfigManager.PROFILE_PATHS:\n            if path.exists():\n                try:\n                    with open(path, \"r\", encoding=\"utf-8\") as f:\n                        profiles = json.load(f)\n\n                    if profile_name in profiles:\n                        print(\n                            f\"{Colors.info('‚úì')} Loaded profile: {profile_name} from {path}\",\n                            file=sys.stderr,\n                        )\n                        return profiles[profile_name]\n                except Exception as e:\n                    logger.error(f\"Failed to load profile {path}: {e}\")\n\n        if profile_name != \"default\":\n            print(\n                f\"{Colors.warn('‚ö†')} Profile '{profile_name}' not found, using default\",\n                file=sys.stderr,\n            )\n        return None\n\n    @staticmethod\n    def load_request_body(config_path: str, args) -> Dict[str, Any]:\n        \"\"\"Load request body from config.json with safe parameter handling\"\"\"\n        path = Path(config_path).expanduser().resolve()\n\n        if not path.exists():\n            if (\n                config_path != \"config.json\"\n            ):  # User explicitly specified non-existent file\n                raise FileNotFoundError(f\"Config file not found: {config_path}\")\n            # Use empty config if default config.json doesn't exist\n            return {}\n\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            print(f\"{Colors.info('‚úì')} Loaded request config: {path}\", file=sys.stderr)\n\n            # Apply CLI overrides for safety-critical parameters\n            ConfigManager._apply_cli_overrides(data, args)\n\n            return data\n        except Exception as e:\n            raise ValueError(f\"Failed to load config file ({config_path}): {e}\")\n\n    @staticmethod\n    def _apply_cli_overrides(data: Dict[str, Any], args) -> None:\n        \"\"\"Apply CLI arguments to request body, removing unsupported parameters\"\"\"\n        # Safety: Remove problematic parameters if CLI provides alternatives\n        if hasattr(args, \"model\") and args.model:\n            data[\"model\"] = args.model\n            # Remove any model-specific parameters that might conflict\n            data.pop(\"temperature\", None)  # Some models don't support temperature\n            data.pop(\"top_p\", None)\n            data.pop(\"frequency_penalty\", None)\n            data.pop(\"presence_penalty\", None)\n\n        # OpenAI standard parameters override\n        if hasattr(args, \"temperature\") and args.temperature is not None:\n            data[\"temperature\"] = args.temperature\n        if hasattr(args, \"max_tokens\") and args.max_tokens is not None:\n            data[\"max_tokens\"] = args.max_tokens\n        if hasattr(args, \"top_p\") and args.top_p is not None:\n            data[\"top_p\"] = args.top_p\n        if hasattr(args, \"presence_penalty\") and args.presence_penalty is not None:\n            data[\"presence_penalty\"] = args.presence_penalty\n        if hasattr(args, \"frequency_penalty\") and args.frequency_penalty is not None:\n            data[\"frequency_penalty\"] = args.frequency_penalty\n        if hasattr(args, \"seed\") and args.seed is not None:\n            data[\"seed\"] = args.seed\n\n        # Stream handling\n        if hasattr(args, \"no_stream\") and args.no_stream:\n            data[\"stream\"] = False\n        elif \"stream\" not in data:\n            data[\"stream\"] = True  # Default to stream for better UX\n\n        # Thinking enable/disable\n        if hasattr(args, \"enable_thinking\"):\n            if args.enable_thinking is not None:\n                data[\"enable_thinking\"] = args.enable_thinking\n                # Some APIs don't support reasoning_content, remove if disabled\n                if not args.enable_thinking:\n                    data.pop(\"reasoning_content\", None)\n\n    @staticmethod\n    def save_context_to_config(config_path: str, messages: List[Dict]) -> None:\n        \"\"\"Save conversation context back to config.json\"\"\"\n        try:\n            path = Path(config_path).expanduser().resolve()\n            \n            # Load existing config\n            if path.exists():\n                with open(path, \"r\", encoding=\"utf-8\") as f:\n                    config_data = json.load(f)\n            else:\n                config_data = {}\n            \n            # Update messages\n            config_data[\"messages\"] = messages\n            \n            # Save back\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(config_data, f, ensure_ascii=False, indent=2)\n            \n            print(f\"{Colors.info('‚úì')} Saved context to: {path}\", file=sys.stderr)\n            \n        except Exception as e:\n            print(f\"{Colors.warn('‚ö†')} Failed to save context: {e}\", file=sys.stderr)\n\n\n# ============ File & Message Building ============\n\nclass MessageBuilder:\n    \"\"\"Builds messages with file context support\"\"\"\n\n    IMAGE_TYPES = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\", \".bmp\"}\n\n    @staticmethod\n    def load_file(filepath: str) -> Union[str, Dict[str, Any]]:\n        \"\"\"Load file content with MIME type detection\"\"\"\n        if filepath.startswith(\"@\"):\n            filepath = filepath[1:]\n\n        path = Path(filepath).expanduser().resolve()\n        if not path.exists():\n            raise FileNotFoundError(f\"File not found: {filepath}\")\n\n        mime_type, _ = mimetypes.guess_type(path)\n        if not mime_type:\n            mime_type = \"application/octet-stream\"\n\n        # Image handling\n        if mime_type.startswith(\"image/\") or path.suffix.lower() in MessageBuilder.IMAGE_TYPES:\n            with open(path, \"rb\") as f:\n                encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n            return {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:{mime_type};base64,{encoded}\"},\n            }\n\n        # Text file handling with encoding detection\n        encodings = [\"utf-8\", \"utf-8-sig\", \"gbk\", \"latin-1\"]\n        content = None\n        \n        for enc in encodings:\n            try:\n                with open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n                    content = f.read()\n                break\n            except UnicodeDecodeError:\n                continue\n        \n        if content is None:\n            with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n                content = f.read()\n\n        # Format based on file type\n        suffix = path.suffix[1:] if path.suffix else \"text\"\n        if path.suffix in [\".md\", \".txt\", \".rst\"]:\n            return f\"{content}\\n\\n[File: {path.name}]\"\n        else:\n            return f\"{CODE_BLOCK}{suffix}\\n{content}\\n{CODE_BLOCK}\\n[File: {path.name}]\"\n\n    @staticmethod\n    def build_messages(\n        args, config_data: Dict | None = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Build final message list with priority: context > config > prompt\"\"\"\n        messages = []\n        \n        # Extract system prompt from config if exists\n        system_prompt = None\n        config_messages = []\n        if config_data:\n            system_prompt = config_data.pop(\"system\", None)\n            config_messages = config_data.pop(\"messages\", [])\n\n        # 1. Add system prompt if exists (always first)\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n            print(f\"{Colors.info('‚úì')} Added system prompt\", file=sys.stderr)\n\n        # 2. Load context if provided\n        if hasattr(args, \"context\") and args.context:\n            try:\n                context_messages = MessageBuilder._load_context(args.context)\n                messages.extend(context_messages)\n                print(\n                    f\"{Colors.info('‚úì')} Loaded conversation context: {args.context}\",\n                    file=sys.stderr,\n                )\n            except Exception as e:\n                print(\n                    f\"{Colors.warn('‚ö†')} Failed to load context: {e}\", file=sys.stderr\n                )\n\n        # 3. Add messages from config\n        if config_messages:\n            messages.extend(config_messages)\n\n        # 4. Add current user input\n        if hasattr(args, \"prompt\") and args.prompt:\n            user_messages = MessageBuilder._build_user_messages(args.prompt)\n            messages.extend(user_messages)\n\n        if not messages and not system_prompt:\n            raise ConfigurationError(\n                \"No messages to send. Please provide prompt or messages in config.\"\n            )\n\n        return messages\n\n    @staticmethod\n    def _load_context(context_path: str) -> List[Dict]:\n        \"\"\"Load conversation history from JSON file\"\"\"\n        path = Path(context_path).expanduser().resolve()\n        if not path.exists():\n            raise FileNotFoundError(f\"Context file not found: {context_path}\")\n\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n\n        if isinstance(data, list):\n            return data\n        elif isinstance(data, dict) and \"messages\" in data:\n            return data[\"messages\"]\n        else:\n            raise ValueError(\n                \"Invalid context format: expected list or {messages: [...]}\"\n            )\n\n    @staticmethod\n    def _build_user_messages(prompt_parts: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Build user messages from prompt parts\"\"\"\n        if not prompt_parts:\n            return []\n\n        content_parts = []\n        has_image = False\n\n        for part in prompt_parts:\n            if part.startswith(\"@\"):\n                try:\n                    file_content = MessageBuilder.load_file(part)\n                    if isinstance(file_content, dict):\n                        has_image = True\n                        print(f\"{Colors.info('‚úì')} Attached image: {part[1:]}\", file=sys.stderr)\n                    else:\n                        print(f\"{Colors.info('‚úì')} Attached file: {part[1:]}\", file=sys.stderr)\n                    content_parts.append(file_content)\n                except Exception as e:\n                    print(\n                        f\"{Colors.warn('‚ö†')} Failed to load file {part}: {e}\",\n                        file=sys.stderr,\n                    )\n                    content_parts.append(f\"[Failed to load file: {part}]\")\n            else:\n                content_parts.append(part)\n\n        # Build message content\n        if has_image:\n            multimodal_content = []\n            current_text = []\n\n            for part in content_parts:\n                if isinstance(part, dict):\n                    if current_text:\n                        multimodal_content.append(\n                            {\"type\": \"text\", \"text\": \"\\n\".join(current_text)}\n                        )\n                        current_text = []\n                    multimodal_content.append(part)\n                else:\n                    current_text.append(part)\n\n            if current_text:\n                multimodal_content.append(\n                    {\"type\": \"text\", \"text\": \"\\n\\n\".join(current_text)}\n                )\n\n            return [{\"role\": \"user\", \"content\": multimodal_content}]\n        else:\n            user_content = \"\\n\\n\".join(content_parts)\n            return [{\"role\": \"user\", \"content\": user_content}]\n\n\n# ============ Typewriter Output ============\n\nclass TypewriterPrinter:\n    \"\"\"Typewriter effect printer with file output\"\"\"\n\n    def __init__(self, output_file: Optional[io.TextIOWrapper] = None):\n        self.output_file = output_file\n        self.reasoning_buffer = \"\"\n        self.content_buffer = \"\"\n        self.reasoning_printed = 0\n        self.content_printed = 0\n        self.in_reasoning = True\n        self.start_time = time.time()\n        self.first_token_time = None\n        self.token_count = 0\n\n        # Performance tracking\n        self.reasoning_start = None\n        self.content_start = None\n\n    def write(self, text: str, is_reasoning: bool = False):\n        \"\"\"Write text to file\"\"\"\n        if self.output_file:\n            try:\n                self.output_file.write(text)\n                if not is_reasoning:\n                    self.output_file.flush()\n            except Exception as e:\n                print(f\"\\n{Colors.error('[File write error]')} {e}\", file=sys.stderr)\n\n    def update_reasoning(self, delta: str):\n        \"\"\"Update and print reasoning content\"\"\"\n        if not delta:\n            return\n\n        if self.first_token_time is None:\n            self.first_token_time = time.time()\n\n        if self.reasoning_start is None:\n            self.reasoning_start = time.time()\n            print(f\"\\n{Colors.BLUE}üí≠ Thinking:{Colors.END}\")\n            if self.output_file:\n                self.write(\"\\nüí≠ Thinking:\\n\", is_reasoning=True)\n\n        self.reasoning_buffer += delta\n        new_text = self.reasoning_buffer[self.reasoning_printed :]\n\n        # Print to console with typewriter effect\n        for char in new_text:\n            print(char, end=\"\", flush=True)\n            time.sleep(Constants.STREAM_DELAY_REASONING)\n\n        # Write to file\n        self.write(new_text, is_reasoning=True)\n\n        self.reasoning_printed = len(self.reasoning_buffer)\n        self.token_count += len(delta.split())  # Rough estimate\n\n    def switch_to_content(self):\n        \"\"\"Switch from reasoning to content output\"\"\"\n        if self.in_reasoning:\n            self.in_reasoning = False\n            self.content_start = time.time()\n\n            # Console output\n            print(f\"\\n{Colors.bold('='*50)}\")\n            print(f\"{Colors.CYAN}‚ú® Response:{Colors.END}\")\n            print(f\"{Colors.bold('='*50)}\")\n\n            # File output\n            if self.output_file:\n                self.write(f\"\\n\\n{'='*50}\\n‚ú® Response:\\n{'='*50}\\n\\n\")\n\n    def update_content(self, delta: str):\n        \"\"\"Update and print response content\"\"\"\n        if not delta:\n            return\n\n        if self.in_reasoning:\n            self.switch_to_content()\n\n        if self.first_token_time is None:\n            self.first_token_time = time.time()\n\n        self.content_buffer += delta\n        new_text = self.content_buffer[self.content_printed :]\n\n        # Typewriter effect\n        for char in new_text:\n            print(char, end=\"\", flush=True)\n            time.sleep(Constants.STREAM_DELAY_CONTENT)\n        \n        self.write(new_text)\n\n        self.content_printed = len(self.content_buffer)\n        self.token_count += len(delta.split())  # Rough estimate\n\n    def finalize(self):\n        \"\"\"Finalize output\"\"\"\n        if self.output_file:\n            try:\n                self.output_file.flush()\n            except Exception as e:\n                print(f\"{Colors.warn('‚ö†')} Failed to flush file: {e}\", file=sys.stderr)\n\n\n# ============ HTTP Client ============\n\nclass APIClient:\n    \"\"\"HTTP client with retry and error handling using Session\"\"\"\n\n    def __init__(self, config: APIConfig, max_retries: int = 3):\n        self.config = config\n        self.max_retries = max_retries\n        self.session = requests.Session()\n        self.session.headers.update({\n            \"Authorization\": f\"Bearer {config.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"text/event-stream\",\n        })\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.session.close()\n\n    def _estimate_tokens(self, messages: List[Dict]) -> Tuple[int, int]:\n        \"\"\"‰º∞ÁÆó Prompt ÁöÑ Token Êï∞Ôºà‰øùÂÆà‰º∞ËÆ°ÔºöÊØè 3 ‰∏™Â≠óÁ¨¶ 1 ‰∏™ TokenÔºâ\"\"\"\n        try:\n            # Â∞Ü messages ËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤ËÆ°ÁÆó\n            text = json.dumps(messages, ensure_ascii=False)\n            char_count = len(text)\n            # ‰øùÂÆà‰º∞ËÆ°ÔºöËã±ÊñáÁ∫¶ 4 Â≠óÁ¨¶/tokenÔºå‰∏≠ÊñáÁ∫¶ 1-2 Â≠óÁ¨¶/tokenÔºåÊ∑∑ÂêàÊåâ 3 ËÆ°ÁÆó\n            estimated_tokens = char_count // 3\n            return char_count, estimated_tokens\n        except:\n            return 0, 0\n\n    def request(\n        self, payload: Dict[str, Any], printer: TypewriterPrinter, tool_manager: Optional[ToolManager] = None\n    ) -> RequestMetadata:\n        \"\"\"Execute API request with retry logic\"\"\"\n        metadata = RequestMetadata(model=payload.get(\"model\"))\n        is_stream = payload.get(\"stream\", True)\n\n        # ËÆ°ÁÆó Prompt ‰ø°ÊÅØ\n        messages = payload.get(\"messages\", [])\n        char_count, estimated_tokens = self._estimate_tokens(messages)\n        max_tokens = payload.get(\"max_tokens\", \"Not set\")\n        \n        # ÊòæÁ§∫ËØ∑Ê±Ç‰ø°ÊÅØÔºåÂåÖÊã¨ Token È¢Ñ‰º∞\n        print(\n            f\"\\n{Colors.CYAN}üöÄ Requesting {metadata.model or 'unknown model'}...{Colors.END}\"\n        )\n        print(f\"   Endpoint: {self.config.endpoint}\")\n        print(f\"   Stream: {'Yes' if is_stream else 'No'}\")\n        print(f\"   Prompt Size: {char_count} chars (Est. ~{estimated_tokens} tokens)\")\n        \n        if max_tokens != \"Not set\":\n            print(f\"   Max Tokens: {max_tokens}\")\n        else:\n            print(f\"   Max Tokens: {Colors.warn('Not set')} (model default)\")\n\n        # Safety: Remove any empty or None parameters that might cause API errors\n        clean_payload = self._clean_payload(payload)\n\n        for attempt in range(self.max_retries + 1):\n            try:\n                response = self.session.post(\n                    self.config.endpoint,\n                    json=clean_payload,\n                    stream=is_stream,\n                    timeout=self.config.timeout,\n                )\n                response.raise_for_status()\n\n                if is_stream:\n                    return self._handle_stream(response, printer, metadata, tool_manager)\n                else:\n                    return self._handle_non_stream(response, printer, metadata, tool_manager)\n\n            except requests.exceptions.RequestException as e:\n                if attempt < self.max_retries:\n                    wait_time = 2**attempt\n                    print(\n                        f\"{Colors.warn(f'‚ö†Ô∏è  Request failed ({e}), retrying in {wait_time}s...')}\",\n                        file=sys.stderr,\n                    )\n                    time.sleep(wait_time)\n                else:\n                    raise APIError(\n                        f\"Request failed after {self.max_retries} retries: {e}\"\n                    )\n\n        raise APIError(\"Max retries exceeded\")\n\n    def _clean_payload(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove potentially problematic parameters\"\"\"\n        clean = payload.copy()\n\n        # Remove None values\n        for key in list(clean.keys()):\n            if clean[key] is None:\n                del clean[key]\n\n        # Remove empty strings\n        for key in list(clean.keys()):\n            if isinstance(clean[key], str) and not clean[key].strip():\n                del clean[key]\n\n        return clean\n\n    def _handle_stream(\n        self, response, printer: TypewriterPrinter, metadata: RequestMetadata, tool_manager: Optional[ToolManager]\n    ) -> RequestMetadata:\n        \"\"\"Handle streaming response\"\"\"\n        buffer = \"\"\n        tool_calls_buffer = {}\n\n        for chunk in response.iter_content(chunk_size=Constants.CHUNK_SIZE):\n            if not chunk:\n                continue\n\n            buffer += chunk.decode(\"utf-8\", errors=\"replace\")\n\n            while \"\\n\" in buffer:\n                line, buffer = buffer.split(\"\\n\", 1)\n                line = line.strip()\n\n                if not line.startswith(\"data: \"):\n                    continue\n\n                data_str = line[6:]\n                if data_str == \"[DONE]\":\n                    break\n\n                try:\n                    data = json.loads(data_str)\n                    self._process_stream_data(data, printer, metadata, tool_calls_buffer)\n                except json.JSONDecodeError:\n                    continue\n\n        # Handle tool calls after stream ends\n        if tool_calls_buffer and tool_manager:\n            self._handle_tool_calls(tool_calls_buffer, metadata, tool_manager)\n\n        metadata.finalize()\n        return metadata\n\n    def _process_stream_data(\n        self, data: Dict, printer: TypewriterPrinter, metadata: RequestMetadata, tool_calls_buffer: Dict\n    ):\n        \"\"\"Process individual stream data chunk\"\"\"\n        # Update usage\n        if \"usage\" in data and data[\"usage\"]:\n            metadata.prompt_tokens = data[\"usage\"].get(\"prompt_tokens\", 0)\n            metadata.completion_tokens = data[\"usage\"].get(\"completion_tokens\", 0)\n            metadata.total_tokens = data[\"usage\"].get(\"total_tokens\", 0)\n\n        choices = data.get(\"choices\", [])\n        if not choices:\n            return\n\n        choice = choices[0]\n        delta = choice.get(\"delta\", {})\n\n        # Update finish reason\n        finish_reason = choice.get(\"finish_reason\")\n        if finish_reason:\n            metadata.finish_reason = finish_reason\n\n        # Process tool calls\n        tool_calls = delta.get(\"tool_calls\", [])\n        for tool_call in tool_calls:\n            idx = tool_call.get(\"index\", 0)\n            if idx not in tool_calls_buffer:\n                tool_calls_buffer[idx] = {\"id\": \"\", \"type\": \"function\", \"function\": {\"name\": \"\", \"arguments\": \"\"}}\n            \n            if \"id\" in tool_call:\n                tool_calls_buffer[idx][\"id\"] = tool_call[\"id\"]\n            if \"function\" in tool_call:\n                if \"name\" in tool_call[\"function\"]:\n                    tool_calls_buffer[idx][\"function\"][\"name\"] = tool_call[\"function\"][\"name\"]\n                if \"arguments\" in tool_call[\"function\"]:\n                    tool_calls_buffer[idx][\"function\"][\"arguments\"] += tool_call[\"function\"][\"arguments\"]\n\n        # Process content\n        reasoning_delta = delta.get(\"reasoning_content\") or delta.get(\"reasoning\") or \"\"\n        content_delta = delta.get(\"content\", \"\")\n\n        if reasoning_delta:\n            printer.update_reasoning(reasoning_delta)\n\n        if content_delta:\n            printer.update_content(content_delta)\n\n    def _handle_non_stream(\n        self, response, printer: TypewriterPrinter, metadata: RequestMetadata, tool_manager: Optional[ToolManager]\n    ) -> RequestMetadata:\n        \"\"\"Handle non-streaming response\"\"\"\n        data = response.json()\n        choice = data.get(\"choices\", [{}])[0]\n        message = choice.get(\"message\", {})\n\n        reasoning = message.get(\"reasoning_content\", \"\")\n        content = message.get(\"content\", \"\")\n\n        if reasoning:\n            printer.update_reasoning(reasoning)\n\n        printer.switch_to_content()\n        printer.update_content(content)\n\n        # Handle tool calls\n        tool_calls = message.get(\"tool_calls\", [])\n        if tool_calls and tool_manager:\n            self._handle_tool_calls(tool_calls, metadata, tool_manager)\n\n        # Update metadata\n        if \"usage\" in data:\n            metadata.prompt_tokens = data[\"usage\"].get(\"prompt_tokens\", 0)\n            metadata.completion_tokens = data[\"usage\"].get(\"completion_tokens\", 0)\n            metadata.total_tokens = data[\"usage\"].get(\"total_tokens\", 0)\n\n        metadata.finish_reason = choice.get(\"finish_reason\")\n        metadata.finalize()\n\n        return metadata\n\n    def _handle_tool_calls(self, tool_calls: Union[List, Dict], metadata: RequestMetadata, tool_manager: ToolManager):\n        \"\"\"Handle tool calls and execute them\"\"\"\n        if isinstance(tool_calls, dict):\n            # Convert dict to list\n            tool_calls_list = []\n            for idx in sorted(tool_calls.keys()):\n                tool_calls_list.append(tool_calls[idx])\n            tool_calls = tool_calls_list\n        \n        metadata.tool_calls = tool_calls\n        \n        print(f\"\\n{Colors.CYAN}üõ†Ô∏è  Tool Calls Detected:{Colors.END}\")\n        for i, tool_call in enumerate(tool_calls, 1):\n            print(f\"   {i}. {tool_call['function']['name']}({tool_call['function']['arguments']})\")\n        \n        # Execute tool calls\n        for tool_call in tool_calls:\n            result = tool_manager.execute_tool(tool_call)\n            metadata.tool_results.append(result)\n\n\n# ============ Result Saving ============\n\nclass ResultSaver:\n    \"\"\"Saves conversation results to files\"\"\"\n\n    @staticmethod\n    def save(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Tuple[Path, Path]:\n        \"\"\"Save results as Markdown and JSON\"\"\"\n        # Create directory if needed\n        base_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # 1. Save Markdown\n        md_path = ResultSaver._save_markdown(base_path, payload, metadata, printer)\n\n        # 2. Save JSON for continuation\n        json_path = ResultSaver._save_json(base_path, payload, metadata, printer)\n\n        return md_path, json_path\n\n    @staticmethod\n    def _save_markdown(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Path:\n        \"\"\"Save conversation as Markdown with full history\"\"\"\n        md_path = base_path.with_suffix(\".md\")\n\n        # Prepare payload for display (hide messages to save space)\n        display_payload = payload.copy()\n        messages = display_payload.pop(\"messages\", [])\n        if messages:\n            display_payload[\"messages\"] = f\"[{len(messages)} messages hidden]\"\n\n        # Build Markdown content\n        md_content = f\"\"\"# AI Conversation Log\n**Time**: {metadata.start_time.strftime('%Y-%m-%d %H:%M:%S')}  \n**Model**: {metadata.model or 'Unknown'}  \n**Duration**: {metadata.duration:.2f}s  \n\n## Request Configuration\n{CODE_BLOCK}json\n{json.dumps(display_payload, indent=2, ensure_ascii=False)}\n{CODE_BLOCK}\n\n## Conversation History\n\n\"\"\"\n\n        # Add all messages\n        for msg in messages:\n            role = msg.get(\"role\", \"unknown\").upper()\n            content = msg.get(\"content\", \"\")\n            \n            # Handle multimodal content\n            if isinstance(content, list):\n                texts = []\n                for item in content:\n                    if isinstance(item, dict):\n                        if item.get(\"type\") == \"text\":\n                            texts.append(item.get(\"text\", \"\"))\n                        elif item.get(\"type\") == \"image_url\":\n                            texts.append(\"[Image]\")\n                content = \"\\n\".join(texts)\n            \n            md_content += f\"### {role}\\n\\n{content}\\n\\n\"\n\n        # Add reasoning if present\n        if printer.reasoning_buffer:\n            md_content += f\"\"\"\n---\n\n## üí≠ Thinking Process\n<details>\n<summary>Click to expand ({len(printer.reasoning_buffer)} chars)</summary>\n\n{printer.reasoning_buffer}\n</details>\n\n\"\"\"\n\n        # Add response\n        md_content += f\"\"\"\n---\n\n## ‚ú® Response\n{printer.content_buffer}\n\n\"\"\"\n\n        # Add tool calls if present\n        if metadata.tool_calls:\n            md_content += f\"\"\"\n---\n\n## üõ†Ô∏è Tool Calls\n\"\"\"\n            for i, tool_call in enumerate(metadata.tool_calls, 1):\n                md_content += f\"\"\"\n### Tool Call {i}\n- **Function**: `{tool_call['function']['name']}`\n- **Arguments**: `{tool_call['function']['arguments']}`\n\"\"\"\n            \n            if metadata.tool_results:\n                md_content += f\"\"\"\n### Tool Results\n\"\"\"\n                for result in metadata.tool_results:\n                    md_content += f\"\"\"\n- **{result['name']}**: {result['content']}\n\"\"\"\n\n        md_content += f\"\"\"\n---\n## üìä Statistics\n\n- **Finish Reason**: {metadata.finish_reason or 'N/A'}\n- **Tokens**: {metadata.total_tokens or 'N/A'} (Prompt: {metadata.prompt_tokens or 'N/A'}, Completion: {metadata.completion_tokens or 'N/A'})\n- **Token Rate**: {f'{metadata.get_token_rate():.1f} tokens/s' if metadata.get_token_rate() else 'N/A'}\n- **First Token Latency**: {f'{(printer.first_token_time - printer.start_time):.2f}s' if printer.first_token_time else 'N/A'}\n- **Estimated Total Tokens**: {printer.token_count}\n\"\"\"\n\n        with open(md_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(md_content)\n\n        return md_path\n\n    @staticmethod\n    def _save_json(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Path:\n        \"\"\"Save conversation as JSON for continuation\"\"\"\n        json_path = base_path.with_suffix(\".json\")\n\n        # Build conversation for continuation\n        continuation = {\n            \"metadata\": {\n                \"export_time\": datetime.now().isoformat(),\n                \"model\": metadata.model,\n                \"total_tokens\": metadata.total_tokens,\n                \"duration\": metadata.duration,\n            },\n            \"messages\": payload.get(\"messages\", []).copy(),\n        }\n\n        # Add assistant response\n        assistant_msg = {\n            \"role\": \"assistant\",\n            \"content\": printer.content_buffer,\n        }\n        if printer.reasoning_buffer:\n            assistant_msg[\"reasoning_content\"] = printer.reasoning_buffer\n        if metadata.tool_calls:\n            assistant_msg[\"tool_calls\"] = metadata.tool_calls\n\n        continuation[\"messages\"].append(assistant_msg)\n\n        # Add tool results as tool messages\n        for result in metadata.tool_results:\n            continuation[\"messages\"].append(result)\n\n        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(continuation, f, ensure_ascii=False, indent=2)\n\n        return json_path\n\n\n# ============ Argument Parser ============\n\ndef create_argument_parser() -> argparse.ArgumentParser:\n    \"\"\"Create enhanced argument parser\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"AI Chat CLI - Enhanced with graceful parameter handling\",\n        formatter_class=argparse.RawTextHelpFormatter,\n        epilog=\"\"\"\nConfiguration Strategy:\n  1. Unified settings via config.json to avoid model-specific parameter issues\n  2. CLI arguments safely override config.json (removes conflicting params)\n  3. profiles.json or .env provides endpoint and api_key\n\nImage Support:\n  Use @filename.png to attach images (png, jpg, jpeg, gif, webp, bmp supported)\n  Example: python chat.py @photo.jpg \"Describe this image\"\n\nToken Estimation:\n  Before sending, the CLI will display estimated prompt tokens and max_tokens\n  to help you avoid context overflow.\n\nTool Support:\n  Define tools in tools.py with proper function signatures and docstrings\n  The CLI will automatically load and make them available to the model\n\nExamples:\n  # Basic usage with default config\n  python chat.py \"Hello, how are you?\"\n  \n  # With file attachment (code or image)\n  python chat.py @code.py \"Explain this code\"\n  python chat.py @image.png \"What's in this image?\"\n  \n  # Custom profile and config\n  python chat.py -p deepseek -f deepseek_config.json \"Explain quantum computing\"\n  \n  # Continue conversation\n  python chat.py --context conversation.json \"Continue from here\"\n  \n  # Start new conversation (don't save to config)\n  python chat.py --new \"Start fresh conversation\"\n  \n  # Disable streaming\n  python chat.py --no-stream \"Generate a long response\"\n  \n  # Override parameters\n  python chat.py --temperature 0.5 --max-tokens 2000 \"Creative writing\"\n        \"\"\",\n    )\n\n    # Connection\n    parser.add_argument(\n        \"--profile\", \"-p\", default=\"default\", help=\"Profile name in profiles.json\"\n    )\n    parser.add_argument(\"--endpoint\", \"-e\", help=\"Override API endpoint\")\n    parser.add_argument(\"--api-key\", \"-k\", help=\"Override API key\")\n\n    # Configuration\n    parser.add_argument(\n        \"--config\", \"-c\", default=\"config.json\", help=\"Request body configuration file\"\n    )\n    parser.add_argument(\n        \"--new\", action=\"store_true\", help=\"Start new conversation (don't save context to config)\"\n    )\n\n    # Input\n    parser.add_argument(\"prompt\", nargs=\"*\", help=\"Prompt text (supports @filename for files/images)\")\n    parser.add_argument(\"--context\", help=\"Conversation context JSON file for continuation\")\n\n    # OpenAI Parameters (override config.json)\n    parser.add_argument(\"--model\", \"-m\", help=\"Model name (overrides config)\")\n    parser.add_argument(\"--temperature\", \"-t\", type=float, help=\"Sampling temperature (0-2)\")\n    parser.add_argument(\"--max-tokens\", type=int, help=\"Maximum tokens to generate\")\n    parser.add_argument(\"--top-p\", type=float, help=\"Nucleus sampling parameter\")\n    parser.add_argument(\"--presence-penalty\", type=float, help=\"Presence penalty\")\n    parser.add_argument(\"--frequency-penalty\", type=float, help=\"Frequency penalty\")\n    parser.add_argument(\"--seed\", type=int, help=\"Random seed for reproducibility\")\n\n    # Behavior (safely override config)\n    parser.add_argument(\n        \"--no-stream\", action=\"store_true\", help=\"Disable streaming output\"\n    )\n    parser.add_argument(\n        \"--enable-thinking\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable thinking process\",\n    )\n    parser.add_argument(\n        \"--no-thinking\",\n        dest=\"enable_thinking\",\n        action=\"store_false\",\n        help=\"Disable thinking process\",\n    )\n\n    # Tools\n    parser.add_argument(\n        \"--tools\", default=\"tools.py\", help=\"Python module containing tool functions\"\n    )\n    parser.add_argument(\n        \"--no-tools\", action=\"store_true\", help=\"Disable tool functions\"\n    )\n\n    # Output\n    parser.add_argument(\"--output\", \"-o\", help=\"Output file base name\")\n\n    return parser\n\n\n# ============ Main Function ============\n\ndef handle_interrupt(signum, frame):\n    \"\"\"Handle Ctrl+C gracefully\"\"\"\n    print(f\"\\n{Colors.warn('‚ö†Ô∏è  Interrupted by user')}\")\n    sys.exit(130)\n\n\ndef main():\n    \"\"\"Main entry point\"\"\"\n    # Setup\n    setup_utf8()\n    signal.signal(signal.SIGINT, handle_interrupt)\n\n    # Parse arguments\n    parser = create_argument_parser()\n    args = parser.parse_args()\n\n    try:\n        # 1. Load connection configuration\n        api_config = ConfigManager.load_connection_config(args)\n        api_config.validate()\n\n        # 2. Load request body from config.json\n        request_body = ConfigManager.load_request_body(args.config, args)\n\n        # 3. Initialize tool manager\n        tool_manager = None\n        if not args.no_tools and Path(args.tools).exists():\n            tool_manager = ToolManager(args.tools)\n            tool_definitions = tool_manager.get_tool_definitions()\n            if tool_definitions:\n                request_body[\"tools\"] = tool_definitions\n                print(f\"{Colors.info('‚úì')} Added {len(tool_definitions)} tools to request\", file=sys.stderr)\n\n        # 4. Build messages\n        messages = MessageBuilder.build_messages(args, request_body)\n        request_body[\"messages\"] = messages\n\n        # 5. Determine output path\n        if args.output:\n            output_base = Path(args.output)\n        else:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            prompt_hint = (\n                args.prompt[0][:20]\n                if args.prompt\n                else request_body.get(\"model\", \"chat\")\n            )\n            safe_name = re.sub(r\"[^\\w\\s-]\", \"\", prompt_hint).strip().replace(\" \", \"_\")\n            output_base = Path(f\"chat_{safe_name}_{timestamp}\")\n\n        # 6. Execute request\n        with open(output_base.with_suffix(\".md\"), \"w\", encoding=\"utf-8\") as out_file:\n            printer = TypewriterPrinter(out_file)\n            \n            with APIClient(api_config) as client:\n                metadata = client.request(request_body, printer, tool_manager)\n            \n            printer.finalize()\n\n            # 7. Add separator before complete message\n            print(f\"\\n{Colors.bold('‚îÄ' * 60)}\")\n\n            # 8. Print statistics\n            print(f\"\\n{Colors.info('‚úÖ Complete')}\")\n            print(f\"   Duration: {metadata.duration:.2f}s\")\n\n            if metadata.total_tokens:\n                print(\n                    f\"   Tokens: {metadata.total_tokens} (Prompt: {metadata.prompt_tokens}, Completion: {metadata.completion_tokens})\"\n                )\n                if token_rate := metadata.get_token_rate():\n                    print(f\"   Token Rate: {token_rate:.1f} tokens/s\")\n\n            if metadata.finish_reason:\n                print(f\"   Finish Reason: {metadata.finish_reason}\")\n\n            if printer.first_token_time:\n                print(\n                    f\"   First Token: {(printer.first_token_time - printer.start_time):.2f}s\"\n                )\n\n            if metadata.tool_calls:\n                print(f\"   Tool Calls: {len(metadata.tool_calls)} executed\")\n\n            # 9. Save results\n            md_path, json_path = ResultSaver.save(\n                output_base, request_body, metadata, printer\n            )\n\n            print(f\"\\n{Colors.info('üíæ Saved:')}\")\n            print(f\"   {md_path}\")\n            print(f\"   {json_path}\")\n            print(\n                f\"\\n{Colors.info('üí° Tip:')} Continue with: --context {json_path.name}\"\n            )\n\n            # 10. Save context to config.json if not --new\n            if not args.new:\n                # Update messages with assistant response\n                updated_messages = messages.copy()\n                \n                # Add assistant message\n                assistant_msg = {\n                    \"role\": \"assistant\",\n                    \"content\": printer.content_buffer,\n                }\n                if printer.reasoning_buffer:\n                    assistant_msg[\"reasoning_content\"] = printer.reasoning_buffer\n                if metadata.tool_calls:\n                    assistant_msg[\"tool_calls\"] = metadata.tool_calls\n                \n                updated_messages.append(assistant_msg)\n                \n                # Add tool results\n                for result in metadata.tool_results:\n                    updated_messages.append(result)\n                \n                # Save to config.json\n                ConfigManager.save_context_to_config(args.config, updated_messages)\n\n    except KeyboardInterrupt:\n        print(f\"\\n{Colors.warn('‚ö†Ô∏è  Interrupted')}\", file=sys.stderr)\n        sys.exit(130)\n    except (ConfigurationError, FileNotFoundError) as e:\n        print(f\"{Colors.error('‚ùå Configuration Error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n    except APIError as e:\n        print(f\"{Colors.error('‚ùå API Error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n    except Exception as e:\n        logger.exception(\"Unexpected error\")\n        print(f\"{Colors.error('‚ùå Unexpected error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n[File: deepseek.py]\n\n```json\n{\n  \"metadata\": {\n    \"export_time\": \"2026-02-06T03:52:29.393673\",\n    \"model\": \"Qwen/Qwen3-8B\",\n    \"total_tokens\": 771,\n    \"duration\": 13.744392\n  },\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"ÈòøÂ∑¥ÈòøÂ∑¥\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\nÊàëÊó†Ê≥ïÁêÜËß£‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùËøô‰∏™ËØçÁöÑÂÖ∑‰ΩìÂê´‰πâÔºåÊÇ®ÊòØÂê¶Âú®Ë°®ËææÊüêÁßçÊÉÖÁª™ÊàñÈúÄË¶ÅÂ∏ÆÂä©ÔºüÂ¶ÇÊûúÈúÄË¶ÅËÆ°ÁÆó„ÄÅÊü•ËØ¢Êó∂Èó¥ÊàñÂ§©Ê∞î‰ø°ÊÅØÔºåËØ∑ÂëäËØâÊàëÂÖ∑‰ΩìÂÜÖÂÆπÔºåÊàë‰ºöÂ∞ΩÂäõÂçèÂä©ÊÇ®„ÄÇ\",\n      \"reasoning_content\": \"\\nOkay, the user just said \\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\". Let me think about how to respond. First, I need to understand what they mean. \\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\" is a term that's often used in Chinese online culture, especially in gaming communities. It's a way to express frustration or exasperation, similar to \\\"arara\\\" in Japanese or \\\"arrgghh\\\" in English. People might use it when they're upset about something in a game, like a loss or a difficult situation.\\n\\nNow, looking at the tools provided, there are three functions: calculate, get_current_time, and search_weather. The user's message doesn't seem to involve any of these. They didn't ask for a math calculation, didn't mention a timezone for the current time, and didn't request weather info for a city. So, none of the tools are applicable here. \\n\\nI should check if there's any hidden request or if the user is testing me. But \\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\" is just an expression. Maybe they want me to acknowledge their frustration or explain the term? However, since the tools don't cover that, I need to respond without using any function calls. \\n\\nThe correct approach is to inform the user that I can't assist with their current message and ask how I can help. That way, I stay within the provided tools and offer further support if needed.\\n\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"‰ªäÊó•Â§©Ê∞î\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\nËØ∑ÂëäËØâÊàëÊÇ®ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞îÔºåÊàëÂ∞Ü‰∏∫ÊÇ®ÊêúÁ¥¢Áõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ\",\n      \"reasoning_content\": \"\\nÂ•ΩÁöÑÔºåÁî®Êà∑Áé∞Âú®ÈóÆÁöÑÊòØ‚Äú‰ªäÊó•Â§©Ê∞î‚ÄùÔºåÁúãËµ∑Êù•‰ªñ‰ª¨ÊÉ≥‰∫ÜËß£‰ªäÂ§©ÁöÑÂ§©Ê∞îÊÉÖÂÜµ„ÄÇ‰∏çËøá‰πãÂâçÁöÑÂØπËØù‰∏≠ÔºåÁî®Êà∑ËØ¥‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùÔºåÊàëÊ≤°ËÉΩÂáÜÁ°ÆÁêÜËß£‰ªñ‰ª¨ÁöÑÊÑèÊÄù„ÄÇÂèØËÉΩÁî®Êà∑ÊòØÂú®ÊµãËØïÊàñËÄÖÁî®ÊüêÁßçÊöóÂè∑Ôºå‰ΩÜÂêéÊù•‰ªñ‰ª¨Áõ¥Êé•ÈóÆÂ§©Ê∞î‰∫Ü„ÄÇ\\n\\nÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁ°ÆÂÆöÁî®Êà∑ÊòØÂê¶ÈúÄË¶ÅÊü•ËØ¢Â§©Ê∞î„ÄÇÊ†πÊçÆÂΩìÂâçÁöÑÂ∑•ÂÖ∑ÔºåÊúâ‰∏Ä‰∏™search_weatherÂáΩÊï∞ÔºåÈúÄË¶ÅÂèÇÊï∞ÊòØÂüéÂ∏Ç„ÄÇÁî®Êà∑Ê≤°ÊúâÊèêÂà∞ÂÖ∑‰ΩìÂüéÂ∏ÇÔºåÊâÄ‰ª•ÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•ËØ¢ÈóÆ„ÄÇ‰ΩÜÊúâÊó∂ÂÄôÁî®Êà∑ÂèØËÉΩÈªòËÆ§Êü•ËØ¢ÊâÄÂú®ÂüéÂ∏ÇÔºå‰∏çËøáËøôÈáåÊ≤°Êúâ‰∏ä‰∏ãÊñáÔºåÊâÄ‰ª•‰∏çËÉΩÁ°ÆÂÆö„ÄÇ\\n\\nÂè¶Â§ñÔºåÁî®Êà∑ÂèØËÉΩÊòØÂú®ÁªßÁª≠‰πãÂâçÁöÑÂØπËØùÔºå‰ΩÜ‰πãÂâçÁöÑ‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùÊ≤°ÊúâÊòéÁ°Æ‰ø°ÊÅØ„ÄÇÊâÄ‰ª•ÊúÄÂ•ΩËøòÊòØÂÖàÁ°ÆËÆ§Áî®Êà∑ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞î„ÄÇ‰∏çËøá‰πüÊúâÂèØËÉΩÁî®Êà∑Âè™ÊòØÁ™ÅÁÑ∂ÊÉ≥‰∫ÜËß£Â§©Ê∞îÔºåÊ≤°ÊúâÊèêÂà∞ÂüéÂ∏ÇÔºåËøôÊó∂ÂÄôÂèØËÉΩÈúÄË¶ÅÊèêÁ§∫‰ªñ‰ª¨Êèê‰æõÂüéÂ∏ÇÂêçÁß∞„ÄÇ\\n\\nÊ£ÄÊü•Â∑•ÂÖ∑‰∏≠ÁöÑÂáΩÊï∞Ôºåsearch_weatherÁöÑÂèÇÊï∞ÊòØcityÔºåÂøÖÂ°´„ÄÇÊâÄ‰ª•ÂøÖÈ°ªËé∑ÂèñÂüéÂ∏Ç‰ø°ÊÅØÊâçËÉΩË∞ÉÁî®„ÄÇÂõ†Ê≠§ÔºåÊàëÂ∫îËØ•ÂõûÂ§çÁî®Êà∑ÔºåËØ¢ÈóÆ‰ªñ‰ª¨ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞î„ÄÇËøôÊ†∑Êó¢Á¨¶ÂêàÂ∑•ÂÖ∑ÁöÑË¶ÅÊ±ÇÔºå‰πüËÉΩÁ°Æ‰øùÊèê‰æõÂáÜÁ°ÆÁöÑ‰ø°ÊÅØ„ÄÇ\\n\\nÂè¶Â§ñÔºåÊòØÂê¶ÊúâÂÖ∂‰ªñÂèØËÉΩÔºüÊØîÂ¶ÇÁî®Êà∑ÂèØËÉΩËØØËæìÂÖ•ÔºåÊàñËÄÖÊúâÂÖ∂‰ªñÊÑèÂõæÔºü‰ΩÜÊ†πÊçÆÁé∞ÊúâÂ∑•ÂÖ∑ÔºåÂè™ÊúâÂ§©Ê∞î„ÄÅËÆ°ÁÆóÊó∂Èó¥ÂíåËé∑ÂèñÊó∂Èó¥ÁöÑÂäüËÉΩ„ÄÇÊâÄ‰ª•ÊúÄÂêàÁêÜÁöÑÂõûÂ∫îÊòØËØ∑Ê±ÇÂüéÂ∏ÇÂêçÁß∞„ÄÇ\\n\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"ÂçóÊûÅ\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\n\",\n      \"reasoning_content\": \"\\nÂ•ΩÁöÑÔºåÁî®Êà∑Áé∞Âú®ÊèêÂà∞‚ÄúÂçóÊûÅ‚ÄùÔºåÈúÄË¶ÅÊü•ËØ¢ÈÇ£ÈáåÁöÑÂ§©Ê∞î„ÄÇÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁ°ÆËÆ§Áî®Êà∑ÊòØÂê¶ÁúüÁöÑÊÉ≥Êü•ËØ¢ÂçóÊûÅÁöÑÂ§©Ê∞î„ÄÇÂçóÊûÅÊòØ‰∏Ä‰∏™Â§ßÊ¥≤ÔºåÂ§©Ê∞îÊÉÖÂÜµÂèØËÉΩÊØîËæÉÁâπÊÆäÔºåËÄå‰∏îÁî®Êà∑ÂèØËÉΩÊ≤°ÊúâÊÑèËØÜÂà∞ÈúÄË¶ÅÊåáÂÆöÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ‰∏çËøáÔºåÊ†πÊçÆ‰πãÂâçÁöÑÂØπËØùÔºåÁî®Êà∑ÂèØËÉΩÊòØÂú®ÊµãËØïÊàñËÄÖÊúâÂÖ∂‰ªñÊÑèÂõæÔºå‰ΩÜÊ†πÊçÆÂ∑•ÂÖ∑‰∏≠ÁöÑsearch_weatherÂáΩÊï∞ÔºåÂèÇÊï∞ÊòØÂüéÂ∏ÇÔºåÊâÄ‰ª•ÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•Á°ÆËÆ§„ÄÇ\\n\\n‰∏çËøáÔºåÁî®Êà∑‰πãÂâçÂ∑≤ÁªèÈóÆËøá‚Äú‰ªäÊó•Â§©Ê∞î‚ÄùÔºåÁÑ∂ÂêéÊàëËÆ©‰ªñÊåáÂÆöÂüéÂ∏ÇÔºåÁé∞Âú®‰ªñÁõ¥Êé•ÂõûÁ≠î‚ÄúÂçóÊûÅ‚ÄùÔºåÂèØËÉΩÊòØÊåáÊï¥‰∏™ÂçóÊûÅÊ¥≤ÁöÑÂ§©Ê∞îÊÉÖÂÜµ„ÄÇ‰ΩÜÂçóÊûÅÊ¥≤Ê≤°ÊúâÁªü‰∏ÄÁöÑÂüéÂ∏ÇÔºåÂèØËÉΩÊúâÁßëÁ†îÁ´ôÔºåÊØîÂ¶ÇÈ∫¶ÂÖãÈªòÂ§öÁ´ô„ÄÅÈïøÂüéÁ´ôÁ≠â„ÄÇËøôÊó∂ÂÄôÊàëÈúÄË¶ÅÂà§Êñ≠ÊòØÂê¶Â∫îËØ•Áõ¥Êé•‰ΩøÁî®‚ÄúÂçóÊûÅ‚Äù‰Ωú‰∏∫ÂüéÂ∏ÇÂèÇÊï∞ÔºåÊàñËÄÖÊòØÂê¶ÈúÄË¶ÅÊèêÁ§∫Áî®Êà∑ËæìÂÖ•ÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ\\n\\nÊ†πÊçÆÂ∑•ÂÖ∑ÁöÑÊèèËø∞Ôºåsearch_weatherÂáΩÊï∞ÁöÑÂèÇÊï∞ÊòØcityÔºåÁ±ªÂûã‰∏∫Â≠óÁ¨¶‰∏≤Ôºå‰ΩÜÂèØËÉΩËØ•ÂáΩÊï∞ÂÜÖÈÉ®Â§ÑÁêÜ‰∫ÜÁ±ª‰ºº‚ÄúÂçóÊûÅ‚ÄùÁöÑËæìÂÖ•ÔºåÊàñËÄÖÁî®Êà∑ÂèØËÉΩÊúüÊúõÂæóÂà∞ÂçóÊûÅÊ¥≤ÁöÑÂ§©Ê∞îÊ¶ÇÂÜµ„ÄÇ‰πüÊúâÂèØËÉΩÁî®Êà∑ËæìÂÖ•ÊúâËØØÔºåÊàñËÄÖÊÉ≥‰∫ÜËß£Êüê‰∏™ÁâπÂÆöÂüéÂ∏ÇÁöÑÂ§©Ê∞îÔºå‰ΩÜËØØÂÜô‰∏∫ÂçóÊûÅ„ÄÇ‰æãÂ¶ÇÔºåÁî®Êà∑ÂèØËÉΩÊÉ≥Êü•Âåó‰∫¨„ÄÅ‰∏äÊµ∑Á≠âÂüéÂ∏ÇÔºå‰ΩÜÈîôËØØËæìÂÖ•‰∫Ü‚ÄúÂçóÊûÅ‚Äù„ÄÇ\\n\\nËøôÊó∂ÂÄôÔºåÊàëÈúÄË¶ÅÂÖàË∞ÉÁî®search_weatherÂáΩÊï∞ÔºåÂ∞ùËØï‰ΩøÁî®‚ÄúÂçóÊûÅ‚Äù‰Ωú‰∏∫cityÂèÇÊï∞ÔºåÁúãÁúãÊòØÂê¶ËÉΩËøîÂõûÊúâÊïàÁªìÊûú„ÄÇÂ¶ÇÊûúËøîÂõûÈîôËØØÔºåÂèØËÉΩÈúÄË¶ÅÊèêÁ§∫Áî®Êà∑Á°ÆËÆ§ÂüéÂ∏ÇÂêçÁß∞„ÄÇ‰ΩÜÊ†πÊçÆÂΩìÂâçÂ∑•ÂÖ∑ÁöÑËÆæÂÆöÔºåÂèØËÉΩÈúÄË¶ÅÁõ¥Êé•Ë∞ÉÁî®ÂáΩÊï∞ÔºåÁÑ∂ÂêéÊ†πÊçÆÁªìÊûúÂ§ÑÁêÜ„ÄÇÂ¶ÇÊûúÂáΩÊï∞ÊîØÊåÅÂú∞Âå∫Êü•ËØ¢ÔºåÂèØËÉΩÊ≤°ÈóÆÈ¢òÔºõÂ¶ÇÊûú‰∏çÊîØÊåÅÔºåÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•ÊæÑÊ∏Ö„ÄÇ\\n\\nÂõ†Ê≠§ÔºåÊ≠£Á°ÆÁöÑÂÅöÊ≥ïÊòØÂÖàË∞ÉÁî®search_weatherÂáΩÊï∞ÔºåÂèÇÊï∞‰∏∫city: \\\"ÂçóÊûÅ\\\"ÔºåÁÑ∂ÂêéÊ†πÊçÆËøîÂõûÁöÑÊï∞ÊçÆÂÜ≥ÂÆöÊòØÂê¶ÈúÄË¶ÅËøõ‰∏ÄÊ≠•ËØ¢ÈóÆÁî®Êà∑ÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ‰ΩÜÊ†πÊçÆÁî®Êà∑‰πãÂâçÁöÑÂØπËØùÔºå‰ªñ‰ª¨ÂèØËÉΩÂè™ÊòØÊÉ≥‰∫ÜËß£ÂçóÊûÅÁöÑÂ§©Ê∞îÊÉÖÂÜµÔºåÊâÄ‰ª•Áõ¥Êé•Ë∞ÉÁî®ÂáΩÊï∞ÊòØÂêàÈÄÇÁöÑ„ÄÇ\\n\",\n      \"tool_calls\": [\n        {\n          \"id\": null,\n          \"type\": \"function\",\n          \"function\": {\n            \"name\": \"search_weather\",\n            \"arguments\": \" {\\\"city\\\": \\\"ÂçóÊûÅ\\\"}\"\n          }\n        }\n      ]\n    },\n    {\n      \"tool_call_id\": null,\n      \"role\": \"tool\",\n      \"name\": \"search_weather\",\n      \"content\": \"\\\"Weather in ÂçóÊûÅ: Sunny, 25¬∞C\\\"\"\n    }\n  ]\n}\n```\n[File: chat_ÂçóÊûÅ_20260206_035215.json]\n\nÂú®Ëøô‰∏™tool call‰πãÂêéÔºåËøîÂõûtool_calls stop reason,‰ΩÜÊòØ‰Ωú‰∏∫contextËæìÂÖ•ÂõûÂéªÁöÑÊó∂ÂÄôÔºåÂìçÂ∫î400ÔºåÂÖ∑‰ΩìÂÜÖÂÆπ‰πàÔºåÂõ†‰∏∫pythonËÑöÊú¨‰∏çÊâìÂç∞ÔºåÊâÄ‰ª•Êàë‰πü‰∏çÁü•ÈÅì„ÄÇapiÂèØËÉΩËøîÂõû‰∫Ü‰πüÂèØËÉΩÊ≤°ËøîÂõûÔºåÊàë‰∏çÁü•ÈÅì„ÄÇÊÄª‰πã‰Ω†ÊÉ≥ÊÉ≥ÂäûÊ≥ïdebug‰∏ã"
    },
    {
      "role": "assistant",
      "content": "Ê†πÊçÆÊÇ®Êèê‰æõÁöÑÂØπËØùÂéÜÂè≤ÂíåÈîôËØØÊèèËø∞ÔºåÈóÆÈ¢òÂæàÂèØËÉΩÂá∫Áé∞Âú® **Â∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫null** ‰∏ä„ÄÇÂú®OpenAIÂÖºÂÆπÁöÑAPI‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®ÂøÖÈ°ªÂåÖÂê´ÂîØ‰∏ÄÁöÑIDÔºåÂ∑•ÂÖ∑ÂìçÂ∫î‰πüÂøÖÈ°ªÂºïÁî®Áõ∏ÂêåÁöÑID„ÄÇÂΩìID‰∏∫nullÊó∂ÔºåAPI‰ºöËøîÂõû400ÈîôËØØ„ÄÇ\n\n## ÈóÆÈ¢òÂàÜÊûê\n\n‰ªé `chat_ÂçóÊûÅ_20260206_035215.json` ÂèØ‰ª•ÁúãÂà∞Ôºö\n```json\n{\n  \"tool_call_id\": null,\n  \"role\": \"tool\",\n  \"name\": \"search_weather\",\n  \"content\": \"\\\"Weather in ÂçóÊûÅ: Sunny, 25¬∞C\\\"\"\n}\n```\n\n`tool_call_id` ‰∏∫ `null`ÔºåËøôËøùÂèç‰∫ÜAPIËßÑËåÉ„ÄÇÂΩìËøô‰∏™ÂØπËØùÂéÜÂè≤‰Ωú‰∏∫‰∏ä‰∏ãÊñáÂÜçÊ¨°ÂèëÈÄÅÊó∂ÔºåAPI‰ºöÊãíÁªùËØ∑Ê±Ç„ÄÇ\n\n## Ëß£ÂÜ≥ÊñπÊ°à\n\n‰øÆÊîπ `deepseek.py` ‰∏≠ÁöÑ `ToolManager.execute_tool` ÊñπÊ≥ïÔºåÂú®Â∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÊó∂Ëá™Âä®ÁîüÊàêÂîØ‰∏ÄID„ÄÇ\n\n### ‰øÆÊîπÂêéÁöÑ `ToolManager.execute_tool` ÊñπÊ≥ïÔºö\n\n```python\ndef execute_tool(self, tool_call: Dict) -> Dict:\n    \"\"\"Execute a tool call and return result\"\"\"\n    try:\n        function_name = tool_call[\"function\"][\"name\"]\n        function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n        \n        if function_name not in self.available_functions:\n            raise ToolError(f\"Unknown function: {function_name}\")\n        \n        print(f\"\\n{Colors.CYAN}üîß Executing tool: {function_name}{Colors.END}\", file=sys.stderr)\n        print(f\"   Arguments: {function_args}\", file=sys.stderr)\n        \n        # Execute the function\n        result = self.available_functions[function_name](**function_args)\n        \n        print(f\"   Result: {result}\", file=sys.stderr)\n        \n        # ‰øÆÂ§çÔºöÂ§ÑÁêÜnullÊàñÁ©∫ÁöÑtool_call_id\n        tool_call_id = tool_call.get(\"id\")\n        if not tool_call_id:\n            import uuid\n            tool_call_id = str(uuid.uuid4())\n            print(f\"{Colors.warn('‚ö†')} Generated tool call ID: {tool_call_id}\", file=sys.stderr)\n        \n        return {\n            \"tool_call_id\": tool_call_id,\n            \"role\": \"tool\",\n            \"name\": function_name,\n            \"content\": json.dumps(result, ensure_ascii=False)\n        }\n        \n    except Exception as e:\n        error_msg = f\"Tool execution failed: {str(e)}\"\n        print(f\"{Colors.error('‚ùå')} {error_msg}\", file=sys.stderr)\n        \n        # ‰øÆÂ§çÔºöÈîôËØØÂìçÂ∫î‰πüÁîüÊàêID\n        tool_call_id = tool_call.get(\"id\")\n        if not tool_call_id:\n            import uuid\n            tool_call_id = str(uuid.uuid4())\n        \n        return {\n            \"tool_call_id\": tool_call_id,\n            \"role\": \"tool\",\n            \"name\": tool_call[\"function\"][\"name\"],\n            \"content\": json.dumps({\"error\": error_msg}, ensure_ascii=False)\n        }\n```\n\n### È¢ùÂ§ñ‰øÆÂ§çÔºöÁ°Æ‰øù‰øùÂ≠ò‰∏ä‰∏ãÊñáÊó∂ID‰∏ç‰∏∫null\n\nÂú® `ResultSaver._save_json` ÊñπÊ≥ï‰∏≠ÔºåÊ∑ªÂä†IDÈ™åËØÅÔºö\n\n```python\n@staticmethod\ndef _save_json(\n    base_path: Path,\n    payload: Dict[str, Any],\n    metadata: RequestMetadata,\n    printer: TypewriterPrinter,\n) -> Path:\n    \"\"\"Save conversation as JSON for continuation\"\"\"\n    json_path = base_path.with_suffix(\".json\")\n\n    # Build conversation for continuation\n    continuation = {\n        \"metadata\": {\n            \"export_time\": datetime.now().isoformat(),\n            \"model\": metadata.model,\n            \"total_tokens\": metadata.total_tokens,\n            \"duration\": metadata.duration,\n        },\n        \"messages\": payload.get(\"messages\", []).copy(),\n    }\n\n    # Add assistant response\n    assistant_msg = {\n        \"role\": \"assistant\",\n        \"content\": printer.content_buffer,\n    }\n    if printer.reasoning_buffer:\n        assistant_msg[\"reasoning_content\"] = printer.reasoning_buffer\n    if metadata.tool_calls:\n        # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_calls‰∏≠ÁöÑID‰∏ç‰∏∫null\n        for tool_call in metadata.tool_calls:\n            if not tool_call.get(\"id\"):\n                import uuid\n                tool_call[\"id\"] = str(uuid.uuid4())\n        assistant_msg[\"tool_calls\"] = metadata.tool_calls\n\n    continuation[\"messages\"].append(assistant_msg)\n\n    # Add tool results as tool messages\n    for result in metadata.tool_results:\n        # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_call_id‰∏ç‰∏∫null\n        if not result.get(\"tool_call_id\"):\n            import uuid\n            result[\"tool_call_id\"] = str(uuid.uuid4())\n        continuation[\"messages\"].append(result)\n\n    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(continuation, f, ensure_ascii=False, indent=2)\n\n    return json_path\n```\n\n## Â¶Ç‰Ωï‰ΩøÁî®‰øÆÂ§ç\n\n1. **Â§á‰ªΩÂéüÊñá‰ª∂**Ôºö\n   ```bash\n   cp deepseek.py deepseek.py.backup\n   ```\n\n2. **Â∫îÁî®‰øÆÂ§ç**Ôºö\n   - ÊâìÂºÄ `deepseek.py`\n   - ÊâæÂà∞ `ToolManager.execute_tool` ÊñπÊ≥ïÔºàÂ§ßÁ∫¶Âú®Á¨¨200Ë°åÔºâ\n   - Áî®‰∏äÈù¢ÁöÑ‰øÆÂ§ç‰ª£Á†ÅÊõøÊç¢\n   - ÂèØÈÄâÔºöÊâæÂà∞ `ResultSaver._save_json` ÊñπÊ≥ïÂπ∂Â∫îÁî®Á¨¨‰∫å‰∏™‰øÆÂ§ç\n\n3. **ÈáçÊñ∞ËøêË°åÂØπËØù**Ôºö\n   ```bash\n   python deepseek.py --context chat_ÂçóÊûÅ_20260206_035215.json \"ÁªßÁª≠\"\n   ```\n\n## È™åËØÅ‰øÆÂ§ç\n\n‰øÆÂ§çÂêéÔºåÊñ∞ÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®Â∞Ü‰ºöÊúâÂîØ‰∏ÄÁöÑIDÔºå‰øùÂ≠òÁöÑJSONÊñá‰ª∂‰∏≠ÁöÑ `tool_call_id` ‰πü‰∏ç‰ºöÊòØnull„ÄÇËøôÊ†∑ÂΩìÊÇ®ÂÜçÊ¨°‰ΩøÁî® `--context` ÂèÇÊï∞Âä†ËΩΩÂØπËØùÂéÜÂè≤Êó∂ÔºåAPIÂ∞±‰∏ç‰ºöËøîÂõû400ÈîôËØØ‰∫Ü„ÄÇ\n\n## ‰∏¥Êó∂Ëß£ÂÜ≥ÊñπÊ°à\n\nÂ¶ÇÊûúÊÇ®‰∏çÊÉ≥‰øÆÊîπ‰ª£Á†ÅÔºåÂèØ‰ª•ÊâãÂä®ÁºñËæëJSONÊñá‰ª∂ÔºåÂ∞Ü `tool_call_id` ‰ªé `null` Êîπ‰∏∫‰∏Ä‰∏™ÂîØ‰∏ÄÂÄºÔºö\n\n```json\n{\n  \"tool_call_id\": \"generated_id_123\",\n  \"role\": \"tool\",\n  \"name\": \"search_weather\",\n  \"content\": \"\\\"Weather in ÂçóÊûÅ: Sunny, 25¬∞C\\\"\"\n}\n```\n\n‰ΩÜÂª∫ËÆÆ‰ΩøÁî®‰ª£Á†Å‰øÆÂ§çÔºåËøôÊ†∑ÂèØ‰ª•Ëá™Âä®Â§ÑÁêÜÊâÄÊúâÊú™Êù•ÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®„ÄÇ",
      "reasoning_content": "Áî®Êà∑Êèê‰æõ‰∫Ü‰∏Ä‰∏™PythonËÑöÊú¨Ôºàdeepseek.pyÔºâÂíå‰∏Ä‰∏™JSONÊñá‰ª∂Ôºàchat_ÂçóÊûÅ_20260206_035215.jsonÔºâ„ÄÇËÑöÊú¨ÊòØ‰∏Ä‰∏™AIËÅäÂ§©CLIÂ∑•ÂÖ∑ÔºåÁî®‰∫é‰∏éOpenAIÂÖºÂÆπÁöÑAPI‰∫§‰∫í„ÄÇJSONÊñá‰ª∂ÊòØÂØπËØùÂéÜÂè≤ËÆ∞ÂΩïÔºåÂåÖÂê´Áî®Êà∑ÂíåÂä©Êâã‰πãÈó¥ÁöÑÊ∂àÊÅØÔºå‰ª•ÂèäÂ∑•ÂÖ∑Ë∞ÉÁî®„ÄÇ\n\nÁî®Êà∑ÊèêÂà∞ÔºåÂú®Â∑•ÂÖ∑Ë∞ÉÁî®‰πãÂêéÔºåËøîÂõû‰∫Ü‚Äútool_calls stop reason‚ÄùÔºå‰ΩÜÊòØÂΩì‰Ωú‰∏∫‰∏ä‰∏ãÊñáËæìÂÖ•ÂõûÂéªÊó∂ÔºåÂìçÂ∫îÊòØ400„ÄÇÁî®Êà∑‰∏çÁü•ÈÅìÂÖ∑‰ΩìÂÜÖÂÆπÔºåÂõ†‰∏∫PythonËÑöÊú¨Ê≤°ÊúâÊâìÂç∞Âá∫Êù•„ÄÇÁî®Êà∑Â∏åÊúõÊàëÂ∏ÆÂøôË∞ÉËØïËøô‰∏™ÈóÆÈ¢ò„ÄÇ\n\nÊàëÈúÄË¶ÅÂàÜÊûêËøô‰∏™ÈóÆÈ¢ò„ÄÇ‰ªéJSONÊñá‰ª∂Êù•ÁúãÔºåÂØπËØùÂéÜÂè≤Â¶Ç‰∏ãÔºö\n\n1. Áî®Êà∑Ôºö‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚Äù\n2. Âä©ÊâãÔºöÂõûÂ∫î‰∏çÁêÜËß£ÔºåÂπ∂ËØ¢ÈóÆÊòØÂê¶ÈúÄË¶ÅÂ∏ÆÂä©„ÄÇ\n3. Áî®Êà∑Ôºö‚Äú‰ªäÊó•Â§©Ê∞î‚Äù\n4. Âä©ÊâãÔºöËØ¢ÈóÆÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ\n5. Áî®Êà∑Ôºö‚ÄúÂçóÊûÅ‚Äù\n6. Âä©ÊâãÔºöÊÄùËÄÉÂêéÂÜ≥ÂÆöË∞ÉÁî®search_weatherÂ∑•ÂÖ∑ÔºåÂèÇÊï∞‰∏∫city: \"ÂçóÊûÅ\"„ÄÇ\n7. Â∑•ÂÖ∑Ë∞ÉÁî®ÁªìÊûúÔºöËøîÂõû‰∫Ü‚ÄúWeather in ÂçóÊûÅ: Sunny, 25¬∞C‚Äù„ÄÇ\n\nÁé∞Âú®ÔºåÈóÆÈ¢òÂá∫Áé∞Âú®Â∑•ÂÖ∑Ë∞ÉÁî®‰πãÂêé„ÄÇÂΩìËøô‰∏™ÂØπËØùÂéÜÂè≤‰Ωú‰∏∫‰∏ä‰∏ãÊñáËæìÂÖ•ÂõûAPIÊó∂ÔºåÂèØËÉΩÂá∫Áé∞‰∫Ü400ÈîôËØØ„ÄÇËøôÂèØËÉΩÊòØÂõ†‰∏∫Ê∂àÊÅØÊ†ºÂºè‰∏çÊ≠£Á°ÆÔºåÊàñËÄÖÂ∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÂìçÂ∫îÊ†ºÂºè‰∏çÁ¨¶ÂêàAPIÁöÑÊúüÊúõ„ÄÇ\n\nËÆ©Êàë‰ªîÁªÜÁúãÁúãJSON‰∏≠ÁöÑÊ∂àÊÅØÁªìÊûÑÔºö\n\n- Áî®Êà∑Ê∂àÊÅØÔºöÂåÖÂê´‚Äúrole‚ÄùÂíå‚Äúcontent‚Äù„ÄÇ\n- Âä©ÊâãÊ∂àÊÅØÔºöÂåÖÂê´‚Äúrole‚Äù„ÄÅ‚Äúcontent‚Äù„ÄÅ‚Äúreasoning_content‚ÄùÔºåÊúâÊó∂ËøòÊúâ‚Äútool_calls‚Äù„ÄÇ\n- Â∑•ÂÖ∑Ê∂àÊÅØÔºöÂåÖÂê´‚Äútool_call_id‚Äù„ÄÅ‚Äúrole‚Äù„ÄÅ‚Äúname‚Äù„ÄÅ‚Äúcontent‚Äù„ÄÇ\n\nÂú®JSON‰∏≠ÔºåÊúâ‰∏Ä‰∏™Â∑•ÂÖ∑Ê∂àÊÅØÔºåÂÖ∂‚Äútool_call_id‚Äù‰∏∫null„ÄÇËøôÂèØËÉΩÊòØÈóÆÈ¢òÊâÄÂú®ÔºöÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÔºåËøôÂèØËÉΩÂØºËá¥APIÊãíÁªùËØ∑Ê±Ç„ÄÇ\n\nÂú®OpenAIÁöÑAPI‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®ÈúÄË¶ÅÊúâ‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑIDÔºåÂπ∂‰∏îÂ∑•ÂÖ∑ÂìçÂ∫îÂøÖÈ°ªÂºïÁî®Áõ∏ÂêåÁöÑID„ÄÇÂ¶ÇÊûúID‰∏∫nullÊàñÁº∫Â§±ÔºåAPIÂèØËÉΩ‰ºöËøîÂõû400ÈîôËØØ„ÄÇ\n\nÊ≠§Â§ñÔºåÂä©ÊâãÊ∂àÊÅØ‰∏≠ÁöÑ‚Äútool_calls‚ÄùÊï∞ÁªÑÂåÖÂê´‰∏Ä‰∏™ID‰∏∫nullÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®„ÄÇËøô‰πü‰∏çÁ¨¶ÂêàËßÑËåÉ„ÄÇ\n\nÂõ†Ê≠§ÔºåÈóÆÈ¢òÂèØËÉΩÂú®‰∫éÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫null„ÄÇÊàë‰ª¨ÈúÄË¶ÅÁ°Æ‰øùÂ∑•ÂÖ∑Ë∞ÉÁî®ÊúâÂîØ‰∏ÄÁöÑIDÔºåÂπ∂‰∏îÂ∑•ÂÖ∑ÂìçÂ∫î‰∏é‰πãÂåπÈÖç„ÄÇ\n\nÂè¶Â§ñÔºåPythonËÑöÊú¨ÂèØËÉΩÊ≤°ÊúâÊ≠£Á°ÆÂ§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®ÁöÑID„ÄÇÂú®ËÑöÊú¨‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®IDÊòØ‰ªéAPIÂìçÂ∫î‰∏≠Ëé∑ÂèñÁöÑÔºå‰ΩÜÂèØËÉΩÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ã‰∏∫null„ÄÇËøôÈúÄË¶ÅÊ£ÄÊü•ËÑöÊú¨‰∏≠Â§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÈÉ®ÂàÜ„ÄÇ\n\nÁÑ∂ËÄåÔºåÁî®Êà∑Ê≤°ÊúâÊèê‰æõÂÆåÊï¥ÁöÑÈîôËØØ‰ø°ÊÅØÔºåÊâÄ‰ª•ÊàëÂè™ËÉΩÊé®Êµã„ÄÇ\n\n‰∏∫‰∫ÜË∞ÉËØïÔºåÊàëÂèØ‰ª•Âª∫ËÆÆÁî®Êà∑Ê£ÄÊü•APIÂìçÂ∫î‰∏≠Â∑•ÂÖ∑Ë∞ÉÁî®ÁöÑIDÔºåÂπ∂Á°Æ‰øùÂú®Â∑•ÂÖ∑ÂìçÂ∫î‰∏≠Ê≠£Á°ÆËÆæÁΩÆ„ÄÇÊ≠§Â§ñÔºåÂèØ‰ª•Â∞ùËØï‰øÆÊîπËÑöÊú¨ÔºåÁ°Æ‰øùÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏ç‰∏∫null„ÄÇ\n\n‰ΩÜÁî®Êà∑ÂèØËÉΩÂ∏åÊúõÊàëÁõ¥Êé•‰øÆÂ§çËÑöÊú¨„ÄÇÊàëÂèØ‰ª•ÂàÜÊûêËÑöÊú¨‰∏≠Â§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÈÉ®ÂàÜÔºåÊâæÂá∫ÊΩúÂú®ÈóÆÈ¢ò„ÄÇ\n\nËÆ©ÊàëÁúãÁúãËÑöÊú¨‰∏≠‰∏éÂ∑•ÂÖ∑Ë∞ÉÁî®Áõ∏ÂÖ≥ÁöÑÈÉ®ÂàÜÔºö\n\nÂú®APIClientÁ±ªÁöÑ`_process_stream_data`ÊñπÊ≥ï‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®Ë¢´Êî∂ÈõÜÂà∞`tool_calls_buffer`‰∏≠„ÄÇIDÊòØ‰ªé`tool_call[\"id\"]`Ëé∑ÂèñÁöÑ„ÄÇÂ¶ÇÊûúAPIËøîÂõûÁöÑID‰∏∫nullÔºåÈÇ£‰πàËÑöÊú¨‰∏≠Â≠òÂÇ®ÁöÑIDÂ∞±ÊòØnull„ÄÇ\n\nÂú®`_handle_tool_calls`ÊñπÊ≥ï‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®Ë¢´ÊâßË°åÔºåÁªìÊûúË¢´Ê∑ªÂä†Âà∞`metadata.tool_results`‰∏≠„ÄÇËøôÈáåÔºå`result`ÂåÖÂê´`\"tool_call_id\": tool_call[\"id\"]`ÔºåÂ¶ÇÊûúID‰∏∫nullÔºåÈÇ£‰πàÁªìÊûú‰∏≠ÁöÑID‰πüÊòØnull„ÄÇ\n\nÂΩì‰øùÂ≠ò‰∏∫JSONÊó∂ÔºåÂ∑•ÂÖ∑Ê∂àÊÅØÁöÑ`\"tool_call_id\"`‰∏∫null„ÄÇ\n\nÂΩìËøô‰∏™JSON‰Ωú‰∏∫‰∏ä‰∏ãÊñáÂä†ËΩΩÊó∂ÔºåAPIÂèØËÉΩÂõ†‰∏∫ID‰∏∫nullËÄåÊãíÁªù„ÄÇ\n\nËß£ÂÜ≥ÊñπÊ°àÔºöÁ°Æ‰øùÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏ç‰∏∫null„ÄÇÂ¶ÇÊûúAPIËøîÂõûnullÔºåÊàë‰ª¨ÂèØ‰ª•ÁîüÊàê‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑID„ÄÇÊàñËÄÖÔºåÂú®‰øùÂ≠ò‰∏ä‰∏ãÊñáÊó∂ÔºåÊàë‰ª¨ÂèØ‰ª•ÂøΩÁï•ID‰∏∫nullÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®Ôºü‰ΩÜËøôÊ†∑ÂèØËÉΩÁ†¥ÂùèÂØπËØùÁöÑËøûÁª≠ÊÄß„ÄÇ\n\nÊõ¥Â•ΩÁöÑÊñπÊ≥ïÊòØÔºöÂú®Â∑•ÂÖ∑Ë∞ÉÁî®ÊâßË°åÂêéÔºåÁ°Æ‰øùÁªìÊûú‰∏≠ÁöÑ`tool_call_id`‰∏éÂéüÂßãË∞ÉÁî®ÂåπÈÖç„ÄÇÂ¶ÇÊûúÂéüÂßãID‰∏∫nullÔºåÊàë‰ª¨ÂèØ‰ª•ÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫ID„ÄÇ\n\n‰ΩÜÊàë‰ª¨ÈúÄË¶Å‰øÆÊîπËÑöÊú¨„ÄÇÁî®Êà∑ÂèØËÉΩÂ∏åÊúõÊàëÊèê‰æõ‰∏Ä‰∏™‰øÆÂ§çÊñπÊ°à„ÄÇ\n\nËÆ©ÊàëÊ£ÄÊü•ËÑöÊú¨‰∏≠ÊòØÂê¶ÊúâÁîüÊàêIDÁöÑÈÄªËæë„ÄÇÂú®`ToolManager.execute_tool`‰∏≠ÔºåËøîÂõûÁöÑÂ≠óÂÖ∏ÂåÖÂê´`\"tool_call_id\": tool_call[\"id\"]`„ÄÇÂ¶ÇÊûú`tool_call[\"id\"]`‰∏∫nullÔºåÈÇ£‰πàÂ∞±ÊòØnull„ÄÇ\n\nÊàë‰ª¨ÂèØ‰ª•‰øÆÊîπËøôÈáåÔºöÂ¶ÇÊûúID‰∏∫nullÔºåÂàôÁîüÊàê‰∏Ä‰∏™UUID„ÄÇ\n\n‰ΩÜÈ¶ñÂÖàÔºåÊàë‰ª¨ÈúÄË¶Å‰∫ÜËß£API‰∏∫‰ΩïËøîÂõûnullÁöÑID„ÄÇÂèØËÉΩÊòØAPIÁöÑbugÔºåÊàñËÄÖÂìçÂ∫îÊ†ºÂºè‰∏çÊ≠£Á°Æ„ÄÇ\n\nÂè¶Â§ñÔºåÂú®JSON‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®ÊòØ`\"id\": null`ÔºåËøôÊòéÊòæÊòØnull„ÄÇ\n\nÂõ†Ê≠§ÔºåÂú®ËÑöÊú¨‰∏≠ÔºåÊàë‰ª¨Â∫îËØ•Âú®Â∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÊó∂ÁîüÊàê‰∏Ä‰∏™ID„ÄÇ\n\nËÆ©ÊàëÂú®`ToolManager.execute_tool`‰∏≠‰øÆÊîπÔºöÊ£ÄÊü•`tool_call[\"id\"]`ÔºåÂ¶ÇÊûú‰∏∫nullÊàñÁ©∫ÔºåÂàôÁîüÊàê‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑID„ÄÇ\n\n‰ΩÜÂ∑•ÂÖ∑Ë∞ÉÁî®IDÊòØAPIÊèê‰æõÁöÑÔºåÊàë‰ª¨Â∫îËØ•‰øùÊåÅ‰∏ÄËá¥ÊÄß„ÄÇ‰∏çËøáÔºå‰∏∫‰∫ÜÂÖºÂÆπÊÄßÔºåÊàë‰ª¨ÂèØ‰ª•ÁîüÊàê‰∏Ä‰∏™ID„ÄÇ\n\nÊ≠§Â§ñÔºåÂú®‰øùÂ≠ò‰∏ä‰∏ãÊñáÊó∂ÔºåÊàë‰ª¨ÂèØËÉΩÈúÄË¶ÅÁ°Æ‰øùID‰∏ç‰∏∫null„ÄÇ\n\nÁé∞Âú®ÔºåÁî®Êà∑ÂèØËÉΩÂ∏åÊúõÊàëÁõ¥Êé•‰øÆÊîπËÑöÊú¨Âπ∂Êèê‰æõ‰øÆÂ§çÂêéÁöÑÁâàÊú¨„ÄÇ\n\nËÆ©ÊàëÁºñÂÜô‰∏Ä‰∏™‰øÆÂ§çÊñπÊ°àÔºö\n\nÂú®`ToolManager.execute_tool`ÊñπÊ≥ï‰∏≠Ôºö\n\n```python\ndef execute_tool(self, tool_call: Dict) -> Dict:\n    try:\n        function_name = tool_call[\"function\"][\"name\"]\n        function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n        \n        if function_name not in self.available_functions:\n            raise ToolError(f\"Unknown function: {function_name}\")\n        \n        print(f\"\\n{Colors.CYAN}üîß Executing tool: {function_name}{Colors.END}\", file=sys.stderr)\n        print(f\"   Arguments: {function_args}\", file=sys.stderr)\n        \n        # Execute the function\n        result = self.available_functions[function_name](**function_args)\n        \n        print(f\"   Result: {result}\", file=sys.stderr)\n        \n        # Get tool call ID, generate if null or empty\n        tool_call_id = tool_call.get(\"id\")\n        if not tool_call_id:\n            import uuid\n            tool_call_id = str(uuid.uuid4())\n            print(f\"{Colors.warn('‚ö†')} Generated tool call ID: {tool_call_id}\", file=sys.stderr)\n        \n        return {\n            \"tool_call_id\": tool_call_id,\n            \"role\": \"tool\",\n            \"name\": function_name,\n            \"content\": json.dumps(result, ensure_ascii=False)\n        }\n        \n    except Exception as e:\n        error_msg = f\"Tool execution failed: {str(e)}\"\n        print(f\"{Colors.error('‚ùå')} {error_msg}\", file=sys.stderr)\n        # Generate ID for error response as well\n        tool_call_id = tool_call.get(\"id\")\n        if not tool_call_id:\n            import uuid\n            tool_call_id = str(uuid.uuid4())\n        return {\n            \"tool_call_id\": tool_call_id,\n            \"role\": \"tool\",\n            \"name\": tool_call[\"function\"][\"name\"],\n            \"content\": json.dumps({\"error\": error_msg}, ensure_ascii=False)\n        }\n```\n\nÊ≠§Â§ñÔºåÂú®`APIClient._handle_tool_calls`‰∏≠ÔºåÊàë‰ª¨ÂèØËÉΩËøòÈúÄË¶ÅÁ°Æ‰øùÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏ç‰∏∫nullÔºå‰ΩÜËøôÈáåÊàë‰ª¨Âè™ÊòØ‰º†ÈÄíÂéüÂßãÂ∑•ÂÖ∑Ë∞ÉÁî®„ÄÇ\n\nÂè¶Â§ñÔºåÂú®‰øùÂ≠òJSONÊó∂ÔºåÊàë‰ª¨ÂèØ‰ª•Ê£ÄÊü•Âπ∂‰øÆÂ§çID„ÄÇ\n\n‰ΩÜÊúÄÂ•ΩÂú®Ê∫êÂ§¥Ëß£ÂÜ≥ÔºöÂΩìAPIËøîÂõûnull IDÊó∂ÔºåÊàë‰ª¨ÁîüÊàê‰∏Ä‰∏™ID„ÄÇ\n\nÁÑ∂ËÄåÔºåÊàë‰ª¨Êó†Ê≥ïÊéßÂà∂APIÂìçÂ∫î„ÄÇ‰ΩÜÊàë‰ª¨ÂèØ‰ª•‰øÆÊîπËÑöÊú¨ÔºåÂú®Êé•Êî∂Âà∞Â∑•ÂÖ∑Ë∞ÉÁî®Êó∂Ê£ÄÊü•IDÔºåÂ¶ÇÊûú‰∏∫nullÔºåÂàôÁîüÊàê‰∏Ä‰∏™„ÄÇ\n\nÂú®`_process_stream_data`ÊñπÊ≥ï‰∏≠ÔºåÂΩìÂ§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®Êó∂ÔºåÊàë‰ª¨ÂèØ‰ª•Ê£ÄÊü•IDÂπ∂ÁîüÊàê‰∏Ä‰∏™„ÄÇ\n\n‰ΩÜËøôÊ†∑ÂèØËÉΩ‰ºö‰Ωø‰ª£Á†ÅÂ§çÊùÇÂåñ„ÄÇ‰∏Ä‰∏™Êõ¥ÁÆÄÂçïÁöÑÊñπÊ≥ïÊòØÂú®Â∑•ÂÖ∑ÊâßË°åÊó∂ÁîüÊàêIDÔºåÂ¶Ç‰∏äÊâÄËø∞„ÄÇ\n\n‰ΩÜÂ∑•ÂÖ∑Ë∞ÉÁî®IDÈúÄË¶Å‰∏éÂéüÂßãË∞ÉÁî®ÂåπÈÖçÔºåÂê¶ÂàôAPIÂèØËÉΩÊó†Ê≥ïÂÖ≥ËÅîÂ∑•ÂÖ∑ÂìçÂ∫î„ÄÇ‰∏çËøáÔºåÂ¶ÇÊûúÂéüÂßãID‰∏∫nullÔºåÈÇ£‰πàÁîüÊàê‰∏Ä‰∏™IDÂ∫îËØ•Ê≤°ÈóÆÈ¢òÔºåÂõ†‰∏∫APIÂèØËÉΩ‰∏çÂÖ≥ÂøÉID„ÄÇ\n\n‰ΩÜ‰∏∫‰∫ÜÂÆâÂÖ®Ëµ∑ËßÅÔºåÊàë‰ª¨Â∫îËØ•Âú®Â∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÊó∂ÁîüÊàê‰∏Ä‰∏™ÔºåÂπ∂Âú®Â∑•ÂÖ∑ÂìçÂ∫î‰∏≠‰ΩøÁî®Áõ∏ÂêåÁöÑID„ÄÇ\n\nÊ≠§Â§ñÔºåÂú®‰øùÂ≠ò‰∏ä‰∏ãÊñáÊó∂ÔºåÊàë‰ª¨Â∫îËØ•Á°Æ‰øùÂ∑•ÂÖ∑Ë∞ÉÁî®ÂíåÂ∑•ÂÖ∑ÂìçÂ∫î‰∏≠ÁöÑID‰∏ÄËá¥„ÄÇ\n\nÁé∞Âú®ÔºåÁî®Êà∑ËøòÊèêÂà∞‚ÄúËøîÂõûtool_calls stop reason‚ÄùÔºåËøôÊÑèÂë≥ÁùÄAPIÂú®Â∑•ÂÖ∑Ë∞ÉÁî®ÂêéÂÅúÊ≠¢‰∫ÜÔºå‰ΩÜÂΩìÊàë‰ª¨Â∞ÜÂÖ∂‰Ωú‰∏∫‰∏ä‰∏ãÊñáËæìÂÖ•Êó∂ÔºåÂá∫Áé∞‰∫Ü400ÈîôËØØ„ÄÇ\n\nËøôÂèØËÉΩÊòØÂõ†‰∏∫Ê∂àÊÅØÈ°∫Â∫èÊàñÊ†ºÂºèÈóÆÈ¢ò„ÄÇ\n\nËÆ©Êàë‰ª¨ÁúãÁúãJSON‰∏≠ÁöÑÊ∂àÊÅØÈ°∫Â∫èÔºö\n\n1. Áî®Êà∑Ê∂àÊÅØ\n2. Âä©ÊâãÊ∂àÊÅØÔºàÂ∏¶ÊÄùËÄÉÂÜÖÂÆπÔºâ\n3. Áî®Êà∑Ê∂àÊÅØ\n4. Âä©ÊâãÊ∂àÊÅØÔºàÂ∏¶ÊÄùËÄÉÂÜÖÂÆπÔºâ\n5. Áî®Êà∑Ê∂àÊÅØ\n6. Âä©ÊâãÊ∂àÊÅØÔºàÂ∏¶ÊÄùËÄÉÂÜÖÂÆπÂíåÂ∑•ÂÖ∑Ë∞ÉÁî®Ôºâ\n7. Â∑•ÂÖ∑Ê∂àÊÅØÔºàtool_call_id‰∏∫nullÔºâ\n\nÂΩìËøô‰∏™‰∏ä‰∏ãÊñáË¢´Âä†ËΩΩÊó∂ÔºåËÑöÊú¨‰ºöÂ∞ÜÂÖ∂‰Ωú‰∏∫Ê∂àÊÅØÂàóË°®ÂèëÈÄÅ„ÄÇAPIÂèØËÉΩÊúüÊúõÂ∑•ÂÖ∑Ê∂àÊÅØÂú®Âä©ÊâãÊ∂àÊÅØ‰πãÂêéÔºåÂπ∂‰∏îtool_call_id‰∏çËÉΩ‰∏∫null„ÄÇ\n\nÂõ†Ê≠§Ôºå‰øÆÂ§çÂ∑•ÂÖ∑Ë∞ÉÁî®IDÂèØËÉΩËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ\n\nÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÈúÄË¶ÅÁ°Æ‰øùÂ∑•ÂÖ∑Ê∂àÊÅØÁöÑÊ†ºÂºèÊ≠£Á°Æ„ÄÇÂú®OpenAI API‰∏≠ÔºåÂ∑•ÂÖ∑Ê∂àÊÅØÁöÑroleÊòØ‚Äútool‚ÄùÔºåÂπ∂‰∏îÂøÖÈ°ªÂåÖÂê´tool_call_id„ÄÇ\n\nÂõ†Ê≠§ÔºåÊàëÂª∫ËÆÆ‰øÆÊîπËÑöÊú¨ÔºåÂú®Â∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÊó∂ÁîüÊàê‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑID„ÄÇ\n\nÁé∞Âú®ÔºåÊàëÈúÄË¶ÅÂ∞Ü‰øÆÂ§çÊñπÊ°àÊèê‰æõÁªôÁî®Êà∑„ÄÇÊàëÂèØ‰ª•ËæìÂá∫‰øÆÊîπÂêéÁöÑËÑöÊú¨ÈÉ®ÂàÜÔºåÊàñËÄÖÊèê‰æõ‰∏Ä‰∏™Ë°•‰∏Å„ÄÇ\n\n‰ΩÜÁî®Êà∑ÂèØËÉΩÂ∏åÊúõÊàëÁõ¥Êé•‰øÆÊîπËÑöÊú¨Âπ∂ËøîÂõûÊï¥‰∏™Êñá‰ª∂„ÄÇÁÑ∂ËÄåÔºåËÑöÊú¨ÂæàÈïøÔºåÊàëÂèØ‰ª•Âè™Êèê‰æõ‰øÆÊîπÂêéÁöÑÈÉ®ÂàÜ„ÄÇ\n\nËÆ©ÊàëÂÖàËæìÂá∫‰øÆÊîπÂêéÁöÑToolManager.execute_toolÊñπÊ≥ï„ÄÇ\n\nÂè¶Â§ñÔºåÊàë‰ª¨ËøòÈúÄË¶ÅÂú®APIClient._process_stream_data‰∏≠Â§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÁöÑÊÉÖÂÜµÔºå‰ΩÜ‰πüËÆ∏‰∏çÈúÄË¶Å„ÄÇ\n\nËÆ©Êàë‰ª¨ÂÖàÂÆûÁé∞‰øÆÂ§ç„ÄÇ\n\nÊàëÂ∞ÜÁºñÂÜô‰∏Ä‰∏™Ë°•‰∏ÅÔºåÂåÖÂê´ÂØπToolManager.execute_toolÁöÑ‰øÆÊîπ„ÄÇ\n\nÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÈúÄË¶ÅÂú®‰øùÂ≠ò‰∏ä‰∏ãÊñáÊó∂Á°Æ‰øùID‰∏ç‰∏∫nullÔºå‰ΩÜ‰øÆÂ§çÂ∑•ÂÖ∑ÊâßË°åÂêéÔºåIDÂ∫îËØ•Â∑≤ÁªèÂ≠òÂú®„ÄÇ\n\nÁé∞Âú®ÔºåÊàëÂ∞ÜÁîüÊàê‰∏Ä‰∏™ÂõûÁ≠îÔºåËß£ÈáäÈóÆÈ¢òÂπ∂Êèê‰æõ‰øÆÂ§çÊñπÊ°à„ÄÇ"
    },
    {
      "role": "user",
      "content": "```py\n#!/usr/bin/env python3\n\"\"\"\nAdvanced Typewriter Effect HTTP Client for OpenAI-compatible APIs (Enhanced Version).\nZero-default policy with graceful fallback - avoids crashes when encountering unsupported parameters.\n\"\"\"\n\nimport argparse\nimport base64\nimport importlib.util\nimport inspect\nimport io\nimport json\nimport logging\nimport mimetypes\nimport os\nimport re\nimport signal\nimport sys\nimport time\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional, Dict, List, Any, Tuple, Union, Callable\n\nimport requests\nfrom dotenv import load_dotenv\n\n# ============ Setup & Constants ============\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    stream=sys.stderr,\n)\nlogger = logging.getLogger(__name__)\n\nCODE_BLOCK = \"`\" * 3\n\n\nclass Constants:\n    \"\"\"Application constants\"\"\"\n\n    CHUNK_SIZE = 1024\n    STREAM_DELAY_REASONING = 0.01\n    STREAM_DELAY_CONTENT = 0.03\n    TIMEOUT = 60 * 60 * 6  # ‰øùÊåÅ 1 Â∞èÊó∂Ë∂ÖÊó∂\n\n\nclass Colors:\n    \"\"\"ANSI Color codes for terminal output.\"\"\"\n\n    GREEN = \"\\033[92m\"\n    YELLOW = \"\\033[93m\"\n    RED = \"\\033[91m\"\n    BLUE = \"\\033[94m\"\n    CYAN = \"\\033[96m\"\n    BOLD = \"\\033[1m\"\n    END = \"\\033[0m\"\n\n    @classmethod\n    def info(cls, text: str) -> str:\n        return f\"{cls.GREEN}{text}{cls.END}\"\n\n    @classmethod\n    def warn(cls, text: str) -> str:\n        return f\"{cls.YELLOW}{text}{cls.END}\"\n\n    @classmethod\n    def error(cls, text: str) -> str:\n        return f\"{cls.RED}{text}{cls.END}\"\n\n    @classmethod\n    def bold(cls, text: str) -> str:\n        return f\"{cls.BOLD}{text}{cls.END}\"\n\n\n# ============ Exceptions ============\n\n\nclass ChatError(Exception):\n    \"\"\"Base exception for chat errors\"\"\"\n\n    pass\n\n\nclass ConfigurationError(ChatError):\n    \"\"\"Missing required configuration\"\"\"\n\n    pass\n\n\nclass APIError(ChatError):\n    \"\"\"API communication error\"\"\"\n\n    pass\n\n\nclass FileProcessingError(ChatError):\n    \"\"\"File I/O error\"\"\"\n\n    pass\n\n\nclass ToolError(ChatError):\n    \"\"\"Tool execution error\"\"\"\n\n    pass\n\n\n# ============ UTF-8 Setup ============\n\n\ndef setup_utf8():\n    \"\"\"Force UTF-8 encoding for Windows and Unix\"\"\"\n    if sys.platform == \"win32\":\n        import ctypes\n\n        try:\n            kernel32 = ctypes.windll.kernel32\n            kernel32.SetConsoleCP(65001)\n            kernel32.SetConsoleOutputCP(65001)\n        except Exception:\n            pass\n\n    if hasattr(sys.stdout, \"buffer\"):\n        sys.stdout = io.TextIOWrapper(\n            sys.stdout.buffer, encoding=\"utf-8\", errors=\"replace\", line_buffering=True\n        )\n\n\n# ============ Data Models ============\n\n\n@dataclass\nclass APIConfig:\n    \"\"\"API connection configuration - only endpoint and api_key\"\"\"\n\n    endpoint: str = \"\"\n    api_key: str = \"\"\n    timeout: int = Constants.TIMEOUT  # HTTP timeout, not API parameter\n\n    def validate(self) -> None:\n        if not self.endpoint:\n            raise ConfigurationError(\"API endpoint is required\")\n        if not self.api_key:\n            raise ConfigurationError(\"API key is required\")\n\n\n@dataclass\nclass RequestMetadata:\n    \"\"\"Metadata for request tracking and reporting\"\"\"\n\n    start_time: datetime = field(default_factory=datetime.now)\n    end_time: Optional[datetime] = None\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n    total_tokens: int = 0\n    finish_reason: Optional[str] = None\n    model: Optional[str] = None\n    duration: float = 0.0\n    tool_calls: List[Dict] = field(default_factory=list)\n    tool_results: List[Dict] = field(default_factory=list)\n\n    def finalize(self) -> None:\n        self.end_time = datetime.now()\n        self.duration = (self.end_time - self.start_time).total_seconds()\n\n    def get_token_rate(self) -> Optional[float]:\n        \"\"\"Calculate tokens per second\"\"\"\n        if self.duration > 0 and self.completion_tokens > 0:\n            return self.completion_tokens / self.duration\n        return None\n\n\n# ============ Tool System ============\n\n\nclass ToolManager:\n    \"\"\"Manages tool functions and execution\"\"\"\n\n    def __init__(self, tools_module_path: str = \"tools.py\"):\n        self.tools_module_path = tools_module_path\n        self.available_functions: Dict[str, Callable] = {}\n        self.tool_definitions: List[Dict] = []\n        self._load_tools()\n\n    def _load_tools(self):\n        \"\"\"Load tool functions from tools.py module\"\"\"\n        try:\n            spec = importlib.util.spec_from_file_location(\n                \"tools\", self.tools_module_path\n            )\n            if spec and spec.loader:\n                tools_module = importlib.util.module_from_spec(spec)\n                spec.loader.exec_module(tools_module)\n\n                # Find all functions in the module\n                for name, obj in inspect.getmembers(tools_module):\n                    if inspect.isfunction(obj) and not name.startswith(\"_\"):\n                        # Check if function has proper annotations for OpenAI tools\n                        sig = inspect.signature(obj)\n                        params = {}\n                        for param_name, param in sig.parameters.items():\n                            if param_name != \"self\":\n                                param_type = (\n                                    str(param.annotation)\n                                    if param.annotation != inspect.Parameter.empty\n                                    else \"string\"\n                                )\n                                params[param_name] = {\n                                    \"type\": param_type.replace(\"<class '\", \"\")\n                                    .replace(\"'>\", \"\")\n                                    .lower(),\n                                    \"description\": f\"Parameter {param_name}\",\n                                }\n\n                        # Create tool definition\n                        tool_def = {\n                            \"type\": \"function\",\n                            \"function\": {\n                                \"name\": name,\n                                \"description\": obj.__doc__ or f\"Function {name}\",\n                                \"parameters\": {\n                                    \"type\": \"object\",\n                                    \"properties\": params,\n                                    \"required\": list(params.keys()),\n                                },\n                            },\n                        }\n\n                        self.tool_definitions.append(tool_def)\n                        self.available_functions[name] = obj\n\n                        print(\n                            f\"{Colors.info('‚úì')} Loaded tool: {name}\", file=sys.stderr\n                        )\n\n                if not self.tool_definitions:\n                    print(\n                        f\"{Colors.warn('‚ö†')} No tools found in {self.tools_module_path}\",\n                        file=sys.stderr,\n                    )\n            else:\n                print(\n                    f\"{Colors.warn('‚ö†')} Tools module not found: {self.tools_module_path}\",\n                    file=sys.stderr,\n                )\n        except Exception as e:\n            print(f\"{Colors.warn('‚ö†')} Failed to load tools: {e}\", file=sys.stderr)\n\n    def execute_tool(self, tool_call: Dict) -> Dict:\n        \"\"\"Execute a tool call and return result\"\"\"\n        try:\n            function_name = tool_call[\"function\"][\"name\"]\n            function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n\n            if function_name not in self.available_functions:\n                raise ToolError(f\"Unknown function: {function_name}\")\n\n            print(\n                f\"\\n{Colors.CYAN}üîß Executing tool: {function_name}{Colors.END}\",\n                file=sys.stderr,\n            )\n            print(f\"   Arguments: {function_args}\", file=sys.stderr)\n\n            # Execute the function\n            result = self.available_functions[function_name](**function_args)\n\n            print(f\"   Result: {result}\", file=sys.stderr)\n\n            # ‰øÆÂ§çÔºöÂ§ÑÁêÜnullÊàñÁ©∫ÁöÑtool_call_id\n            tool_call_id = tool_call.get(\"id\")\n            if not tool_call_id:\n                import uuid\n\n                tool_call_id = str(uuid.uuid4())\n                print(\n                    f\"{Colors.warn('‚ö†')} Generated tool call ID: {tool_call_id}\",\n                    file=sys.stderr,\n                )\n\n            return {\n                \"tool_call_id\": tool_call_id,\n                \"role\": \"tool\",\n                \"name\": function_name,\n                \"content\": json.dumps(result, ensure_ascii=False),\n            }\n\n        except Exception as e:\n            error_msg = f\"Tool execution failed: {str(e)}\"\n            print(f\"{Colors.error('‚ùå')} {error_msg}\", file=sys.stderr)\n\n            # ‰øÆÂ§çÔºöÈîôËØØÂìçÂ∫î‰πüÁîüÊàêID\n            tool_call_id = tool_call.get(\"id\")\n            if not tool_call_id:\n                import uuid\n\n                tool_call_id = str(uuid.uuid4())\n\n            return {\n                \"tool_call_id\": tool_call_id,\n                \"role\": \"tool\",\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": json.dumps({\"error\": error_msg}, ensure_ascii=False),\n            }\n\n    def get_tool_definitions(self) -> List[Dict]:\n        \"\"\"Get OpenAI-compatible tool definitions\"\"\"\n        return self.tool_definitions\n\n\n# ============ Configuration Management ============\n\n\nclass ConfigManager:\n    \"\"\"Manages configuration loading with graceful fallback\"\"\"\n\n    ENV_PATHS = [\".env\", \"../.env\", Path.home() / \".ai_chat.env\"]\n    PROFILE_PATHS = [\n        Path(\"profiles.json\"),\n        Path(\"../profiles.json\"),\n        Path.home() / \".ai_chat_profiles.json\",\n    ]\n\n    @staticmethod\n    def load_connection_config(args) -> APIConfig:\n        \"\"\"Load endpoint and api_key with priority: CLI > Profile > Env > Default\"\"\"\n        config = APIConfig()\n\n        # 1. Load .env files first\n        for env_path in ConfigManager.ENV_PATHS:\n            if Path(env_path).exists():\n                load_dotenv(env_path)\n                logger.info(f\"Loaded environment: {env_path}\")\n                break\n\n        # 2. From environment variables\n        config.endpoint = os.getenv(\"ENDPOINT\", \"\")\n        config.api_key = os.getenv(\"API_KEY\", \"\")\n\n        # 3. Try to load from profile\n        profile_config = ConfigManager._load_profile(args.profile)\n        if profile_config:\n            if \"endpoint\" in profile_config:\n                config.endpoint = profile_config[\"endpoint\"]\n            if \"api_key\" in profile_config:\n                config.api_key = profile_config[\"api_key\"]\n            if \"apiKey\" in profile_config:\n                config.api_key = profile_config[\"apiKey\"]\n\n        # 4. CLI arguments override everything\n        if hasattr(args, \"endpoint\") and args.endpoint:\n            config.endpoint = args.endpoint\n        if hasattr(args, \"api_key\") and args.api_key:\n            config.api_key = args.api_key\n\n        return config\n\n    @staticmethod\n    def _load_profile(profile_name: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Load profile from various locations\"\"\"\n        for path in ConfigManager.PROFILE_PATHS:\n            if path.exists():\n                try:\n                    with open(path, \"r\", encoding=\"utf-8\") as f:\n                        profiles = json.load(f)\n\n                    if profile_name in profiles:\n                        print(\n                            f\"{Colors.info('‚úì')} Loaded profile: {profile_name} from {path}\",\n                            file=sys.stderr,\n                        )\n                        return profiles[profile_name]\n                except Exception as e:\n                    logger.error(f\"Failed to load profile {path}: {e}\")\n\n        if profile_name != \"default\":\n            print(\n                f\"{Colors.warn('‚ö†')} Profile '{profile_name}' not found, using default\",\n                file=sys.stderr,\n            )\n        return None\n\n    @staticmethod\n    def load_request_body(config_path: str, args) -> Dict[str, Any]:\n        \"\"\"Load request body from config.json with safe parameter handling\"\"\"\n        path = Path(config_path).expanduser().resolve()\n\n        if not path.exists():\n            if (\n                config_path != \"config.json\"\n            ):  # User explicitly specified non-existent file\n                raise FileNotFoundError(f\"Config file not found: {config_path}\")\n            # Use empty config if default config.json doesn't exist\n            return {}\n\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            print(f\"{Colors.info('‚úì')} Loaded request config: {path}\", file=sys.stderr)\n\n            # Apply CLI overrides for safety-critical parameters\n            ConfigManager._apply_cli_overrides(data, args)\n\n            return data\n        except Exception as e:\n            raise ValueError(f\"Failed to load config file ({config_path}): {e}\")\n\n    @staticmethod\n    def _apply_cli_overrides(data: Dict[str, Any], args) -> None:\n        \"\"\"Apply CLI arguments to request body, removing unsupported parameters\"\"\"\n        # Safety: Remove problematic parameters if CLI provides alternatives\n        if hasattr(args, \"model\") and args.model:\n            data[\"model\"] = args.model\n            # Remove any model-specific parameters that might conflict\n            data.pop(\"temperature\", None)  # Some models don't support temperature\n            data.pop(\"top_p\", None)\n            data.pop(\"frequency_penalty\", None)\n            data.pop(\"presence_penalty\", None)\n\n        # OpenAI standard parameters override\n        if hasattr(args, \"temperature\") and args.temperature is not None:\n            data[\"temperature\"] = args.temperature\n        if hasattr(args, \"max_tokens\") and args.max_tokens is not None:\n            data[\"max_tokens\"] = args.max_tokens\n        if hasattr(args, \"top_p\") and args.top_p is not None:\n            data[\"top_p\"] = args.top_p\n        if hasattr(args, \"presence_penalty\") and args.presence_penalty is not None:\n            data[\"presence_penalty\"] = args.presence_penalty\n        if hasattr(args, \"frequency_penalty\") and args.frequency_penalty is not None:\n            data[\"frequency_penalty\"] = args.frequency_penalty\n        if hasattr(args, \"seed\") and args.seed is not None:\n            data[\"seed\"] = args.seed\n\n        # Stream handling\n        if hasattr(args, \"no_stream\") and args.no_stream:\n            data[\"stream\"] = False\n        elif \"stream\" not in data:\n            data[\"stream\"] = True  # Default to stream for better UX\n\n        # Thinking enable/disable\n        if hasattr(args, \"enable_thinking\"):\n            if args.enable_thinking is not None:\n                data[\"enable_thinking\"] = args.enable_thinking\n                # Some APIs don't support reasoning_content, remove if disabled\n                if not args.enable_thinking:\n                    data.pop(\"reasoning_content\", None)\n\n    @staticmethod\n    def save_context_to_config(config_path: str, messages: List[Dict]) -> None:\n        \"\"\"Save conversation context back to config.json\"\"\"\n        try:\n            path = Path(config_path).expanduser().resolve()\n\n            # Load existing config\n            if path.exists():\n                with open(path, \"r\", encoding=\"utf-8\") as f:\n                    config_data = json.load(f)\n            else:\n                config_data = {}\n\n            # Update messages\n            config_data[\"messages\"] = messages\n\n            # Save back\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(config_data, f, ensure_ascii=False, indent=2)\n\n            print(f\"{Colors.info('‚úì')} Saved context to: {path}\", file=sys.stderr)\n\n        except Exception as e:\n            print(f\"{Colors.warn('‚ö†')} Failed to save context: {e}\", file=sys.stderr)\n\n\n# ============ File & Message Building ============\n\n\nclass MessageBuilder:\n    \"\"\"Builds messages with file context support\"\"\"\n\n    IMAGE_TYPES = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\", \".bmp\"}\n\n    @staticmethod\n    def load_file(filepath: str) -> Union[str, Dict[str, Any]]:\n        \"\"\"Load file content with MIME type detection\"\"\"\n        if filepath.startswith(\"@\"):\n            filepath = filepath[1:]\n\n        path = Path(filepath).expanduser().resolve()\n        if not path.exists():\n            raise FileNotFoundError(f\"File not found: {filepath}\")\n\n        mime_type, _ = mimetypes.guess_type(path)\n        if not mime_type:\n            mime_type = \"application/octet-stream\"\n\n        # Image handling\n        if (\n            mime_type.startswith(\"image/\")\n            or path.suffix.lower() in MessageBuilder.IMAGE_TYPES\n        ):\n            with open(path, \"rb\") as f:\n                encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n            return {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:{mime_type};base64,{encoded}\"},\n            }\n\n        # Text file handling with encoding detection\n        encodings = [\"utf-8\", \"utf-8-sig\", \"gbk\", \"latin-1\"]\n        content = None\n\n        for enc in encodings:\n            try:\n                with open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n                    content = f.read()\n                break\n            except UnicodeDecodeError:\n                continue\n\n        if content is None:\n            with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n                content = f.read()\n\n        # Format based on file type\n        suffix = path.suffix[1:] if path.suffix else \"text\"\n        if path.suffix in [\".md\", \".txt\", \".rst\"]:\n            return f\"{content}\\n\\n[File: {path.name}]\"\n        else:\n            return f\"{CODE_BLOCK}{suffix}\\n{content}\\n{CODE_BLOCK}\\n[File: {path.name}]\"\n\n    @staticmethod\n    def build_messages(args, config_data: Dict | None = None) -> List[Dict[str, Any]]:\n        \"\"\"Build final message list with priority: context > config > prompt\"\"\"\n        messages = []\n\n        # Extract system prompt from config if exists\n        system_prompt = None\n        config_messages = []\n        if config_data:\n            system_prompt = config_data.pop(\"system\", None)\n            config_messages = config_data.pop(\"messages\", [])\n\n        # 1. Add system prompt if exists (always first)\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n            print(f\"{Colors.info('‚úì')} Added system prompt\", file=sys.stderr)\n\n        # 2. Load context if provided\n        if hasattr(args, \"context\") and args.context:\n            try:\n                context_messages = MessageBuilder._load_context(args.context)\n                messages.extend(context_messages)\n                print(\n                    f\"{Colors.info('‚úì')} Loaded conversation context: {args.context}\",\n                    file=sys.stderr,\n                )\n            except Exception as e:\n                print(\n                    f\"{Colors.warn('‚ö†')} Failed to load context: {e}\", file=sys.stderr\n                )\n\n        # 3. Add messages from config\n        if config_messages:\n            messages.extend(config_messages)\n\n        # 4. Add current user input\n        if hasattr(args, \"prompt\") and args.prompt:\n            user_messages = MessageBuilder._build_user_messages(args.prompt)\n            messages.extend(user_messages)\n\n        if not messages and not system_prompt:\n            raise ConfigurationError(\n                \"No messages to send. Please provide prompt or messages in config.\"\n            )\n\n        return messages\n\n    @staticmethod\n    def _load_context(context_path: str) -> List[Dict]:\n        \"\"\"Load conversation history from JSON file\"\"\"\n        path = Path(context_path).expanduser().resolve()\n        if not path.exists():\n            raise FileNotFoundError(f\"Context file not found: {context_path}\")\n\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n\n        if isinstance(data, list):\n            return data\n        elif isinstance(data, dict) and \"messages\" in data:\n            return data[\"messages\"]\n        else:\n            raise ValueError(\n                \"Invalid context format: expected list or {messages: [...]}\"\n            )\n\n    @staticmethod\n    def _build_user_messages(prompt_parts: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Build user messages from prompt parts\"\"\"\n        if not prompt_parts:\n            return []\n\n        content_parts = []\n        has_image = False\n\n        for part in prompt_parts:\n            if part.startswith(\"@\"):\n                try:\n                    file_content = MessageBuilder.load_file(part)\n                    if isinstance(file_content, dict):\n                        has_image = True\n                        print(\n                            f\"{Colors.info('‚úì')} Attached image: {part[1:]}\",\n                            file=sys.stderr,\n                        )\n                    else:\n                        print(\n                            f\"{Colors.info('‚úì')} Attached file: {part[1:]}\",\n                            file=sys.stderr,\n                        )\n                    content_parts.append(file_content)\n                except Exception as e:\n                    print(\n                        f\"{Colors.warn('‚ö†')} Failed to load file {part}: {e}\",\n                        file=sys.stderr,\n                    )\n                    content_parts.append(f\"[Failed to load file: {part}]\")\n            else:\n                content_parts.append(part)\n\n        # Build message content\n        if has_image:\n            multimodal_content = []\n            current_text = []\n\n            for part in content_parts:\n                if isinstance(part, dict):\n                    if current_text:\n                        multimodal_content.append(\n                            {\"type\": \"text\", \"text\": \"\\n\".join(current_text)}\n                        )\n                        current_text = []\n                    multimodal_content.append(part)\n                else:\n                    current_text.append(part)\n\n            if current_text:\n                multimodal_content.append(\n                    {\"type\": \"text\", \"text\": \"\\n\\n\".join(current_text)}\n                )\n\n            return [{\"role\": \"user\", \"content\": multimodal_content}]\n        else:\n            user_content = \"\\n\\n\".join(content_parts)\n            return [{\"role\": \"user\", \"content\": user_content}]\n\n\n# ============ Typewriter Output ============\n\n\nclass TypewriterPrinter:\n    \"\"\"Typewriter effect printer with file output\"\"\"\n\n    def __init__(self, output_file: Optional[io.TextIOWrapper] = None):\n        self.output_file = output_file\n        self.reasoning_buffer = \"\"\n        self.content_buffer = \"\"\n        self.reasoning_printed = 0\n        self.content_printed = 0\n        self.in_reasoning = True\n        self.start_time = time.time()\n        self.first_token_time = None\n        self.token_count = 0\n\n        # Performance tracking\n        self.reasoning_start = None\n        self.content_start = None\n\n    def write(self, text: str, is_reasoning: bool = False):\n        \"\"\"Write text to file\"\"\"\n        if self.output_file:\n            try:\n                self.output_file.write(text)\n                if not is_reasoning:\n                    self.output_file.flush()\n            except Exception as e:\n                print(f\"\\n{Colors.error('[File write error]')} {e}\", file=sys.stderr)\n\n    def update_reasoning(self, delta: str):\n        \"\"\"Update and print reasoning content\"\"\"\n        if not delta:\n            return\n\n        if self.first_token_time is None:\n            self.first_token_time = time.time()\n\n        if self.reasoning_start is None:\n            self.reasoning_start = time.time()\n            print(f\"\\n{Colors.BLUE}üí≠ Thinking:{Colors.END}\")\n            if self.output_file:\n                self.write(\"\\nüí≠ Thinking:\\n\", is_reasoning=True)\n\n        self.reasoning_buffer += delta\n        new_text = self.reasoning_buffer[self.reasoning_printed :]\n\n        # Print to console with typewriter effect\n        for char in new_text:\n            print(char, end=\"\", flush=True)\n            time.sleep(Constants.STREAM_DELAY_REASONING)\n\n        # Write to file\n        self.write(new_text, is_reasoning=True)\n\n        self.reasoning_printed = len(self.reasoning_buffer)\n        self.token_count += len(delta.split())  # Rough estimate\n\n    def switch_to_content(self):\n        \"\"\"Switch from reasoning to content output\"\"\"\n        if self.in_reasoning:\n            self.in_reasoning = False\n            self.content_start = time.time()\n\n            # Console output\n            print(f\"\\n{Colors.bold('='*50)}\")\n            print(f\"{Colors.CYAN}‚ú® Response:{Colors.END}\")\n            print(f\"{Colors.bold('='*50)}\")\n\n            # File output\n            if self.output_file:\n                self.write(f\"\\n\\n{'='*50}\\n‚ú® Response:\\n{'='*50}\\n\\n\")\n\n    def update_content(self, delta: str):\n        \"\"\"Update and print response content\"\"\"\n        if not delta:\n            return\n\n        if self.in_reasoning:\n            self.switch_to_content()\n\n        if self.first_token_time is None:\n            self.first_token_time = time.time()\n\n        self.content_buffer += delta\n        new_text = self.content_buffer[self.content_printed :]\n\n        # Typewriter effect\n        for char in new_text:\n            print(char, end=\"\", flush=True)\n            time.sleep(Constants.STREAM_DELAY_CONTENT)\n\n        self.write(new_text)\n\n        self.content_printed = len(self.content_buffer)\n        self.token_count += len(delta.split())  # Rough estimate\n\n    def finalize(self):\n        \"\"\"Finalize output\"\"\"\n        if self.output_file:\n            try:\n                self.output_file.flush()\n            except Exception as e:\n                print(f\"{Colors.warn('‚ö†')} Failed to flush file: {e}\", file=sys.stderr)\n\n\n# ============ HTTP Client ============\n\n\nclass APIClient:\n    \"\"\"HTTP client with retry and error handling using Session\"\"\"\n\n    def __init__(self, config: APIConfig, max_retries: int = 3):\n        self.config = config\n        self.max_retries = max_retries\n        self.session = requests.Session()\n        self.session.headers.update(\n            {\n                \"Authorization\": f\"Bearer {config.api_key}\",\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"text/event-stream\",\n            }\n        )\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.session.close()\n\n    def _estimate_tokens(self, messages: List[Dict]) -> Tuple[int, int]:\n        \"\"\"‰º∞ÁÆó Prompt ÁöÑ Token Êï∞Ôºà‰øùÂÆà‰º∞ËÆ°ÔºöÊØè 3 ‰∏™Â≠óÁ¨¶ 1 ‰∏™ TokenÔºâ\"\"\"\n        try:\n            # Â∞Ü messages ËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤ËÆ°ÁÆó\n            text = json.dumps(messages, ensure_ascii=False)\n            char_count = len(text)\n            # ‰øùÂÆà‰º∞ËÆ°ÔºöËã±ÊñáÁ∫¶ 4 Â≠óÁ¨¶/tokenÔºå‰∏≠ÊñáÁ∫¶ 1-2 Â≠óÁ¨¶/tokenÔºåÊ∑∑ÂêàÊåâ 3 ËÆ°ÁÆó\n            estimated_tokens = char_count // 3\n            return char_count, estimated_tokens\n        except:\n            return 0, 0\n\n    def request(\n        self,\n        payload: Dict[str, Any],\n        printer: TypewriterPrinter,\n        tool_manager: Optional[ToolManager] = None,\n    ) -> RequestMetadata:\n        \"\"\"Execute API request with retry logic\"\"\"\n        metadata = RequestMetadata(model=payload.get(\"model\"))\n        is_stream = payload.get(\"stream\", True)\n\n        # ËÆ°ÁÆó Prompt ‰ø°ÊÅØ\n        messages = payload.get(\"messages\", [])\n        char_count, estimated_tokens = self._estimate_tokens(messages)\n        max_tokens = payload.get(\"max_tokens\", \"Not set\")\n\n        # ÊòæÁ§∫ËØ∑Ê±Ç‰ø°ÊÅØÔºåÂåÖÊã¨ Token È¢Ñ‰º∞\n        print(\n            f\"\\n{Colors.CYAN}üöÄ Requesting {metadata.model or 'unknown model'}...{Colors.END}\"\n        )\n        print(f\"   Endpoint: {self.config.endpoint}\")\n        print(f\"   Stream: {'Yes' if is_stream else 'No'}\")\n        print(f\"   Prompt Size: {char_count} chars (Est. ~{estimated_tokens} tokens)\")\n\n        if max_tokens != \"Not set\":\n            print(f\"   Max Tokens: {max_tokens}\")\n        else:\n            print(f\"   Max Tokens: {Colors.warn('Not set')} (model default)\")\n\n        # Safety: Remove any empty or None parameters that might cause API errors\n        clean_payload = self._clean_payload(payload)\n\n        for attempt in range(self.max_retries + 1):\n            try:\n                response = self.session.post(\n                    self.config.endpoint,\n                    json=clean_payload,\n                    stream=is_stream,\n                    timeout=self.config.timeout,\n                )\n                response.raise_for_status()\n\n                if is_stream:\n                    return self._handle_stream(\n                        response, printer, metadata, tool_manager\n                    )\n                else:\n                    return self._handle_non_stream(\n                        response, printer, metadata, tool_manager\n                    )\n\n            except requests.exceptions.RequestException as e:\n                if attempt < self.max_retries:\n                    wait_time = 2**attempt\n                    print(\n                        f\"{Colors.warn(f'‚ö†Ô∏è  Request failed ({e}), retrying in {wait_time}s...')}\",\n                        file=sys.stderr,\n                    )\n                    time.sleep(wait_time)\n                else:\n                    raise APIError(\n                        f\"Request failed after {self.max_retries} retries: {e}\"\n                    )\n\n        raise APIError(\"Max retries exceeded\")\n\n    def _clean_payload(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove potentially problematic parameters\"\"\"\n        clean = payload.copy()\n\n        # Remove None values\n        for key in list(clean.keys()):\n            if clean[key] is None:\n                del clean[key]\n\n        # Remove empty strings\n        for key in list(clean.keys()):\n            if isinstance(clean[key], str) and not clean[key].strip():\n                del clean[key]\n\n        return clean\n\n    def _handle_stream(\n        self,\n        response,\n        printer: TypewriterPrinter,\n        metadata: RequestMetadata,\n        tool_manager: Optional[ToolManager],\n    ) -> RequestMetadata:\n        \"\"\"Handle streaming response\"\"\"\n        buffer = \"\"\n        tool_calls_buffer = {}\n\n        for chunk in response.iter_content(chunk_size=Constants.CHUNK_SIZE):\n            if not chunk:\n                continue\n\n            buffer += chunk.decode(\"utf-8\", errors=\"replace\")\n\n            while \"\\n\" in buffer:\n                line, buffer = buffer.split(\"\\n\", 1)\n                line = line.strip()\n\n                if not line.startswith(\"data: \"):\n                    continue\n\n                data_str = line[6:]\n                if data_str == \"[DONE]\":\n                    break\n\n                try:\n                    data = json.loads(data_str)\n                    self._process_stream_data(\n                        data, printer, metadata, tool_calls_buffer\n                    )\n                except json.JSONDecodeError:\n                    continue\n\n        # Handle tool calls after stream ends\n        if tool_calls_buffer and tool_manager:\n            self._handle_tool_calls(tool_calls_buffer, metadata, tool_manager)\n\n        metadata.finalize()\n        return metadata\n\n    def _process_stream_data(\n        self,\n        data: Dict,\n        printer: TypewriterPrinter,\n        metadata: RequestMetadata,\n        tool_calls_buffer: Dict,\n    ):\n        \"\"\"Process individual stream data chunk\"\"\"\n        # Update usage\n        if \"usage\" in data and data[\"usage\"]:\n            metadata.prompt_tokens = data[\"usage\"].get(\"prompt_tokens\", 0)\n            metadata.completion_tokens = data[\"usage\"].get(\"completion_tokens\", 0)\n            metadata.total_tokens = data[\"usage\"].get(\"total_tokens\", 0)\n\n        choices = data.get(\"choices\", [])\n        if not choices:\n            return\n\n        choice = choices[0]\n        delta = choice.get(\"delta\", {})\n\n        # Update finish reason\n        finish_reason = choice.get(\"finish_reason\")\n        if finish_reason:\n            metadata.finish_reason = finish_reason\n\n        # Process tool calls\n        tool_calls = delta.get(\"tool_calls\", [])\n        for tool_call in tool_calls:\n            idx = tool_call.get(\"index\", 0)\n            if idx not in tool_calls_buffer:\n                tool_calls_buffer[idx] = {\n                    \"id\": \"\",\n                    \"type\": \"function\",\n                    \"function\": {\"name\": \"\", \"arguments\": \"\"},\n                }\n\n            if \"id\" in tool_call:\n                tool_calls_buffer[idx][\"id\"] = tool_call[\"id\"]\n            if \"function\" in tool_call:\n                if \"name\" in tool_call[\"function\"]:\n                    tool_calls_buffer[idx][\"function\"][\"name\"] = tool_call[\"function\"][\n                        \"name\"\n                    ]\n                if \"arguments\" in tool_call[\"function\"]:\n                    tool_calls_buffer[idx][\"function\"][\"arguments\"] += tool_call[\n                        \"function\"\n                    ][\"arguments\"]\n\n        # Process content\n        reasoning_delta = delta.get(\"reasoning_content\") or delta.get(\"reasoning\") or \"\"\n        content_delta = delta.get(\"content\", \"\")\n\n        if reasoning_delta:\n            printer.update_reasoning(reasoning_delta)\n\n        if content_delta:\n            printer.update_content(content_delta)\n\n    def _handle_non_stream(\n        self,\n        response,\n        printer: TypewriterPrinter,\n        metadata: RequestMetadata,\n        tool_manager: Optional[ToolManager],\n    ) -> RequestMetadata:\n        \"\"\"Handle non-streaming response\"\"\"\n        data = response.json()\n        choice = data.get(\"choices\", [{}])[0]\n        message = choice.get(\"message\", {})\n\n        reasoning = message.get(\"reasoning_content\", \"\")\n        content = message.get(\"content\", \"\")\n\n        if reasoning:\n            printer.update_reasoning(reasoning)\n\n        printer.switch_to_content()\n        printer.update_content(content)\n\n        # Handle tool calls\n        tool_calls = message.get(\"tool_calls\", [])\n        if tool_calls and tool_manager:\n            self._handle_tool_calls(tool_calls, metadata, tool_manager)\n\n        # Update metadata\n        if \"usage\" in data:\n            metadata.prompt_tokens = data[\"usage\"].get(\"prompt_tokens\", 0)\n            metadata.completion_tokens = data[\"usage\"].get(\"completion_tokens\", 0)\n            metadata.total_tokens = data[\"usage\"].get(\"total_tokens\", 0)\n\n        metadata.finish_reason = choice.get(\"finish_reason\")\n        metadata.finalize()\n\n        return metadata\n\n    def _handle_tool_calls(\n        self,\n        tool_calls: Union[List, Dict],\n        metadata: RequestMetadata,\n        tool_manager: ToolManager,\n    ):\n        \"\"\"Handle tool calls and execute them\"\"\"\n        if isinstance(tool_calls, dict):\n            # Convert dict to list\n            tool_calls_list = []\n            for idx in sorted(tool_calls.keys()):\n                tool_calls_list.append(tool_calls[idx])\n            tool_calls = tool_calls_list\n\n        metadata.tool_calls = tool_calls\n\n        print(f\"\\n{Colors.CYAN}üõ†Ô∏è  Tool Calls Detected:{Colors.END}\")\n        for i, tool_call in enumerate(tool_calls, 1):\n            print(\n                f\"   {i}. {tool_call['function']['name']}({tool_call['function']['arguments']})\"\n            )\n\n        # Execute tool calls\n        for tool_call in tool_calls:\n            result = tool_manager.execute_tool(tool_call)\n            metadata.tool_results.append(result)\n\n\n# ============ Result Saving ============\n\n\nclass ResultSaver:\n    \"\"\"Saves conversation results to files\"\"\"\n\n    @staticmethod\n    def save(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Tuple[Path, Path]:\n        \"\"\"Save results as Markdown and JSON\"\"\"\n        # Create directory if needed\n        base_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # 1. Save Markdown\n        md_path = ResultSaver._save_markdown(base_path, payload, metadata, printer)\n\n        # 2. Save JSON for continuation\n        json_path = ResultSaver._save_json(base_path, payload, metadata, printer)\n\n        return md_path, json_path\n\n    @staticmethod\n    def _save_markdown(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Path:\n        \"\"\"Save conversation as Markdown with full history\"\"\"\n        md_path = base_path.with_suffix(\".md\")\n\n        # Prepare payload for display (hide messages to save space)\n        display_payload = payload.copy()\n        messages = display_payload.pop(\"messages\", [])\n        if messages:\n            display_payload[\"messages\"] = f\"[{len(messages)} messages hidden]\"\n\n        # Build Markdown content\n        md_content = f\"\"\"# AI Conversation Log\n**Time**: {metadata.start_time.strftime('%Y-%m-%d %H:%M:%S')}  \n**Model**: {metadata.model or 'Unknown'}  \n**Duration**: {metadata.duration:.2f}s  \n\n## Request Configuration\n{CODE_BLOCK}json\n{json.dumps(display_payload, indent=2, ensure_ascii=False)}\n{CODE_BLOCK}\n\n## Conversation History\n\n\"\"\"\n\n        # Add all messages\n        for msg in messages:\n            role = msg.get(\"role\", \"unknown\").upper()\n            content = msg.get(\"content\", \"\")\n\n            # Handle multimodal content\n            if isinstance(content, list):\n                texts = []\n                for item in content:\n                    if isinstance(item, dict):\n                        if item.get(\"type\") == \"text\":\n                            texts.append(item.get(\"text\", \"\"))\n                        elif item.get(\"type\") == \"image_url\":\n                            texts.append(\"[Image]\")\n                content = \"\\n\".join(texts)\n\n            md_content += f\"### {role}\\n\\n{content}\\n\\n\"\n\n        # Add reasoning if present\n        if printer.reasoning_buffer:\n            md_content += f\"\"\"\n---\n\n## üí≠ Thinking Process\n<details>\n<summary>Click to expand ({len(printer.reasoning_buffer)} chars)</summary>\n\n{printer.reasoning_buffer}\n</details>\n\n\"\"\"\n\n        # Add response\n        md_content += f\"\"\"\n---\n\n## ‚ú® Response\n{printer.content_buffer}\n\n\"\"\"\n\n        # Add tool calls if present\n        if metadata.tool_calls:\n            md_content += f\"\"\"\n---\n\n## üõ†Ô∏è Tool Calls\n\"\"\"\n            for i, tool_call in enumerate(metadata.tool_calls, 1):\n                md_content += f\"\"\"\n### Tool Call {i}\n- **Function**: `{tool_call['function']['name']}`\n- **Arguments**: `{tool_call['function']['arguments']}`\n\"\"\"\n\n            if metadata.tool_results:\n                md_content += f\"\"\"\n### Tool Results\n\"\"\"\n                for result in metadata.tool_results:\n                    md_content += f\"\"\"\n- **{result['name']}**: {result['content']}\n\"\"\"\n\n        md_content += f\"\"\"\n---\n## üìä Statistics\n\n- **Finish Reason**: {metadata.finish_reason or 'N/A'}\n- **Tokens**: {metadata.total_tokens or 'N/A'} (Prompt: {metadata.prompt_tokens or 'N/A'}, Completion: {metadata.completion_tokens or 'N/A'})\n- **Token Rate**: {f'{metadata.get_token_rate():.1f} tokens/s' if metadata.get_token_rate() else 'N/A'}\n- **First Token Latency**: {f'{(printer.first_token_time - printer.start_time):.2f}s' if printer.first_token_time else 'N/A'}\n- **Estimated Total Tokens**: {printer.token_count}\n\"\"\"\n\n        with open(md_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(md_content)\n\n        return md_path\n\n    @staticmethod\n    def _save_json(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Path:\n        \"\"\"Save conversation as JSON for continuation\"\"\"\n        json_path = base_path.with_suffix(\".json\")\n\n        # Build conversation for continuation\n        continuation = {\n            \"metadata\": {\n                \"export_time\": datetime.now().isoformat(),\n                \"model\": metadata.model,\n                \"total_tokens\": metadata.total_tokens,\n                \"duration\": metadata.duration,\n            },\n            \"messages\": payload.get(\"messages\", []).copy(),\n        }\n\n        # Add assistant response\n        assistant_msg = {\n            \"role\": \"assistant\",\n            \"content\": printer.content_buffer,\n        }\n        if printer.reasoning_buffer:\n            assistant_msg[\"reasoning_content\"] = printer.reasoning_buffer\n        if metadata.tool_calls:\n            # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_calls‰∏≠ÁöÑID‰∏ç‰∏∫null\n            for tool_call in metadata.tool_calls:\n                if not tool_call.get(\"id\"):\n                    import uuid\n\n                    tool_call[\"id\"] = str(uuid.uuid4())\n            assistant_msg[\"tool_calls\"] = metadata.tool_calls\n\n        continuation[\"messages\"].append(assistant_msg)\n\n        # Add tool results as tool messages\n        for result in metadata.tool_results:\n            # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_call_id‰∏ç‰∏∫null\n            if not result.get(\"tool_call_id\"):\n                import uuid\n\n                result[\"tool_call_id\"] = str(uuid.uuid4())\n            continuation[\"messages\"].append(result)\n\n        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(continuation, f, ensure_ascii=False, indent=2)\n\n        return json_path\n\n\n# ============ Argument Parser ============\n\n\ndef create_argument_parser() -> argparse.ArgumentParser:\n    \"\"\"Create enhanced argument parser\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"AI Chat CLI - Enhanced with graceful parameter handling\",\n        formatter_class=argparse.RawTextHelpFormatter,\n        epilog=\"\"\"\nConfiguration Strategy:\n  1. Unified settings via config.json to avoid model-specific parameter issues\n  2. CLI arguments safely override config.json (removes conflicting params)\n  3. profiles.json or .env provides endpoint and api_key\n\nImage Support:\n  Use @filename.png to attach images (png, jpg, jpeg, gif, webp, bmp supported)\n  Example: python chat.py @photo.jpg \"Describe this image\"\n\nToken Estimation:\n  Before sending, the CLI will display estimated prompt tokens and max_tokens\n  to help you avoid context overflow.\n\nTool Support:\n  Define tools in tools.py with proper function signatures and docstrings\n  The CLI will automatically load and make them available to the model\n\nExamples:\n  # Basic usage with default config\n  python chat.py \"Hello, how are you?\"\n  \n  # With file attachment (code or image)\n  python chat.py @code.py \"Explain this code\"\n  python chat.py @image.png \"What's in this image?\"\n  \n  # Custom profile and config\n  python chat.py -p deepseek -f deepseek_config.json \"Explain quantum computing\"\n  \n  # Continue conversation\n  python chat.py --context conversation.json \"Continue from here\"\n  \n  # Start new conversation (don't save to config)\n  python chat.py --new \"Start fresh conversation\"\n  \n  # Disable streaming\n  python chat.py --no-stream \"Generate a long response\"\n  \n  # Override parameters\n  python chat.py --temperature 0.5 --max-tokens 2000 \"Creative writing\"\n        \"\"\",\n    )\n\n    # Connection\n    parser.add_argument(\n        \"--profile\", \"-p\", default=\"default\", help=\"Profile name in profiles.json\"\n    )\n    parser.add_argument(\"--endpoint\", \"-e\", help=\"Override API endpoint\")\n    parser.add_argument(\"--api-key\", \"-k\", help=\"Override API key\")\n\n    # Configuration\n    parser.add_argument(\n        \"--config\", \"-c\", default=\"config.json\", help=\"Request body configuration file\"\n    )\n    parser.add_argument(\n        \"--new\",\n        action=\"store_true\",\n        help=\"Start new conversation (don't save context to config)\",\n    )\n\n    # Input\n    parser.add_argument(\n        \"prompt\", nargs=\"*\", help=\"Prompt text (supports @filename for files/images)\"\n    )\n    parser.add_argument(\n        \"--context\", help=\"Conversation context JSON file for continuation\"\n    )\n\n    # OpenAI Parameters (override config.json)\n    parser.add_argument(\"--model\", \"-m\", help=\"Model name (overrides config)\")\n    parser.add_argument(\n        \"--temperature\", \"-t\", type=float, help=\"Sampling temperature (0-2)\"\n    )\n    parser.add_argument(\"--max-tokens\", type=int, help=\"Maximum tokens to generate\")\n    parser.add_argument(\"--top-p\", type=float, help=\"Nucleus sampling parameter\")\n    parser.add_argument(\"--presence-penalty\", type=float, help=\"Presence penalty\")\n    parser.add_argument(\"--frequency-penalty\", type=float, help=\"Frequency penalty\")\n    parser.add_argument(\"--seed\", type=int, help=\"Random seed for reproducibility\")\n\n    # Behavior (safely override config)\n    parser.add_argument(\n        \"--no-stream\", action=\"store_true\", help=\"Disable streaming output\"\n    )\n    parser.add_argument(\n        \"--enable-thinking\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable thinking process\",\n    )\n    parser.add_argument(\n        \"--no-thinking\",\n        dest=\"enable_thinking\",\n        action=\"store_false\",\n        help=\"Disable thinking process\",\n    )\n\n    # Tools\n    parser.add_argument(\n        \"--tools\", default=\"tools.py\", help=\"Python module containing tool functions\"\n    )\n    parser.add_argument(\n        \"--no-tools\", action=\"store_true\", help=\"Disable tool functions\"\n    )\n\n    # Output\n    parser.add_argument(\"--output\", \"-o\", help=\"Output file base name\")\n\n    return parser\n\n\n# ============ Main Function ============\n\n\ndef handle_interrupt(signum, frame):\n    \"\"\"Handle Ctrl+C gracefully\"\"\"\n    print(f\"\\n{Colors.warn('‚ö†Ô∏è  Interrupted by user')}\")\n    sys.exit(130)\n\n\ndef main():\n    \"\"\"Main entry point\"\"\"\n    # Setup\n    setup_utf8()\n    signal.signal(signal.SIGINT, handle_interrupt)\n\n    # Parse arguments\n    parser = create_argument_parser()\n    args = parser.parse_args()\n\n    try:\n        # 1. Load connection configuration\n        api_config = ConfigManager.load_connection_config(args)\n        api_config.validate()\n\n        # 2. Load request body from config.json\n        request_body = ConfigManager.load_request_body(args.config, args)\n\n        # 3. Initialize tool manager\n        tool_manager = None\n        if not args.no_tools and Path(args.tools).exists():\n            tool_manager = ToolManager(args.tools)\n            tool_definitions = tool_manager.get_tool_definitions()\n            if tool_definitions:\n                request_body[\"tools\"] = tool_definitions\n                print(\n                    f\"{Colors.info('‚úì')} Added {len(tool_definitions)} tools to request\",\n                    file=sys.stderr,\n                )\n\n        # 4. Build messages\n        messages = MessageBuilder.build_messages(args, request_body)\n        request_body[\"messages\"] = messages\n\n        # 5. Determine output path\n        if args.output:\n            output_base = Path(args.output)\n        else:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            prompt_hint = (\n                args.prompt[0][:20]\n                if args.prompt\n                else request_body.get(\"model\", \"chat\")\n            )\n            safe_name = re.sub(r\"[^\\w\\s-]\", \"\", prompt_hint).strip().replace(\" \", \"_\")\n            output_base = Path(f\"chat_{safe_name}_{timestamp}\")\n\n        # 6. Execute request\n        with open(output_base.with_suffix(\".md\"), \"w\", encoding=\"utf-8\") as out_file:\n            printer = TypewriterPrinter(out_file)\n\n            with APIClient(api_config) as client:\n                metadata = client.request(request_body, printer, tool_manager)\n\n            printer.finalize()\n\n            # 7. Add separator before complete message\n            print(f\"\\n{Colors.bold('‚îÄ' * 60)}\")\n\n            # 8. Print statistics\n            print(f\"\\n{Colors.info('‚úÖ Complete')}\")\n            print(f\"   Duration: {metadata.duration:.2f}s\")\n\n            if metadata.total_tokens:\n                print(\n                    f\"   Tokens: {metadata.total_tokens} (Prompt: {metadata.prompt_tokens}, Completion: {metadata.completion_tokens})\"\n                )\n                if token_rate := metadata.get_token_rate():\n                    print(f\"   Token Rate: {token_rate:.1f} tokens/s\")\n\n            if metadata.finish_reason:\n                print(f\"   Finish Reason: {metadata.finish_reason}\")\n\n            if printer.first_token_time:\n                print(\n                    f\"   First Token: {(printer.first_token_time - printer.start_time):.2f}s\"\n                )\n\n            if metadata.tool_calls:\n                print(f\"   Tool Calls: {len(metadata.tool_calls)} executed\")\n\n            # 9. Save results\n            md_path, json_path = ResultSaver.save(\n                output_base, request_body, metadata, printer\n            )\n\n            print(f\"\\n{Colors.info('üíæ Saved:')}\")\n            print(f\"   {md_path}\")\n            print(f\"   {json_path}\")\n            print(\n                f\"\\n{Colors.info('üí° Tip:')} Continue with: --context {json_path.name}\"\n            )\n\n            # 10. Save context to config.json if not --new\n            if not args.new:\n                # Update messages with assistant response\n                updated_messages = messages.copy()\n\n                # Add assistant message\n                assistant_msg = {\n                    \"role\": \"assistant\",\n                    \"content\": printer.content_buffer,\n                }\n                if printer.reasoning_buffer:\n                    assistant_msg[\"reasoning_content\"] = printer.reasoning_buffer\n                if metadata.tool_calls:\n                    assistant_msg[\"tool_calls\"] = metadata.tool_calls\n\n                updated_messages.append(assistant_msg)\n\n                # Add tool results\n                for result in metadata.tool_results:\n                    updated_messages.append(result)\n\n                # Save to config.json\n                ConfigManager.save_context_to_config(args.config, updated_messages)\n\n    except KeyboardInterrupt:\n        print(f\"\\n{Colors.warn('‚ö†Ô∏è  Interrupted')}\", file=sys.stderr)\n        sys.exit(130)\n    except (ConfigurationError, FileNotFoundError) as e:\n        print(f\"{Colors.error('‚ùå Configuration Error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n    except APIError as e:\n        print(f\"{Colors.error('‚ùå API Error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n    except Exception as e:\n        logger.exception(\"Unexpected error\")\n        print(f\"{Colors.error('‚ùå Unexpected error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n[File: deepseek.py]\n\n```json\n{\n  \"metadata\": {\n    \"export_time\": \"2026-02-06T03:52:29.393673\",\n    \"model\": \"Qwen/Qwen3-8B\",\n    \"total_tokens\": 771,\n    \"duration\": 13.744392\n  },\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"ÈòøÂ∑¥ÈòøÂ∑¥\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\nÊàëÊó†Ê≥ïÁêÜËß£‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùËøô‰∏™ËØçÁöÑÂÖ∑‰ΩìÂê´‰πâÔºåÊÇ®ÊòØÂê¶Âú®Ë°®ËææÊüêÁßçÊÉÖÁª™ÊàñÈúÄË¶ÅÂ∏ÆÂä©ÔºüÂ¶ÇÊûúÈúÄË¶ÅËÆ°ÁÆó„ÄÅÊü•ËØ¢Êó∂Èó¥ÊàñÂ§©Ê∞î‰ø°ÊÅØÔºåËØ∑ÂëäËØâÊàëÂÖ∑‰ΩìÂÜÖÂÆπÔºåÊàë‰ºöÂ∞ΩÂäõÂçèÂä©ÊÇ®„ÄÇ\",\n      \"reasoning_content\": \"\\nOkay, the user just said \\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\". Let me think about how to respond. First, I need to understand what they mean. \\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\" is a term that's often used in Chinese online culture, especially in gaming communities. It's a way to express frustration or exasperation, similar to \\\"arara\\\" in Japanese or \\\"arrgghh\\\" in English. People might use it when they're upset about something in a game, like a loss or a difficult situation.\\n\\nNow, looking at the tools provided, there are three functions: calculate, get_current_time, and search_weather. The user's message doesn't seem to involve any of these. They didn't ask for a math calculation, didn't mention a timezone for the current time, and didn't request weather info for a city. So, none of the tools are applicable here. \\n\\nI should check if there's any hidden request or if the user is testing me. But \\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\" is just an expression. Maybe they want me to acknowledge their frustration or explain the term? However, since the tools don't cover that, I need to respond without using any function calls. \\n\\nThe correct approach is to inform the user that I can't assist with their current message and ask how I can help. That way, I stay within the provided tools and offer further support if needed.\\n\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"‰ªäÊó•Â§©Ê∞î\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\nËØ∑ÂëäËØâÊàëÊÇ®ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞îÔºåÊàëÂ∞Ü‰∏∫ÊÇ®ÊêúÁ¥¢Áõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ\",\n      \"reasoning_content\": \"\\nÂ•ΩÁöÑÔºåÁî®Êà∑Áé∞Âú®ÈóÆÁöÑÊòØ‚Äú‰ªäÊó•Â§©Ê∞î‚ÄùÔºåÁúãËµ∑Êù•‰ªñ‰ª¨ÊÉ≥‰∫ÜËß£‰ªäÂ§©ÁöÑÂ§©Ê∞îÊÉÖÂÜµ„ÄÇ‰∏çËøá‰πãÂâçÁöÑÂØπËØù‰∏≠ÔºåÁî®Êà∑ËØ¥‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùÔºåÊàëÊ≤°ËÉΩÂáÜÁ°ÆÁêÜËß£‰ªñ‰ª¨ÁöÑÊÑèÊÄù„ÄÇÂèØËÉΩÁî®Êà∑ÊòØÂú®ÊµãËØïÊàñËÄÖÁî®ÊüêÁßçÊöóÂè∑Ôºå‰ΩÜÂêéÊù•‰ªñ‰ª¨Áõ¥Êé•ÈóÆÂ§©Ê∞î‰∫Ü„ÄÇ\\n\\nÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁ°ÆÂÆöÁî®Êà∑ÊòØÂê¶ÈúÄË¶ÅÊü•ËØ¢Â§©Ê∞î„ÄÇÊ†πÊçÆÂΩìÂâçÁöÑÂ∑•ÂÖ∑ÔºåÊúâ‰∏Ä‰∏™search_weatherÂáΩÊï∞ÔºåÈúÄË¶ÅÂèÇÊï∞ÊòØÂüéÂ∏Ç„ÄÇÁî®Êà∑Ê≤°ÊúâÊèêÂà∞ÂÖ∑‰ΩìÂüéÂ∏ÇÔºåÊâÄ‰ª•ÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•ËØ¢ÈóÆ„ÄÇ‰ΩÜÊúâÊó∂ÂÄôÁî®Êà∑ÂèØËÉΩÈªòËÆ§Êü•ËØ¢ÊâÄÂú®ÂüéÂ∏ÇÔºå‰∏çËøáËøôÈáåÊ≤°Êúâ‰∏ä‰∏ãÊñáÔºåÊâÄ‰ª•‰∏çËÉΩÁ°ÆÂÆö„ÄÇ\\n\\nÂè¶Â§ñÔºåÁî®Êà∑ÂèØËÉΩÊòØÂú®ÁªßÁª≠‰πãÂâçÁöÑÂØπËØùÔºå‰ΩÜ‰πãÂâçÁöÑ‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùÊ≤°ÊúâÊòéÁ°Æ‰ø°ÊÅØ„ÄÇÊâÄ‰ª•ÊúÄÂ•ΩËøòÊòØÂÖàÁ°ÆËÆ§Áî®Êà∑ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞î„ÄÇ‰∏çËøá‰πüÊúâÂèØËÉΩÁî®Êà∑Âè™ÊòØÁ™ÅÁÑ∂ÊÉ≥‰∫ÜËß£Â§©Ê∞îÔºåÊ≤°ÊúâÊèêÂà∞ÂüéÂ∏ÇÔºåËøôÊó∂ÂÄôÂèØËÉΩÈúÄË¶ÅÊèêÁ§∫‰ªñ‰ª¨Êèê‰æõÂüéÂ∏ÇÂêçÁß∞„ÄÇ\\n\\nÊ£ÄÊü•Â∑•ÂÖ∑‰∏≠ÁöÑÂáΩÊï∞Ôºåsearch_weatherÁöÑÂèÇÊï∞ÊòØcityÔºåÂøÖÂ°´„ÄÇÊâÄ‰ª•ÂøÖÈ°ªËé∑ÂèñÂüéÂ∏Ç‰ø°ÊÅØÊâçËÉΩË∞ÉÁî®„ÄÇÂõ†Ê≠§ÔºåÊàëÂ∫îËØ•ÂõûÂ§çÁî®Êà∑ÔºåËØ¢ÈóÆ‰ªñ‰ª¨ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞î„ÄÇËøôÊ†∑Êó¢Á¨¶ÂêàÂ∑•ÂÖ∑ÁöÑË¶ÅÊ±ÇÔºå‰πüËÉΩÁ°Æ‰øùÊèê‰æõÂáÜÁ°ÆÁöÑ‰ø°ÊÅØ„ÄÇ\\n\\nÂè¶Â§ñÔºåÊòØÂê¶ÊúâÂÖ∂‰ªñÂèØËÉΩÔºüÊØîÂ¶ÇÁî®Êà∑ÂèØËÉΩËØØËæìÂÖ•ÔºåÊàñËÄÖÊúâÂÖ∂‰ªñÊÑèÂõæÔºü‰ΩÜÊ†πÊçÆÁé∞ÊúâÂ∑•ÂÖ∑ÔºåÂè™ÊúâÂ§©Ê∞î„ÄÅËÆ°ÁÆóÊó∂Èó¥ÂíåËé∑ÂèñÊó∂Èó¥ÁöÑÂäüËÉΩ„ÄÇÊâÄ‰ª•ÊúÄÂêàÁêÜÁöÑÂõûÂ∫îÊòØËØ∑Ê±ÇÂüéÂ∏ÇÂêçÁß∞„ÄÇ\\n\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"ÂçóÊûÅ\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\n\",\n      \"reasoning_content\": \"\\nÂ•ΩÁöÑÔºåÁî®Êà∑Áé∞Âú®ÊèêÂà∞‚ÄúÂçóÊûÅ‚ÄùÔºåÈúÄË¶ÅÊü•ËØ¢ÈÇ£ÈáåÁöÑÂ§©Ê∞î„ÄÇÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁ°ÆËÆ§Áî®Êà∑ÊòØÂê¶ÁúüÁöÑÊÉ≥Êü•ËØ¢ÂçóÊûÅÁöÑÂ§©Ê∞î„ÄÇÂçóÊûÅÊòØ‰∏Ä‰∏™Â§ßÊ¥≤ÔºåÂ§©Ê∞îÊÉÖÂÜµÂèØËÉΩÊØîËæÉÁâπÊÆäÔºåËÄå‰∏îÁî®Êà∑ÂèØËÉΩÊ≤°ÊúâÊÑèËØÜÂà∞ÈúÄË¶ÅÊåáÂÆöÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ‰∏çËøáÔºåÊ†πÊçÆ‰πãÂâçÁöÑÂØπËØùÔºåÁî®Êà∑ÂèØËÉΩÊòØÂú®ÊµãËØïÊàñËÄÖÊúâÂÖ∂‰ªñÊÑèÂõæÔºå‰ΩÜÊ†πÊçÆÂ∑•ÂÖ∑‰∏≠ÁöÑsearch_weatherÂáΩÊï∞ÔºåÂèÇÊï∞ÊòØÂüéÂ∏ÇÔºåÊâÄ‰ª•ÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•Á°ÆËÆ§„ÄÇ\\n\\n‰∏çËøáÔºåÁî®Êà∑‰πãÂâçÂ∑≤ÁªèÈóÆËøá‚Äú‰ªäÊó•Â§©Ê∞î‚ÄùÔºåÁÑ∂ÂêéÊàëËÆ©‰ªñÊåáÂÆöÂüéÂ∏ÇÔºåÁé∞Âú®‰ªñÁõ¥Êé•ÂõûÁ≠î‚ÄúÂçóÊûÅ‚ÄùÔºåÂèØËÉΩÊòØÊåáÊï¥‰∏™ÂçóÊûÅÊ¥≤ÁöÑÂ§©Ê∞îÊÉÖÂÜµ„ÄÇ‰ΩÜÂçóÊûÅÊ¥≤Ê≤°ÊúâÁªü‰∏ÄÁöÑÂüéÂ∏ÇÔºåÂèØËÉΩÊúâÁßëÁ†îÁ´ôÔºåÊØîÂ¶ÇÈ∫¶ÂÖãÈªòÂ§öÁ´ô„ÄÅÈïøÂüéÁ´ôÁ≠â„ÄÇËøôÊó∂ÂÄôÊàëÈúÄË¶ÅÂà§Êñ≠ÊòØÂê¶Â∫îËØ•Áõ¥Êé•‰ΩøÁî®‚ÄúÂçóÊûÅ‚Äù‰Ωú‰∏∫ÂüéÂ∏ÇÂèÇÊï∞ÔºåÊàñËÄÖÊòØÂê¶ÈúÄË¶ÅÊèêÁ§∫Áî®Êà∑ËæìÂÖ•ÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ\\n\\nÊ†πÊçÆÂ∑•ÂÖ∑ÁöÑÊèèËø∞Ôºåsearch_weatherÂáΩÊï∞ÁöÑÂèÇÊï∞ÊòØcityÔºåÁ±ªÂûã‰∏∫Â≠óÁ¨¶‰∏≤Ôºå‰ΩÜÂèØËÉΩËØ•ÂáΩÊï∞ÂÜÖÈÉ®Â§ÑÁêÜ‰∫ÜÁ±ª‰ºº‚ÄúÂçóÊûÅ‚ÄùÁöÑËæìÂÖ•ÔºåÊàñËÄÖÁî®Êà∑ÂèØËÉΩÊúüÊúõÂæóÂà∞ÂçóÊûÅÊ¥≤ÁöÑÂ§©Ê∞îÊ¶ÇÂÜµ„ÄÇ‰πüÊúâÂèØËÉΩÁî®Êà∑ËæìÂÖ•ÊúâËØØÔºåÊàñËÄÖÊÉ≥‰∫ÜËß£Êüê‰∏™ÁâπÂÆöÂüéÂ∏ÇÁöÑÂ§©Ê∞îÔºå‰ΩÜËØØÂÜô‰∏∫ÂçóÊûÅ„ÄÇ‰æãÂ¶ÇÔºåÁî®Êà∑ÂèØËÉΩÊÉ≥Êü•Âåó‰∫¨„ÄÅ‰∏äÊµ∑Á≠âÂüéÂ∏ÇÔºå‰ΩÜÈîôËØØËæìÂÖ•‰∫Ü‚ÄúÂçóÊûÅ‚Äù„ÄÇ\\n\\nËøôÊó∂ÂÄôÔºåÊàëÈúÄË¶ÅÂÖàË∞ÉÁî®search_weatherÂáΩÊï∞ÔºåÂ∞ùËØï‰ΩøÁî®‚ÄúÂçóÊûÅ‚Äù‰Ωú‰∏∫cityÂèÇÊï∞ÔºåÁúãÁúãÊòØÂê¶ËÉΩËøîÂõûÊúâÊïàÁªìÊûú„ÄÇÂ¶ÇÊûúËøîÂõûÈîôËØØÔºåÂèØËÉΩÈúÄË¶ÅÊèêÁ§∫Áî®Êà∑Á°ÆËÆ§ÂüéÂ∏ÇÂêçÁß∞„ÄÇ‰ΩÜÊ†πÊçÆÂΩìÂâçÂ∑•ÂÖ∑ÁöÑËÆæÂÆöÔºåÂèØËÉΩÈúÄË¶ÅÁõ¥Êé•Ë∞ÉÁî®ÂáΩÊï∞ÔºåÁÑ∂ÂêéÊ†πÊçÆÁªìÊûúÂ§ÑÁêÜ„ÄÇÂ¶ÇÊûúÂáΩÊï∞ÊîØÊåÅÂú∞Âå∫Êü•ËØ¢ÔºåÂèØËÉΩÊ≤°ÈóÆÈ¢òÔºõÂ¶ÇÊûú‰∏çÊîØÊåÅÔºåÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•ÊæÑÊ∏Ö„ÄÇ\\n\\nÂõ†Ê≠§ÔºåÊ≠£Á°ÆÁöÑÂÅöÊ≥ïÊòØÂÖàË∞ÉÁî®search_weatherÂáΩÊï∞ÔºåÂèÇÊï∞‰∏∫city: \\\"ÂçóÊûÅ\\\"ÔºåÁÑ∂ÂêéÊ†πÊçÆËøîÂõûÁöÑÊï∞ÊçÆÂÜ≥ÂÆöÊòØÂê¶ÈúÄË¶ÅËøõ‰∏ÄÊ≠•ËØ¢ÈóÆÁî®Êà∑ÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ‰ΩÜÊ†πÊçÆÁî®Êà∑‰πãÂâçÁöÑÂØπËØùÔºå‰ªñ‰ª¨ÂèØËÉΩÂè™ÊòØÊÉ≥‰∫ÜËß£ÂçóÊûÅÁöÑÂ§©Ê∞îÊÉÖÂÜµÔºåÊâÄ‰ª•Áõ¥Êé•Ë∞ÉÁî®ÂáΩÊï∞ÊòØÂêàÈÄÇÁöÑ„ÄÇ\\n\",\n      \"tool_calls\": [\n        {\n          \"id\": null,\n          \"type\": \"function\",\n          \"function\": {\n            \"name\": \"search_weather\",\n            \"arguments\": \" {\\\"city\\\": \\\"ÂçóÊûÅ\\\"}\"\n          }\n        }\n      ]\n    },\n    {\n      \"tool_call_id\": null,\n      \"role\": \"tool\",\n      \"name\": \"search_weather\",\n      \"content\": \"\\\"Weather in ÂçóÊûÅ: Sunny, 25¬∞C\\\"\"\n    }\n  ]\n}\n```\n[File: chat_ÂçóÊûÅ_20260206_035215.json]\n\nËøôÊ†∑ÂØπÂêóÔºå‰ΩÜÊòØÊâæÂú∞ÊñπÂ•ΩÈ∫ªÁÉ¶Ôºå‰∏ãÊ¨°‰Ω†ËøòÊòØÁõ¥Êé•ÁªôÊàëÂÆåÊï¥code"
    },
    {
      "role": "assistant",
      "content": "\n\n",
      "reasoning_content": "Áî®Êà∑Â∑≤ÁªèÂ∫îÁî®‰∫ÜÊàëÂª∫ËÆÆÁöÑ‰øÆÂ§çÔºå‰øÆÊîπ‰∫ÜToolManager.execute_toolÊñπÊ≥ïÔºåÊ∑ªÂä†‰∫ÜuuidÁîüÊàê„ÄÇ‰ΩÜÁî®Êà∑ËØ¥\"ÊâæÂú∞ÊñπÂ•ΩÈ∫ªÁÉ¶Ôºå‰∏ãÊ¨°‰Ω†ËøòÊòØÁõ¥Êé•ÁªôÊàëÂÆåÊï¥code\"„ÄÇÊÑèÊÄùÊòØ‰∏ãÊ¨°ÊàëÂ∫îËØ•Êèê‰æõÂÆåÊï¥ÁöÑ‰ª£Á†ÅÔºåËÄå‰∏çÊòØÂè™Êèê‰æõ‰øÆÊîπÈÉ®ÂàÜ„ÄÇ\n\n‰∏çËøáÔºåÁé∞Âú®Áî®Êà∑ÂèØËÉΩËøòÊúâÈóÆÈ¢òÔºö‰øÆÂ§çÂ∑≤ÁªèÂ∫îÁî®Ôºå‰ΩÜÂèØËÉΩËøòÈúÄË¶Å‰øÆÂ§çResultSaver._save_jsonÊñπÊ≥ïÔºåÁ°Æ‰øù‰øùÂ≠òÁöÑJSON‰∏≠tool_call_id‰∏ç‰∏∫null„ÄÇÁî®Êà∑Â∑≤ÁªèÂ∫îÁî®‰∫ÜÈÇ£‰∏™‰øÆÂ§çÂêóÔºü‰ªéÊèê‰æõÁöÑ‰ª£Á†ÅÁúãÔºåResultSaver._save_jsonÊñπÊ≥ï‰πüÂ∑≤ÁªèË¢´‰øÆÂ§ç‰∫ÜÔºåÊ∑ªÔøΩÔøΩÔøΩ‰∫ÜuuidÁîüÊàê„ÄÇ\n\n‰ΩÜÁî®Êà∑ÂèØËÉΩ‰ªçÁÑ∂ÈÅáÂà∞400ÈîôËØØÔºüÂõ†‰∏∫Áé∞ÊúâÁöÑJSONÊñá‰ª∂Ôºàchat_ÂçóÊûÅ_20260206_035215.jsonÔºâ‰∏≠tool_call_id‰ªçÁÑ∂ÊòØnull„ÄÇ‰øÆÂ§çÂè™ÂØπÊñ∞ÁîüÊàêÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®ÊúâÊïàÔºåÂØπÂ∑≤ÊúâÁöÑJSONÊñá‰ª∂Êó†Êïà„ÄÇÁî®Êà∑ÈúÄË¶ÅÊâãÂä®ÁºñËæëJSONÊñá‰ª∂ÔºåÊàñËÄÖÈáçÊñ∞ËøêË°åÂØπËØùÁîüÊàêÊñ∞ÁöÑJSON„ÄÇ\n\nÁî®Êà∑ÂèØËÉΩÂ∏åÊúõÊàëÊèê‰æõ‰∏Ä‰∏™ËÑöÊú¨ÊàñÊñπÊ≥ïÊù•‰øÆÂ§çÁé∞ÊúâÁöÑJSONÊñá‰ª∂„ÄÇÊàñËÄÖÔºåÊàëÂèØ‰ª•‰øÆÊîπMessageBuilder._load_contextÊñπÊ≥ïÔºåÂú®Âä†ËΩΩ‰∏ä‰∏ãÊñáÊó∂Ëá™Âä®‰øÆÂ§çnullÁöÑtool_call_id„ÄÇ\n\nËÆ©ÊàëÊ£ÄÊü•‰∏Ä‰∏ãMessageBuilder._load_contextÊñπÊ≥ïÔºöÂÆÉÂè™ÊòØÂä†ËΩΩJSONÊñá‰ª∂Âπ∂ËøîÂõûÊ∂àÊÅØÂàóË°®„ÄÇÊàëÂèØ‰ª•‰øÆÊîπËøô‰∏™ÊñπÊ≥ïÔºåÂú®Âä†ËΩΩÊó∂‰øÆÂ§çnullÁöÑtool_call_id„ÄÇ\n\nÂè¶Â§ñÔºåÁî®Êà∑ÂèØËÉΩËøòÂ∏åÊúõ‰øÆÂ§çtool_callsÊï∞ÁªÑ‰∏≠ÁöÑidÂ≠óÊÆµÔºàÂú®assistantÊ∂àÊÅØ‰∏≠ÔºâÔºåÁõÆÂâç‰πüÊòØnull„ÄÇ\n\nÊâÄ‰ª•ÔºåÊàëÈúÄË¶Å‰øÆÊîπMessageBuilder._load_contextÊñπÊ≥ïÔºåÈÅçÂéÜÊâÄÊúâÊ∂àÊÅØÔºå‰øÆÂ§çtool_call_idÂíåtool_calls.id„ÄÇ\n\nËÆ©ÊàëÊèê‰æõ‰∏Ä‰∏™‰øÆÂ§çÁâàÊú¨ÁöÑMessageBuilder._load_contextÊñπÊ≥ï„ÄÇ‰ΩÜÁî®Êà∑ÊÉ≥Ë¶ÅÂÆåÊï¥ÁöÑ‰ª£Á†ÅÔºåÊàëÂèØ‰ª•Êèê‰æõÊï¥‰∏™Á±ªÁöÑ‰øÆÊîπÔºåÊàñËÄÖÂè™Êèê‰æõ‰øÆÊîπÁöÑÊñπÊ≥ï„ÄÇËÄÉËôëÂà∞Áî®Êà∑ËØ¥\"ÊâæÂú∞ÊñπÂ•ΩÈ∫ªÁÉ¶\"ÔºåÊàëÂ∫îËØ•Êèê‰æõÂÆåÊï¥ÁöÑMessageBuilderÁ±ªÔºåÊàñËÄÖËá≥Â∞ëÊèê‰æõ‰øÆÊîπÁöÑÊñπÊ≥ï„ÄÇ\n\nÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁúãÁúãÂΩìÂâçÁöÑMessageBuilderÁ±ªÂú®‰ª£Á†Å‰∏≠ÁöÑ‰ΩçÁΩÆ„ÄÇ‰ªéÊèê‰æõÁöÑ‰ª£Á†ÅÁúãÔºåMessageBuilderÁ±ªÂú®Á¨¨352Ë°åÂºÄÂßã„ÄÇÊàëÈúÄË¶Å‰øÆÊîπ_load_contextÊñπÊ≥ïÔºåÊàñËÄÖÊ∑ªÂä†‰∏Ä‰∏™ËæÖÂä©ÊñπÊ≥ïÊù•‰øÆÂ§çÊ∂àÊÅØ„ÄÇ\n\nÊõ¥Â•ΩÁöÑÊñπÊ≥ïÊòØÂú®Âä†ËΩΩ‰∏ä‰∏ãÊñáÊó∂Ëá™Âä®‰øÆÂ§ç„ÄÇÊàëÂ∞Ü‰øÆÊîπ_load_contextÊñπÊ≥ïÔºåÊ∑ªÂä†‰∏Ä‰∏™ÈÄíÂΩíÂáΩÊï∞Êù•‰øÆÂ§çÊ∂àÊÅØ‰∏≠ÁöÑtool_call_idÂíåtool_calls.id„ÄÇ\n\nËÆ©ÊàëÁºñÂÜô‰øÆÂ§ç‰ª£Á†ÅÔºö",
      "tool_calls": [
        {
          "id": "50d14990-0216-4296-b5d8-fa6cf3d61a76",
          "type": "function",
          "function": {
            "name": "get_current_time",
            "arguments": "{\"timezone\": \"UTC\"}"
          }
        }
      ]
    },
    {
      "tool_call_id": "7163d582-dcb7-41ca-86fe-15985cf0e2af",
      "role": "tool",
      "name": "get_current_time",
      "content": "\"Current time in UTC: 2026-02-06 04:07:38\""
    },
    {
      "role": "user",
      "content": "```py\n#!/usr/bin/env python3\n\"\"\"\nAdvanced Typewriter Effect HTTP Client for OpenAI-compatible APIs (Enhanced Version).\nZero-default policy with graceful fallback - avoids crashes when encountering unsupported parameters.\n\"\"\"\n\nimport argparse\nimport base64\nimport importlib.util\nimport inspect\nimport io\nimport json\nimport logging\nimport mimetypes\nimport os\nimport re\nimport signal\nimport sys\nimport time\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional, Dict, List, Any, Tuple, Union, Callable\n\nimport requests\nfrom dotenv import load_dotenv\n\n# ============ Setup & Constants ============\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    stream=sys.stderr,\n)\nlogger = logging.getLogger(__name__)\n\nCODE_BLOCK = \"`\" * 3\n\n\nclass Constants:\n    \"\"\"Application constants\"\"\"\n\n    CHUNK_SIZE = 1024\n    STREAM_DELAY_REASONING = 0.01\n    STREAM_DELAY_CONTENT = 0.03\n    TIMEOUT = 60 * 60 * 6  # ‰øùÊåÅ 1 Â∞èÊó∂Ë∂ÖÊó∂\n\n\nclass Colors:\n    \"\"\"ANSI Color codes for terminal output.\"\"\"\n\n    GREEN = \"\\033[92m\"\n    YELLOW = \"\\033[93m\"\n    RED = \"\\033[91m\"\n    BLUE = \"\\033[94m\"\n    CYAN = \"\\033[96m\"\n    BOLD = \"\\033[1m\"\n    END = \"\\033[0m\"\n\n    @classmethod\n    def info(cls, text: str) -> str:\n        return f\"{cls.GREEN}{text}{cls.END}\"\n\n    @classmethod\n    def warn(cls, text: str) -> str:\n        return f\"{cls.YELLOW}{text}{cls.END}\"\n\n    @classmethod\n    def error(cls, text: str) -> str:\n        return f\"{cls.RED}{text}{cls.END}\"\n\n    @classmethod\n    def bold(cls, text: str) -> str:\n        return f\"{cls.BOLD}{text}{cls.END}\"\n\n\n# ============ Exceptions ============\n\n\nclass ChatError(Exception):\n    \"\"\"Base exception for chat errors\"\"\"\n\n    pass\n\n\nclass ConfigurationError(ChatError):\n    \"\"\"Missing required configuration\"\"\"\n\n    pass\n\n\nclass APIError(ChatError):\n    \"\"\"API communication error\"\"\"\n\n    pass\n\n\nclass FileProcessingError(ChatError):\n    \"\"\"File I/O error\"\"\"\n\n    pass\n\n\nclass ToolError(ChatError):\n    \"\"\"Tool execution error\"\"\"\n\n    pass\n\n\n# ============ UTF-8 Setup ============\n\n\ndef setup_utf8():\n    \"\"\"Force UTF-8 encoding for Windows and Unix\"\"\"\n    if sys.platform == \"win32\":\n        import ctypes\n\n        try:\n            kernel32 = ctypes.windll.kernel32\n            kernel32.SetConsoleCP(65001)\n            kernel32.SetConsoleOutputCP(65001)\n        except Exception:\n            pass\n\n    if hasattr(sys.stdout, \"buffer\"):\n        sys.stdout = io.TextIOWrapper(\n            sys.stdout.buffer, encoding=\"utf-8\", errors=\"replace\", line_buffering=True\n        )\n\n\n# ============ Data Models ============\n\n\n@dataclass\nclass APIConfig:\n    \"\"\"API connection configuration - only endpoint and api_key\"\"\"\n\n    endpoint: str = \"\"\n    api_key: str = \"\"\n    timeout: int = Constants.TIMEOUT  # HTTP timeout, not API parameter\n\n    def validate(self) -> None:\n        if not self.endpoint:\n            raise ConfigurationError(\"API endpoint is required\")\n        if not self.api_key:\n            raise ConfigurationError(\"API key is required\")\n\n\n@dataclass\nclass RequestMetadata:\n    \"\"\"Metadata for request tracking and reporting\"\"\"\n\n    start_time: datetime = field(default_factory=datetime.now)\n    end_time: Optional[datetime] = None\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n    total_tokens: int = 0\n    finish_reason: Optional[str] = None\n    model: Optional[str] = None\n    duration: float = 0.0\n    tool_calls: List[Dict] = field(default_factory=list)\n    tool_results: List[Dict] = field(default_factory=list)\n\n    def finalize(self) -> None:\n        self.end_time = datetime.now()\n        self.duration = (self.end_time - self.start_time).total_seconds()\n\n    def get_token_rate(self) -> Optional[float]:\n        \"\"\"Calculate tokens per second\"\"\"\n        if self.duration > 0 and self.completion_tokens > 0:\n            return self.completion_tokens / self.duration\n        return None\n\n\n# ============ Tool System ============\n\n\nclass ToolManager:\n    \"\"\"Manages tool functions and execution\"\"\"\n\n    def __init__(self, tools_module_path: str = \"tools.py\"):\n        self.tools_module_path = tools_module_path\n        self.available_functions: Dict[str, Callable] = {}\n        self.tool_definitions: List[Dict] = []\n        self._load_tools()\n\n    def _load_tools(self):\n        \"\"\"Load tool functions from tools.py module\"\"\"\n        try:\n            spec = importlib.util.spec_from_file_location(\n                \"tools\", self.tools_module_path\n            )\n            if spec and spec.loader:\n                tools_module = importlib.util.module_from_spec(spec)\n                spec.loader.exec_module(tools_module)\n\n                # Find all functions in the module\n                for name, obj in inspect.getmembers(tools_module):\n                    if inspect.isfunction(obj) and not name.startswith(\"_\"):\n                        # Check if function has proper annotations for OpenAI tools\n                        sig = inspect.signature(obj)\n                        params = {}\n                        for param_name, param in sig.parameters.items():\n                            if param_name != \"self\":\n                                param_type = (\n                                    str(param.annotation)\n                                    if param.annotation != inspect.Parameter.empty\n                                    else \"string\"\n                                )\n                                params[param_name] = {\n                                    \"type\": param_type.replace(\"<class '\", \"\")\n                                    .replace(\"'>\", \"\")\n                                    .lower(),\n                                    \"description\": f\"Parameter {param_name}\",\n                                }\n\n                        # Create tool definition\n                        tool_def = {\n                            \"type\": \"function\",\n                            \"function\": {\n                                \"name\": name,\n                                \"description\": obj.__doc__ or f\"Function {name}\",\n                                \"parameters\": {\n                                    \"type\": \"object\",\n                                    \"properties\": params,\n                                    \"required\": list(params.keys()),\n                                },\n                            },\n                        }\n\n                        self.tool_definitions.append(tool_def)\n                        self.available_functions[name] = obj\n\n                        print(\n                            f\"{Colors.info('‚úì')} Loaded tool: {name}\", file=sys.stderr\n                        )\n\n                if not self.tool_definitions:\n                    print(\n                        f\"{Colors.warn('‚ö†')} No tools found in {self.tools_module_path}\",\n                        file=sys.stderr,\n                    )\n            else:\n                print(\n                    f\"{Colors.warn('‚ö†')} Tools module not found: {self.tools_module_path}\",\n                    file=sys.stderr,\n                )\n        except Exception as e:\n            print(f\"{Colors.warn('‚ö†')} Failed to load tools: {e}\", file=sys.stderr)\n\n    def execute_tool(self, tool_call: Dict) -> Dict:\n        \"\"\"Execute a tool call and return result\"\"\"\n        try:\n            function_name = tool_call[\"function\"][\"name\"]\n            function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n\n            if function_name not in self.available_functions:\n                raise ToolError(f\"Unknown function: {function_name}\")\n\n            print(\n                f\"\\n{Colors.CYAN}üîß Executing tool: {function_name}{Colors.END}\",\n                file=sys.stderr,\n            )\n            print(f\"   Arguments: {function_args}\", file=sys.stderr)\n\n            # Execute the function\n            result = self.available_functions[function_name](**function_args)\n\n            print(f\"   Result: {result}\", file=sys.stderr)\n\n            # ‰øÆÂ§çÔºöÂ§ÑÁêÜnullÊàñÁ©∫ÁöÑtool_call_id\n            tool_call_id = tool_call.get(\"id\")\n            if not tool_call_id:\n                import uuid\n\n                tool_call_id = str(uuid.uuid4())\n                print(\n                    f\"{Colors.warn('‚ö†')} Generated tool call ID: {tool_call_id}\",\n                    file=sys.stderr,\n                )\n\n            return {\n                \"tool_call_id\": tool_call_id,\n                \"role\": \"tool\",\n                \"name\": function_name,\n                \"content\": json.dumps(result, ensure_ascii=False),\n            }\n\n        except Exception as e:\n            error_msg = f\"Tool execution failed: {str(e)}\"\n            print(f\"{Colors.error('‚ùå')} {error_msg}\", file=sys.stderr)\n\n            # ‰øÆÂ§çÔºöÈîôËØØÂìçÂ∫î‰πüÁîüÊàêID\n            tool_call_id = tool_call.get(\"id\")\n            if not tool_call_id:\n                import uuid\n\n                tool_call_id = str(uuid.uuid4())\n\n            return {\n                \"tool_call_id\": tool_call_id,\n                \"role\": \"tool\",\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": json.dumps({\"error\": error_msg}, ensure_ascii=False),\n            }\n\n    def get_tool_definitions(self) -> List[Dict]:\n        \"\"\"Get OpenAI-compatible tool definitions\"\"\"\n        return self.tool_definitions\n\n\n# ============ Configuration Management ============\n\n\nclass ConfigManager:\n    \"\"\"Manages configuration loading with graceful fallback\"\"\"\n\n    ENV_PATHS = [\".env\", \"../.env\", Path.home() / \".ai_chat.env\"]\n    PROFILE_PATHS = [\n        Path(\"profiles.json\"),\n        Path(\"../profiles.json\"),\n        Path.home() / \".ai_chat_profiles.json\",\n    ]\n\n    @staticmethod\n    def load_connection_config(args) -> APIConfig:\n        \"\"\"Load endpoint and api_key with priority: CLI > Profile > Env > Default\"\"\"\n        config = APIConfig()\n\n        # 1. Load .env files first\n        for env_path in ConfigManager.ENV_PATHS:\n            if Path(env_path).exists():\n                load_dotenv(env_path)\n                logger.info(f\"Loaded environment: {env_path}\")\n                break\n\n        # 2. From environment variables\n        config.endpoint = os.getenv(\"ENDPOINT\", \"\")\n        config.api_key = os.getenv(\"API_KEY\", \"\")\n\n        # 3. Try to load from profile\n        profile_config = ConfigManager._load_profile(args.profile)\n        if profile_config:\n            if \"endpoint\" in profile_config:\n                config.endpoint = profile_config[\"endpoint\"]\n            if \"api_key\" in profile_config:\n                config.api_key = profile_config[\"api_key\"]\n            if \"apiKey\" in profile_config:\n                config.api_key = profile_config[\"apiKey\"]\n\n        # 4. CLI arguments override everything\n        if hasattr(args, \"endpoint\") and args.endpoint:\n            config.endpoint = args.endpoint\n        if hasattr(args, \"api_key\") and args.api_key:\n            config.api_key = args.api_key\n\n        return config\n\n    @staticmethod\n    def _load_profile(profile_name: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Load profile from various locations\"\"\"\n        for path in ConfigManager.PROFILE_PATHS:\n            if path.exists():\n                try:\n                    with open(path, \"r\", encoding=\"utf-8\") as f:\n                        profiles = json.load(f)\n\n                    if profile_name in profiles:\n                        print(\n                            f\"{Colors.info('‚úì')} Loaded profile: {profile_name} from {path}\",\n                            file=sys.stderr,\n                        )\n                        return profiles[profile_name]\n                except Exception as e:\n                    logger.error(f\"Failed to load profile {path}: {e}\")\n\n        if profile_name != \"default\":\n            print(\n                f\"{Colors.warn('‚ö†')} Profile '{profile_name}' not found, using default\",\n                file=sys.stderr,\n            )\n        return None\n\n    @staticmethod\n    def load_request_body(config_path: str, args) -> Dict[str, Any]:\n        \"\"\"Load request body from config.json with safe parameter handling\"\"\"\n        path = Path(config_path).expanduser().resolve()\n\n        if not path.exists():\n            if (\n                config_path != \"config.json\"\n            ):  # User explicitly specified non-existent file\n                raise FileNotFoundError(f\"Config file not found: {config_path}\")\n            # Use empty config if default config.json doesn't exist\n            return {}\n\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            print(f\"{Colors.info('‚úì')} Loaded request config: {path}\", file=sys.stderr)\n\n            # Apply CLI overrides for safety-critical parameters\n            ConfigManager._apply_cli_overrides(data, args)\n\n            return data\n        except Exception as e:\n            raise ValueError(f\"Failed to load config file ({config_path}): {e}\")\n\n    @staticmethod\n    def _apply_cli_overrides(data: Dict[str, Any], args) -> None:\n        \"\"\"Apply CLI arguments to request body, removing unsupported parameters\"\"\"\n        # Safety: Remove problematic parameters if CLI provides alternatives\n        if hasattr(args, \"model\") and args.model:\n            data[\"model\"] = args.model\n            # Remove any model-specific parameters that might conflict\n            data.pop(\"temperature\", None)  # Some models don't support temperature\n            data.pop(\"top_p\", None)\n            data.pop(\"frequency_penalty\", None)\n            data.pop(\"presence_penalty\", None)\n\n        # OpenAI standard parameters override\n        if hasattr(args, \"temperature\") and args.temperature is not None:\n            data[\"temperature\"] = args.temperature\n        if hasattr(args, \"max_tokens\") and args.max_tokens is not None:\n            data[\"max_tokens\"] = args.max_tokens\n        if hasattr(args, \"top_p\") and args.top_p is not None:\n            data[\"top_p\"] = args.top_p\n        if hasattr(args, \"presence_penalty\") and args.presence_penalty is not None:\n            data[\"presence_penalty\"] = args.presence_penalty\n        if hasattr(args, \"frequency_penalty\") and args.frequency_penalty is not None:\n            data[\"frequency_penalty\"] = args.frequency_penalty\n        if hasattr(args, \"seed\") and args.seed is not None:\n            data[\"seed\"] = args.seed\n\n        # Stream handling\n        if hasattr(args, \"no_stream\") and args.no_stream:\n            data[\"stream\"] = False\n        elif \"stream\" not in data:\n            data[\"stream\"] = True  # Default to stream for better UX\n\n        # Thinking enable/disable\n        if hasattr(args, \"enable_thinking\"):\n            if args.enable_thinking is not None:\n                data[\"enable_thinking\"] = args.enable_thinking\n                # Some APIs don't support reasoning_content, remove if disabled\n                if not args.enable_thinking:\n                    data.pop(\"reasoning_content\", None)\n\n    @staticmethod\n    def save_context_to_config(config_path: str, messages: List[Dict]) -> None:\n        \"\"\"Save conversation context back to config.json\"\"\"\n        try:\n            path = Path(config_path).expanduser().resolve()\n\n            # Load existing config\n            if path.exists():\n                with open(path, \"r\", encoding=\"utf-8\") as f:\n                    config_data = json.load(f)\n            else:\n                config_data = {}\n\n            # Update messages\n            config_data[\"messages\"] = messages\n\n            # Save back\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(config_data, f, ensure_ascii=False, indent=2)\n\n            print(f\"{Colors.info('‚úì')} Saved context to: {path}\", file=sys.stderr)\n\n        except Exception as e:\n            print(f\"{Colors.warn('‚ö†')} Failed to save context: {e}\", file=sys.stderr)\n\n\n# ============ File & Message Building ============\n\n\nclass MessageBuilder:\n    \"\"\"Builds messages with file context support\"\"\"\n\n    IMAGE_TYPES = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\", \".bmp\"}\n\n    @staticmethod\n    def load_file(filepath: str) -> Union[str, Dict[str, Any]]:\n        \"\"\"Load file content with MIME type detection\"\"\"\n        if filepath.startswith(\"@\"):\n            filepath = filepath[1:]\n\n        path = Path(filepath).expanduser().resolve()\n        if not path.exists():\n            raise FileNotFoundError(f\"File not found: {filepath}\")\n\n        mime_type, _ = mimetypes.guess_type(path)\n        if not mime_type:\n            mime_type = \"application/octet-stream\"\n\n        # Image handling\n        if (\n            mime_type.startswith(\"image/\")\n            or path.suffix.lower() in MessageBuilder.IMAGE_TYPES\n        ):\n            with open(path, \"rb\") as f:\n                encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n            return {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:{mime_type};base64,{encoded}\"},\n            }\n\n        # Text file handling with encoding detection\n        encodings = [\"utf-8\", \"utf-8-sig\", \"gbk\", \"latin-1\"]\n        content = None\n\n        for enc in encodings:\n            try:\n                with open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n                    content = f.read()\n                break\n            except UnicodeDecodeError:\n                continue\n\n        if content is None:\n            with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n                content = f.read()\n\n        # Format based on file type\n        suffix = path.suffix[1:] if path.suffix else \"text\"\n        if path.suffix in [\".md\", \".txt\", \".rst\"]:\n            return f\"{content}\\n\\n[File: {path.name}]\"\n        else:\n            return f\"{CODE_BLOCK}{suffix}\\n{content}\\n{CODE_BLOCK}\\n[File: {path.name}]\"\n\n    @staticmethod\n    def build_messages(args, config_data: Dict | None = None) -> List[Dict[str, Any]]:\n        \"\"\"Build final message list with priority: context > config > prompt\"\"\"\n        messages = []\n\n        # Extract system prompt from config if exists\n        system_prompt = None\n        config_messages = []\n        if config_data:\n            system_prompt = config_data.pop(\"system\", None)\n            config_messages = config_data.pop(\"messages\", [])\n\n        # 1. Add system prompt if exists (always first)\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n            print(f\"{Colors.info('‚úì')} Added system prompt\", file=sys.stderr)\n\n        # 2. Load context if provided\n        if hasattr(args, \"context\") and args.context:\n            try:\n                context_messages = MessageBuilder._load_context(args.context)\n                messages.extend(context_messages)\n                print(\n                    f\"{Colors.info('‚úì')} Loaded conversation context: {args.context}\",\n                    file=sys.stderr,\n                )\n            except Exception as e:\n                print(\n                    f\"{Colors.warn('‚ö†')} Failed to load context: {e}\", file=sys.stderr\n                )\n\n        # 3. Add messages from config\n        if config_messages:\n            messages.extend(config_messages)\n\n        # 4. Add current user input\n        if hasattr(args, \"prompt\") and args.prompt:\n            user_messages = MessageBuilder._build_user_messages(args.prompt)\n            messages.extend(user_messages)\n\n        if not messages and not system_prompt:\n            raise ConfigurationError(\n                \"No messages to send. Please provide prompt or messages in config.\"\n            )\n\n        return messages\n\n    @staticmethod\n    def _load_context(context_path: str) -> List[Dict]:\n        \"\"\"Load conversation history from JSON file\"\"\"\n        path = Path(context_path).expanduser().resolve()\n        if not path.exists():\n            raise FileNotFoundError(f\"Context file not found: {context_path}\")\n\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n\n        if isinstance(data, list):\n            return data\n        elif isinstance(data, dict) and \"messages\" in data:\n            return data[\"messages\"]\n        else:\n            raise ValueError(\n                \"Invalid context format: expected list or {messages: [...]}\"\n            )\n\n    @staticmethod\n    def _build_user_messages(prompt_parts: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Build user messages from prompt parts\"\"\"\n        if not prompt_parts:\n            return []\n\n        content_parts = []\n        has_image = False\n\n        for part in prompt_parts:\n            if part.startswith(\"@\"):\n                try:\n                    file_content = MessageBuilder.load_file(part)\n                    if isinstance(file_content, dict):\n                        has_image = True\n                        print(\n                            f\"{Colors.info('‚úì')} Attached image: {part[1:]}\",\n                            file=sys.stderr,\n                        )\n                    else:\n                        print(\n                            f\"{Colors.info('‚úì')} Attached file: {part[1:]}\",\n                            file=sys.stderr,\n                        )\n                    content_parts.append(file_content)\n                except Exception as e:\n                    print(\n                        f\"{Colors.warn('‚ö†')} Failed to load file {part}: {e}\",\n                        file=sys.stderr,\n                    )\n                    content_parts.append(f\"[Failed to load file: {part}]\")\n            else:\n                content_parts.append(part)\n\n        # Build message content\n        if has_image:\n            multimodal_content = []\n            current_text = []\n\n            for part in content_parts:\n                if isinstance(part, dict):\n                    if current_text:\n                        multimodal_content.append(\n                            {\"type\": \"text\", \"text\": \"\\n\".join(current_text)}\n                        )\n                        current_text = []\n                    multimodal_content.append(part)\n                else:\n                    current_text.append(part)\n\n            if current_text:\n                multimodal_content.append(\n                    {\"type\": \"text\", \"text\": \"\\n\\n\".join(current_text)}\n                )\n\n            return [{\"role\": \"user\", \"content\": multimodal_content}]\n        else:\n            user_content = \"\\n\\n\".join(content_parts)\n            return [{\"role\": \"user\", \"content\": user_content}]\n\n\n# ============ Typewriter Output ============\n\n\nclass TypewriterPrinter:\n    \"\"\"Typewriter effect printer with file output\"\"\"\n\n    def __init__(self, output_file: Optional[io.TextIOWrapper] = None):\n        self.output_file = output_file\n        self.reasoning_buffer = \"\"\n        self.content_buffer = \"\"\n        self.reasoning_printed = 0\n        self.content_printed = 0\n        self.in_reasoning = True\n        self.start_time = time.time()\n        self.first_token_time = None\n        self.token_count = 0\n\n        # Performance tracking\n        self.reasoning_start = None\n        self.content_start = None\n\n    def write(self, text: str, is_reasoning: bool = False):\n        \"\"\"Write text to file\"\"\"\n        if self.output_file:\n            try:\n                self.output_file.write(text)\n                if not is_reasoning:\n                    self.output_file.flush()\n            except Exception as e:\n                print(f\"\\n{Colors.error('[File write error]')} {e}\", file=sys.stderr)\n\n    def update_reasoning(self, delta: str):\n        \"\"\"Update and print reasoning content\"\"\"\n        if not delta:\n            return\n\n        if self.first_token_time is None:\n            self.first_token_time = time.time()\n\n        if self.reasoning_start is None:\n            self.reasoning_start = time.time()\n            print(f\"\\n{Colors.BLUE}üí≠ Thinking:{Colors.END}\")\n            if self.output_file:\n                self.write(\"\\nüí≠ Thinking:\\n\", is_reasoning=True)\n\n        self.reasoning_buffer += delta\n        new_text = self.reasoning_buffer[self.reasoning_printed :]\n\n        # Print to console with typewriter effect\n        for char in new_text:\n            print(char, end=\"\", flush=True)\n            time.sleep(Constants.STREAM_DELAY_REASONING)\n\n        # Write to file\n        self.write(new_text, is_reasoning=True)\n\n        self.reasoning_printed = len(self.reasoning_buffer)\n        self.token_count += len(delta.split())  # Rough estimate\n\n    def switch_to_content(self):\n        \"\"\"Switch from reasoning to content output\"\"\"\n        if self.in_reasoning:\n            self.in_reasoning = False\n            self.content_start = time.time()\n\n            # Console output\n            print(f\"\\n{Colors.bold('='*50)}\")\n            print(f\"{Colors.CYAN}‚ú® Response:{Colors.END}\")\n            print(f\"{Colors.bold('='*50)}\")\n\n            # File output\n            if self.output_file:\n                self.write(f\"\\n\\n{'='*50}\\n‚ú® Response:\\n{'='*50}\\n\\n\")\n\n    def update_content(self, delta: str):\n        \"\"\"Update and print response content\"\"\"\n        if not delta:\n            return\n\n        if self.in_reasoning:\n            self.switch_to_content()\n\n        if self.first_token_time is None:\n            self.first_token_time = time.time()\n\n        self.content_buffer += delta\n        new_text = self.content_buffer[self.content_printed :]\n\n        # Typewriter effect\n        for char in new_text:\n            print(char, end=\"\", flush=True)\n            time.sleep(Constants.STREAM_DELAY_CONTENT)\n\n        self.write(new_text)\n\n        self.content_printed = len(self.content_buffer)\n        self.token_count += len(delta.split())  # Rough estimate\n\n    def finalize(self):\n        \"\"\"Finalize output\"\"\"\n        if self.output_file:\n            try:\n                self.output_file.flush()\n            except Exception as e:\n                print(f\"{Colors.warn('‚ö†')} Failed to flush file: {e}\", file=sys.stderr)\n\n\n# ============ HTTP Client ============\n\n\nclass APIClient:\n    \"\"\"HTTP client with retry and error handling using Session\"\"\"\n\n    def __init__(self, config: APIConfig, max_retries: int = 3):\n        self.config = config\n        self.max_retries = max_retries\n        self.session = requests.Session()\n        self.session.headers.update(\n            {\n                \"Authorization\": f\"Bearer {config.api_key}\",\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"text/event-stream\",\n            }\n        )\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.session.close()\n\n    def _estimate_tokens(self, messages: List[Dict]) -> Tuple[int, int]:\n        \"\"\"‰º∞ÁÆó Prompt ÁöÑ Token Êï∞Ôºà‰øùÂÆà‰º∞ËÆ°ÔºöÊØè 3 ‰∏™Â≠óÁ¨¶ 1 ‰∏™ TokenÔºâ\"\"\"\n        try:\n            # Â∞Ü messages ËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤ËÆ°ÁÆó\n            text = json.dumps(messages, ensure_ascii=False)\n            char_count = len(text)\n            # ‰øùÂÆà‰º∞ËÆ°ÔºöËã±ÊñáÁ∫¶ 4 Â≠óÁ¨¶/tokenÔºå‰∏≠ÊñáÁ∫¶ 1-2 Â≠óÁ¨¶/tokenÔºåÊ∑∑ÂêàÊåâ 3 ËÆ°ÁÆó\n            estimated_tokens = char_count // 3\n            return char_count, estimated_tokens\n        except:\n            return 0, 0\n\n    def request(\n        self,\n        payload: Dict[str, Any],\n        printer: TypewriterPrinter,\n        tool_manager: Optional[ToolManager] = None,\n    ) -> RequestMetadata:\n        \"\"\"Execute API request with retry logic\"\"\"\n        metadata = RequestMetadata(model=payload.get(\"model\"))\n        is_stream = payload.get(\"stream\", True)\n\n        # ËÆ°ÁÆó Prompt ‰ø°ÊÅØ\n        messages = payload.get(\"messages\", [])\n        char_count, estimated_tokens = self._estimate_tokens(messages)\n        max_tokens = payload.get(\"max_tokens\", \"Not set\")\n\n        # ÊòæÁ§∫ËØ∑Ê±Ç‰ø°ÊÅØÔºåÂåÖÊã¨ Token È¢Ñ‰º∞\n        print(\n            f\"\\n{Colors.CYAN}üöÄ Requesting {metadata.model or 'unknown model'}...{Colors.END}\"\n        )\n        print(f\"   Endpoint: {self.config.endpoint}\")\n        print(f\"   Stream: {'Yes' if is_stream else 'No'}\")\n        print(f\"   Prompt Size: {char_count} chars (Est. ~{estimated_tokens} tokens)\")\n\n        if max_tokens != \"Not set\":\n            print(f\"   Max Tokens: {max_tokens}\")\n        else:\n            print(f\"   Max Tokens: {Colors.warn('Not set')} (model default)\")\n\n        # Safety: Remove any empty or None parameters that might cause API errors\n        clean_payload = self._clean_payload(payload)\n\n        for attempt in range(self.max_retries + 1):\n            try:\n                response = self.session.post(\n                    self.config.endpoint,\n                    json=clean_payload,\n                    stream=is_stream,\n                    timeout=self.config.timeout,\n                )\n                response.raise_for_status()\n\n                if is_stream:\n                    return self._handle_stream(\n                        response, printer, metadata, tool_manager\n                    )\n                else:\n                    return self._handle_non_stream(\n                        response, printer, metadata, tool_manager\n                    )\n\n            except requests.exceptions.RequestException as e:\n                if attempt < self.max_retries:\n                    wait_time = 2**attempt\n                    print(\n                        f\"{Colors.warn(f'‚ö†Ô∏è  Request failed ({e}), retrying in {wait_time}s...')}\",\n                        file=sys.stderr,\n                    )\n                    time.sleep(wait_time)\n                else:\n                    raise APIError(\n                        f\"Request failed after {self.max_retries} retries: {e}\"\n                    )\n\n        raise APIError(\"Max retries exceeded\")\n\n    def _clean_payload(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove potentially problematic parameters\"\"\"\n        clean = payload.copy()\n\n        # Remove None values\n        for key in list(clean.keys()):\n            if clean[key] is None:\n                del clean[key]\n\n        # Remove empty strings\n        for key in list(clean.keys()):\n            if isinstance(clean[key], str) and not clean[key].strip():\n                del clean[key]\n\n        return clean\n\n    def _handle_stream(\n        self,\n        response,\n        printer: TypewriterPrinter,\n        metadata: RequestMetadata,\n        tool_manager: Optional[ToolManager],\n    ) -> RequestMetadata:\n        \"\"\"Handle streaming response\"\"\"\n        buffer = \"\"\n        tool_calls_buffer = {}\n\n        for chunk in response.iter_content(chunk_size=Constants.CHUNK_SIZE):\n            if not chunk:\n                continue\n\n            buffer += chunk.decode(\"utf-8\", errors=\"replace\")\n\n            while \"\\n\" in buffer:\n                line, buffer = buffer.split(\"\\n\", 1)\n                line = line.strip()\n\n                if not line.startswith(\"data: \"):\n                    continue\n\n                data_str = line[6:]\n                if data_str == \"[DONE]\":\n                    break\n\n                try:\n                    data = json.loads(data_str)\n                    self._process_stream_data(\n                        data, printer, metadata, tool_calls_buffer\n                    )\n                except json.JSONDecodeError:\n                    continue\n\n        # Handle tool calls after stream ends\n        if tool_calls_buffer and tool_manager:\n            self._handle_tool_calls(tool_calls_buffer, metadata, tool_manager)\n\n        metadata.finalize()\n        return metadata\n\n    def _process_stream_data(\n        self,\n        data: Dict,\n        printer: TypewriterPrinter,\n        metadata: RequestMetadata,\n        tool_calls_buffer: Dict,\n    ):\n        \"\"\"Process individual stream data chunk\"\"\"\n        # Update usage\n        if \"usage\" in data and data[\"usage\"]:\n            metadata.prompt_tokens = data[\"usage\"].get(\"prompt_tokens\", 0)\n            metadata.completion_tokens = data[\"usage\"].get(\"completion_tokens\", 0)\n            metadata.total_tokens = data[\"usage\"].get(\"total_tokens\", 0)\n\n        choices = data.get(\"choices\", [])\n        if not choices:\n            return\n\n        choice = choices[0]\n        delta = choice.get(\"delta\", {})\n\n        # Update finish reason\n        finish_reason = choice.get(\"finish_reason\")\n        if finish_reason:\n            metadata.finish_reason = finish_reason\n\n        # Process tool calls\n        tool_calls = delta.get(\"tool_calls\", [])\n        for tool_call in tool_calls:\n            idx = tool_call.get(\"index\", 0)\n            if idx not in tool_calls_buffer:\n                tool_calls_buffer[idx] = {\n                    \"id\": \"\",\n                    \"type\": \"function\",\n                    \"function\": {\"name\": \"\", \"arguments\": \"\"},\n                }\n\n            if \"id\" in tool_call:\n                tool_calls_buffer[idx][\"id\"] = tool_call[\"id\"]\n            if \"function\" in tool_call:\n                if \"name\" in tool_call[\"function\"]:\n                    tool_calls_buffer[idx][\"function\"][\"name\"] = tool_call[\"function\"][\n                        \"name\"\n                    ]\n                if \"arguments\" in tool_call[\"function\"]:\n                    tool_calls_buffer[idx][\"function\"][\"arguments\"] += tool_call[\n                        \"function\"\n                    ][\"arguments\"]\n\n        # Process content\n        reasoning_delta = delta.get(\"reasoning_content\") or delta.get(\"reasoning\") or \"\"\n        content_delta = delta.get(\"content\", \"\")\n\n        if reasoning_delta:\n            printer.update_reasoning(reasoning_delta)\n\n        if content_delta:\n            printer.update_content(content_delta)\n\n    def _handle_non_stream(\n        self,\n        response,\n        printer: TypewriterPrinter,\n        metadata: RequestMetadata,\n        tool_manager: Optional[ToolManager],\n    ) -> RequestMetadata:\n        \"\"\"Handle non-streaming response\"\"\"\n        data = response.json()\n        choice = data.get(\"choices\", [{}])[0]\n        message = choice.get(\"message\", {})\n\n        reasoning = message.get(\"reasoning_content\", \"\")\n        content = message.get(\"content\", \"\")\n\n        if reasoning:\n            printer.update_reasoning(reasoning)\n\n        printer.switch_to_content()\n        printer.update_content(content)\n\n        # Handle tool calls\n        tool_calls = message.get(\"tool_calls\", [])\n        if tool_calls and tool_manager:\n            self._handle_tool_calls(tool_calls, metadata, tool_manager)\n\n        # Update metadata\n        if \"usage\" in data:\n            metadata.prompt_tokens = data[\"usage\"].get(\"prompt_tokens\", 0)\n            metadata.completion_tokens = data[\"usage\"].get(\"completion_tokens\", 0)\n            metadata.total_tokens = data[\"usage\"].get(\"total_tokens\", 0)\n\n        metadata.finish_reason = choice.get(\"finish_reason\")\n        metadata.finalize()\n\n        return metadata\n\n    def _handle_tool_calls(\n        self,\n        tool_calls: Union[List, Dict],\n        metadata: RequestMetadata,\n        tool_manager: ToolManager,\n    ):\n        \"\"\"Handle tool calls and execute them\"\"\"\n        if isinstance(tool_calls, dict):\n            # Convert dict to list\n            tool_calls_list = []\n            for idx in sorted(tool_calls.keys()):\n                tool_calls_list.append(tool_calls[idx])\n            tool_calls = tool_calls_list\n\n        metadata.tool_calls = tool_calls\n\n        print(f\"\\n{Colors.CYAN}üõ†Ô∏è  Tool Calls Detected:{Colors.END}\")\n        for i, tool_call in enumerate(tool_calls, 1):\n            print(\n                f\"   {i}. {tool_call['function']['name']}({tool_call['function']['arguments']})\"\n            )\n\n        # Execute tool calls\n        for tool_call in tool_calls:\n            result = tool_manager.execute_tool(tool_call)\n            metadata.tool_results.append(result)\n\n\n# ============ Result Saving ============\n\n\nclass ResultSaver:\n    \"\"\"Saves conversation results to files\"\"\"\n\n    @staticmethod\n    def save(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Tuple[Path, Path]:\n        \"\"\"Save results as Markdown and JSON\"\"\"\n        # Create directory if needed\n        base_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # 1. Save Markdown\n        md_path = ResultSaver._save_markdown(base_path, payload, metadata, printer)\n\n        # 2. Save JSON for continuation\n        json_path = ResultSaver._save_json(base_path, payload, metadata, printer)\n\n        return md_path, json_path\n\n    @staticmethod\n    def _save_markdown(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Path:\n        \"\"\"Save conversation as Markdown with full history\"\"\"\n        md_path = base_path.with_suffix(\".md\")\n\n        # Prepare payload for display (hide messages to save space)\n        display_payload = payload.copy()\n        messages = display_payload.pop(\"messages\", [])\n        if messages:\n            display_payload[\"messages\"] = f\"[{len(messages)} messages hidden]\"\n\n        # Build Markdown content\n        md_content = f\"\"\"# AI Conversation Log\n**Time**: {metadata.start_time.strftime('%Y-%m-%d %H:%M:%S')}  \n**Model**: {metadata.model or 'Unknown'}  \n**Duration**: {metadata.duration:.2f}s  \n\n## Request Configuration\n{CODE_BLOCK}json\n{json.dumps(display_payload, indent=2, ensure_ascii=False)}\n{CODE_BLOCK}\n\n## Conversation History\n\n\"\"\"\n\n        # Add all messages\n        for msg in messages:\n            role = msg.get(\"role\", \"unknown\").upper()\n            content = msg.get(\"content\", \"\")\n\n            # Handle multimodal content\n            if isinstance(content, list):\n                texts = []\n                for item in content:\n                    if isinstance(item, dict):\n                        if item.get(\"type\") == \"text\":\n                            texts.append(item.get(\"text\", \"\"))\n                        elif item.get(\"type\") == \"image_url\":\n                            texts.append(\"[Image]\")\n                content = \"\\n\".join(texts)\n\n            md_content += f\"### {role}\\n\\n{content}\\n\\n\"\n\n        # Add reasoning if present\n        if printer.reasoning_buffer:\n            md_content += f\"\"\"\n---\n\n## üí≠ Thinking Process\n<details>\n<summary>Click to expand ({len(printer.reasoning_buffer)} chars)</summary>\n\n{printer.reasoning_buffer}\n</details>\n\n\"\"\"\n\n        # Add response\n        md_content += f\"\"\"\n---\n\n## ‚ú® Response\n{printer.content_buffer}\n\n\"\"\"\n\n        # Add tool calls if present\n        if metadata.tool_calls:\n            md_content += f\"\"\"\n---\n\n## üõ†Ô∏è Tool Calls\n\"\"\"\n            for i, tool_call in enumerate(metadata.tool_calls, 1):\n                md_content += f\"\"\"\n### Tool Call {i}\n- **Function**: `{tool_call['function']['name']}`\n- **Arguments**: `{tool_call['function']['arguments']}`\n\"\"\"\n\n            if metadata.tool_results:\n                md_content += f\"\"\"\n### Tool Results\n\"\"\"\n                for result in metadata.tool_results:\n                    md_content += f\"\"\"\n- **{result['name']}**: {result['content']}\n\"\"\"\n\n        md_content += f\"\"\"\n---\n## üìä Statistics\n\n- **Finish Reason**: {metadata.finish_reason or 'N/A'}\n- **Tokens**: {metadata.total_tokens or 'N/A'} (Prompt: {metadata.prompt_tokens or 'N/A'}, Completion: {metadata.completion_tokens or 'N/A'})\n- **Token Rate**: {f'{metadata.get_token_rate():.1f} tokens/s' if metadata.get_token_rate() else 'N/A'}\n- **First Token Latency**: {f'{(printer.first_token_time - printer.start_time):.2f}s' if printer.first_token_time else 'N/A'}\n- **Estimated Total Tokens**: {printer.token_count}\n\"\"\"\n\n        with open(md_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(md_content)\n\n        return md_path\n\n    @staticmethod\n    def _save_json(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Path:\n        \"\"\"Save conversation as JSON for continuation\"\"\"\n        json_path = base_path.with_suffix(\".json\")\n\n        # Build conversation for continuation\n        continuation = {\n            \"metadata\": {\n                \"export_time\": datetime.now().isoformat(),\n                \"model\": metadata.model,\n                \"total_tokens\": metadata.total_tokens,\n                \"duration\": metadata.duration,\n            },\n            \"messages\": payload.get(\"messages\", []).copy(),\n        }\n\n        # Add assistant response\n        assistant_msg = {\n            \"role\": \"assistant\",\n            \"content\": printer.content_buffer,\n        }\n        if printer.reasoning_buffer:\n            assistant_msg[\"reasoning_content\"] = printer.reasoning_buffer\n        if metadata.tool_calls:\n            # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_calls‰∏≠ÁöÑID‰∏ç‰∏∫null\n            for tool_call in metadata.tool_calls:\n                if not tool_call.get(\"id\"):\n                    import uuid\n\n                    tool_call[\"id\"] = str(uuid.uuid4())\n            assistant_msg[\"tool_calls\"] = metadata.tool_calls\n\n        continuation[\"messages\"].append(assistant_msg)\n\n        # Add tool results as tool messages\n        for result in metadata.tool_results:\n            # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_call_id‰∏ç‰∏∫null\n            if not result.get(\"tool_call_id\"):\n                import uuid\n\n                result[\"tool_call_id\"] = str(uuid.uuid4())\n            continuation[\"messages\"].append(result)\n\n        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(continuation, f, ensure_ascii=False, indent=2)\n\n        return json_path\n\n\n# ============ Argument Parser ============\n\n\ndef create_argument_parser() -> argparse.ArgumentParser:\n    \"\"\"Create enhanced argument parser\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"AI Chat CLI - Enhanced with graceful parameter handling\",\n        formatter_class=argparse.RawTextHelpFormatter,\n        epilog=\"\"\"\nConfiguration Strategy:\n  1. Unified settings via config.json to avoid model-specific parameter issues\n  2. CLI arguments safely override config.json (removes conflicting params)\n  3. profiles.json or .env provides endpoint and api_key\n\nImage Support:\n  Use @filename.png to attach images (png, jpg, jpeg, gif, webp, bmp supported)\n  Example: python chat.py @photo.jpg \"Describe this image\"\n\nToken Estimation:\n  Before sending, the CLI will display estimated prompt tokens and max_tokens\n  to help you avoid context overflow.\n\nTool Support:\n  Define tools in tools.py with proper function signatures and docstrings\n  The CLI will automatically load and make them available to the model\n\nExamples:\n  # Basic usage with default config\n  python chat.py \"Hello, how are you?\"\n  \n  # With file attachment (code or image)\n  python chat.py @code.py \"Explain this code\"\n  python chat.py @image.png \"What's in this image?\"\n  \n  # Custom profile and config\n  python chat.py -p deepseek -f deepseek_config.json \"Explain quantum computing\"\n  \n  # Continue conversation\n  python chat.py --context conversation.json \"Continue from here\"\n  \n  # Start new conversation (don't save to config)\n  python chat.py --new \"Start fresh conversation\"\n  \n  # Disable streaming\n  python chat.py --no-stream \"Generate a long response\"\n  \n  # Override parameters\n  python chat.py --temperature 0.5 --max-tokens 2000 \"Creative writing\"\n        \"\"\",\n    )\n\n    # Connection\n    parser.add_argument(\n        \"--profile\", \"-p\", default=\"default\", help=\"Profile name in profiles.json\"\n    )\n    parser.add_argument(\"--endpoint\", \"-e\", help=\"Override API endpoint\")\n    parser.add_argument(\"--api-key\", \"-k\", help=\"Override API key\")\n\n    # Configuration\n    parser.add_argument(\n        \"--config\", \"-c\", default=\"config.json\", help=\"Request body configuration file\"\n    )\n    parser.add_argument(\n        \"--new\",\n        action=\"store_true\",\n        help=\"Start new conversation (don't save context to config)\",\n    )\n\n    # Input\n    parser.add_argument(\n        \"prompt\", nargs=\"*\", help=\"Prompt text (supports @filename for files/images)\"\n    )\n    parser.add_argument(\n        \"--context\", help=\"Conversation context JSON file for continuation\"\n    )\n\n    # OpenAI Parameters (override config.json)\n    parser.add_argument(\"--model\", \"-m\", help=\"Model name (overrides config)\")\n    parser.add_argument(\n        \"--temperature\", \"-t\", type=float, help=\"Sampling temperature (0-2)\"\n    )\n    parser.add_argument(\"--max-tokens\", type=int, help=\"Maximum tokens to generate\")\n    parser.add_argument(\"--top-p\", type=float, help=\"Nucleus sampling parameter\")\n    parser.add_argument(\"--presence-penalty\", type=float, help=\"Presence penalty\")\n    parser.add_argument(\"--frequency-penalty\", type=float, help=\"Frequency penalty\")\n    parser.add_argument(\"--seed\", type=int, help=\"Random seed for reproducibility\")\n\n    # Behavior (safely override config)\n    parser.add_argument(\n        \"--no-stream\", action=\"store_true\", help=\"Disable streaming output\"\n    )\n    parser.add_argument(\n        \"--enable-thinking\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable thinking process\",\n    )\n    parser.add_argument(\n        \"--no-thinking\",\n        dest=\"enable_thinking\",\n        action=\"store_false\",\n        help=\"Disable thinking process\",\n    )\n\n    # Tools\n    parser.add_argument(\n        \"--tools\", default=\"tools.py\", help=\"Python module containing tool functions\"\n    )\n    parser.add_argument(\n        \"--no-tools\", action=\"store_true\", help=\"Disable tool functions\"\n    )\n\n    # Output\n    parser.add_argument(\"--output\", \"-o\", help=\"Output file base name\")\n\n    return parser\n\n\n# ============ Main Function ============\n\n\ndef handle_interrupt(signum, frame):\n    \"\"\"Handle Ctrl+C gracefully\"\"\"\n    print(f\"\\n{Colors.warn('‚ö†Ô∏è  Interrupted by user')}\")\n    sys.exit(130)\n\n\ndef main():\n    \"\"\"Main entry point\"\"\"\n    # Setup\n    setup_utf8()\n    signal.signal(signal.SIGINT, handle_interrupt)\n\n    # Parse arguments\n    parser = create_argument_parser()\n    args = parser.parse_args()\n\n    try:\n        # 1. Load connection configuration\n        api_config = ConfigManager.load_connection_config(args)\n        api_config.validate()\n\n        # 2. Load request body from config.json\n        request_body = ConfigManager.load_request_body(args.config, args)\n\n        # 3. Initialize tool manager\n        tool_manager = None\n        if not args.no_tools and Path(args.tools).exists():\n            tool_manager = ToolManager(args.tools)\n            tool_definitions = tool_manager.get_tool_definitions()\n            if tool_definitions:\n                request_body[\"tools\"] = tool_definitions\n                print(\n                    f\"{Colors.info('‚úì')} Added {len(tool_definitions)} tools to request\",\n                    file=sys.stderr,\n                )\n\n        # 4. Build messages\n        messages = MessageBuilder.build_messages(args, request_body)\n        request_body[\"messages\"] = messages\n\n        # 5. Determine output path\n        if args.output:\n            output_base = Path(args.output)\n        else:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            prompt_hint = (\n                args.prompt[0][:20]\n                if args.prompt\n                else request_body.get(\"model\", \"chat\")\n            )\n            safe_name = re.sub(r\"[^\\w\\s-]\", \"\", prompt_hint).strip().replace(\" \", \"_\")\n            output_base = Path(f\"chat_{safe_name}_{timestamp}\")\n\n        # 6. Execute request\n        with open(output_base.with_suffix(\".md\"), \"w\", encoding=\"utf-8\") as out_file:\n            printer = TypewriterPrinter(out_file)\n\n            with APIClient(api_config) as client:\n                metadata = client.request(request_body, printer, tool_manager)\n\n            printer.finalize()\n\n            # 7. Add separator before complete message\n            print(f\"\\n{Colors.bold('‚îÄ' * 60)}\")\n\n            # 8. Print statistics\n            print(f\"\\n{Colors.info('‚úÖ Complete')}\")\n            print(f\"   Duration: {metadata.duration:.2f}s\")\n\n            if metadata.total_tokens:\n                print(\n                    f\"   Tokens: {metadata.total_tokens} (Prompt: {metadata.prompt_tokens}, Completion: {metadata.completion_tokens})\"\n                )\n                if token_rate := metadata.get_token_rate():\n                    print(f\"   Token Rate: {token_rate:.1f} tokens/s\")\n\n            if metadata.finish_reason:\n                print(f\"   Finish Reason: {metadata.finish_reason}\")\n\n            if printer.first_token_time:\n                print(\n                    f\"   First Token: {(printer.first_token_time - printer.start_time):.2f}s\"\n                )\n\n            if metadata.tool_calls:\n                print(f\"   Tool Calls: {len(metadata.tool_calls)} executed\")\n\n            # 9. Save results\n            md_path, json_path = ResultSaver.save(\n                output_base, request_body, metadata, printer\n            )\n\n            print(f\"\\n{Colors.info('üíæ Saved:')}\")\n            print(f\"   {md_path}\")\n            print(f\"   {json_path}\")\n            print(\n                f\"\\n{Colors.info('üí° Tip:')} Continue with: --context {json_path.name}\"\n            )\n\n            # 10. Save context to config.json if not --new\n            if not args.new:\n                # Update messages with assistant response\n                updated_messages = messages.copy()\n\n                # Add assistant message\n                assistant_msg = {\n                    \"role\": \"assistant\",\n                    \"content\": printer.content_buffer,\n                }\n                if printer.reasoning_buffer:\n                    assistant_msg[\"reasoning_content\"] = printer.reasoning_buffer\n                if metadata.tool_calls:\n                    assistant_msg[\"tool_calls\"] = metadata.tool_calls\n\n                updated_messages.append(assistant_msg)\n\n                # Add tool results\n                for result in metadata.tool_results:\n                    updated_messages.append(result)\n\n                # Save to config.json\n                ConfigManager.save_context_to_config(args.config, updated_messages)\n\n    except KeyboardInterrupt:\n        print(f\"\\n{Colors.warn('‚ö†Ô∏è  Interrupted')}\", file=sys.stderr)\n        sys.exit(130)\n    except (ConfigurationError, FileNotFoundError) as e:\n        print(f\"{Colors.error('‚ùå Configuration Error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n    except APIError as e:\n        print(f\"{Colors.error('‚ùå API Error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n    except Exception as e:\n        logger.exception(\"Unexpected error\")\n        print(f\"{Colors.error('‚ùå Unexpected error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n[File: deepseek.py]\n\n```json\n{\n  \"metadata\": {\n    \"export_time\": \"2026-02-06T03:52:29.393673\",\n    \"model\": \"Qwen/Qwen3-8B\",\n    \"total_tokens\": 771,\n    \"duration\": 13.744392\n  },\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"ÈòøÂ∑¥ÈòøÂ∑¥\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\nÊàëÊó†Ê≥ïÁêÜËß£‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùËøô‰∏™ËØçÁöÑÂÖ∑‰ΩìÂê´‰πâÔºåÊÇ®ÊòØÂê¶Âú®Ë°®ËææÊüêÁßçÊÉÖÁª™ÊàñÈúÄË¶ÅÂ∏ÆÂä©ÔºüÂ¶ÇÊûúÈúÄË¶ÅËÆ°ÁÆó„ÄÅÊü•ËØ¢Êó∂Èó¥ÊàñÂ§©Ê∞î‰ø°ÊÅØÔºåËØ∑ÂëäËØâÊàëÂÖ∑‰ΩìÂÜÖÂÆπÔºåÊàë‰ºöÂ∞ΩÂäõÂçèÂä©ÊÇ®„ÄÇ\",\n      \"reasoning_content\": \"\\nOkay, the user just said \\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\". Let me think about how to respond. First, I need to understand what they mean. \\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\" is a term that's often used in Chinese online culture, especially in gaming communities. It's a way to express frustration or exasperation, similar to \\\"arara\\\" in Japanese or \\\"arrgghh\\\" in English. People might use it when they're upset about something in a game, like a loss or a difficult situation.\\n\\nNow, looking at the tools provided, there are three functions: calculate, get_current_time, and search_weather. The user's message doesn't seem to involve any of these. They didn't ask for a math calculation, didn't mention a timezone for the current time, and didn't request weather info for a city. So, none of the tools are applicable here. \\n\\nI should check if there's any hidden request or if the user is testing me. But \\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\" is just an expression. Maybe they want me to acknowledge their frustration or explain the term? However, since the tools don't cover that, I need to respond without using any function calls. \\n\\nThe correct approach is to inform the user that I can't assist with their current message and ask how I can help. That way, I stay within the provided tools and offer further support if needed.\\n\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"‰ªäÊó•Â§©Ê∞î\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\nËØ∑ÂëäËØâÊàëÊÇ®ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞îÔºåÊàëÂ∞Ü‰∏∫ÊÇ®ÊêúÁ¥¢Áõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ\",\n      \"reasoning_content\": \"\\nÂ•ΩÁöÑÔºåÁî®Êà∑Áé∞Âú®ÈóÆÁöÑÊòØ‚Äú‰ªäÊó•Â§©Ê∞î‚ÄùÔºåÁúãËµ∑Êù•‰ªñ‰ª¨ÊÉ≥‰∫ÜËß£‰ªäÂ§©ÁöÑÂ§©Ê∞îÊÉÖÂÜµ„ÄÇ‰∏çËøá‰πãÂâçÁöÑÂØπËØù‰∏≠ÔºåÁî®Êà∑ËØ¥‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùÔºåÊàëÊ≤°ËÉΩÂáÜÁ°ÆÁêÜËß£‰ªñ‰ª¨ÁöÑÊÑèÊÄù„ÄÇÂèØËÉΩÁî®Êà∑ÊòØÂú®ÊµãËØïÊàñËÄÖÁî®ÊüêÁßçÊöóÂè∑Ôºå‰ΩÜÂêéÊù•‰ªñ‰ª¨Áõ¥Êé•ÈóÆÂ§©Ê∞î‰∫Ü„ÄÇ\\n\\nÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁ°ÆÂÆöÁî®Êà∑ÊòØÂê¶ÈúÄË¶ÅÊü•ËØ¢Â§©Ê∞î„ÄÇÊ†πÊçÆÂΩìÂâçÁöÑÂ∑•ÂÖ∑ÔºåÊúâ‰∏Ä‰∏™search_weatherÂáΩÊï∞ÔºåÈúÄË¶ÅÂèÇÊï∞ÊòØÂüéÂ∏Ç„ÄÇÁî®Êà∑Ê≤°ÊúâÊèêÂà∞ÂÖ∑‰ΩìÂüéÂ∏ÇÔºåÊâÄ‰ª•ÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•ËØ¢ÈóÆ„ÄÇ‰ΩÜÊúâÊó∂ÂÄôÁî®Êà∑ÂèØËÉΩÈªòËÆ§Êü•ËØ¢ÊâÄÂú®ÂüéÂ∏ÇÔºå‰∏çËøáËøôÈáåÊ≤°Êúâ‰∏ä‰∏ãÊñáÔºåÊâÄ‰ª•‰∏çËÉΩÁ°ÆÂÆö„ÄÇ\\n\\nÂè¶Â§ñÔºåÁî®Êà∑ÂèØËÉΩÊòØÂú®ÁªßÁª≠‰πãÂâçÁöÑÂØπËØùÔºå‰ΩÜ‰πãÂâçÁöÑ‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùÊ≤°ÊúâÊòéÁ°Æ‰ø°ÊÅØ„ÄÇÊâÄ‰ª•ÊúÄÂ•ΩËøòÊòØÂÖàÁ°ÆËÆ§Áî®Êà∑ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞î„ÄÇ‰∏çËøá‰πüÊúâÂèØËÉΩÁî®Êà∑Âè™ÊòØÁ™ÅÁÑ∂ÊÉ≥‰∫ÜËß£Â§©Ê∞îÔºåÊ≤°ÊúâÊèêÂà∞ÂüéÂ∏ÇÔºåËøôÊó∂ÂÄôÂèØËÉΩÈúÄË¶ÅÊèêÁ§∫‰ªñ‰ª¨Êèê‰æõÂüéÂ∏ÇÂêçÁß∞„ÄÇ\\n\\nÊ£ÄÊü•Â∑•ÂÖ∑‰∏≠ÁöÑÂáΩÊï∞Ôºåsearch_weatherÁöÑÂèÇÊï∞ÊòØcityÔºåÂøÖÂ°´„ÄÇÊâÄ‰ª•ÂøÖÈ°ªËé∑ÂèñÂüéÂ∏Ç‰ø°ÊÅØÊâçËÉΩË∞ÉÁî®„ÄÇÂõ†Ê≠§ÔºåÊàëÂ∫îËØ•ÂõûÂ§çÁî®Êà∑ÔºåËØ¢ÈóÆ‰ªñ‰ª¨ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞î„ÄÇËøôÊ†∑Êó¢Á¨¶ÂêàÂ∑•ÂÖ∑ÁöÑË¶ÅÊ±ÇÔºå‰πüËÉΩÁ°Æ‰øùÊèê‰æõÂáÜÁ°ÆÁöÑ‰ø°ÊÅØ„ÄÇ\\n\\nÂè¶Â§ñÔºåÊòØÂê¶ÊúâÂÖ∂‰ªñÂèØËÉΩÔºüÊØîÂ¶ÇÁî®Êà∑ÂèØËÉΩËØØËæìÂÖ•ÔºåÊàñËÄÖÊúâÂÖ∂‰ªñÊÑèÂõæÔºü‰ΩÜÊ†πÊçÆÁé∞ÊúâÂ∑•ÂÖ∑ÔºåÂè™ÊúâÂ§©Ê∞î„ÄÅËÆ°ÁÆóÊó∂Èó¥ÂíåËé∑ÂèñÊó∂Èó¥ÁöÑÂäüËÉΩ„ÄÇÊâÄ‰ª•ÊúÄÂêàÁêÜÁöÑÂõûÂ∫îÊòØËØ∑Ê±ÇÂüéÂ∏ÇÂêçÁß∞„ÄÇ\\n\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"ÂçóÊûÅ\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\n\",\n      \"reasoning_content\": \"\\nÂ•ΩÁöÑÔºåÁî®Êà∑Áé∞Âú®ÊèêÂà∞‚ÄúÂçóÊûÅ‚ÄùÔºåÈúÄË¶ÅÊü•ËØ¢ÈÇ£ÈáåÁöÑÂ§©Ê∞î„ÄÇÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁ°ÆËÆ§Áî®Êà∑ÊòØÂê¶ÁúüÁöÑÊÉ≥Êü•ËØ¢ÂçóÊûÅÁöÑÂ§©Ê∞î„ÄÇÂçóÊûÅÊòØ‰∏Ä‰∏™Â§ßÊ¥≤ÔºåÂ§©Ê∞îÊÉÖÂÜµÂèØËÉΩÊØîËæÉÁâπÊÆäÔºåËÄå‰∏îÁî®Êà∑ÂèØËÉΩÊ≤°ÊúâÊÑèËØÜÂà∞ÈúÄË¶ÅÊåáÂÆöÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ‰∏çËøáÔºåÊ†πÊçÆ‰πãÂâçÁöÑÂØπËØùÔºåÁî®Êà∑ÂèØËÉΩÊòØÂú®ÊµãËØïÊàñËÄÖÊúâÂÖ∂‰ªñÊÑèÂõæÔºå‰ΩÜÊ†πÊçÆÂ∑•ÂÖ∑‰∏≠ÁöÑsearch_weatherÂáΩÊï∞ÔºåÂèÇÊï∞ÊòØÂüéÂ∏ÇÔºåÊâÄ‰ª•ÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•Á°ÆËÆ§„ÄÇ\\n\\n‰∏çËøáÔºåÁî®Êà∑‰πãÂâçÂ∑≤ÁªèÈóÆËøá‚Äú‰ªäÊó•Â§©Ê∞î‚ÄùÔºåÁÑ∂ÂêéÊàëËÆ©‰ªñÊåáÂÆöÂüéÂ∏ÇÔºåÁé∞Âú®‰ªñÁõ¥Êé•ÂõûÁ≠î‚ÄúÂçóÊûÅ‚ÄùÔºåÂèØËÉΩÊòØÊåáÊï¥‰∏™ÂçóÊûÅÊ¥≤ÁöÑÂ§©Ê∞îÊÉÖÂÜµ„ÄÇ‰ΩÜÂçóÊûÅÊ¥≤Ê≤°ÊúâÁªü‰∏ÄÁöÑÂüéÂ∏ÇÔºåÂèØËÉΩÊúâÁßëÁ†îÁ´ôÔºåÊØîÂ¶ÇÈ∫¶ÂÖãÈªòÂ§öÁ´ô„ÄÅÈïøÂüéÁ´ôÁ≠â„ÄÇËøôÊó∂ÂÄôÊàëÈúÄË¶ÅÂà§Êñ≠ÊòØÂê¶Â∫îËØ•Áõ¥Êé•‰ΩøÁî®‚ÄúÂçóÊûÅ‚Äù‰Ωú‰∏∫ÂüéÂ∏ÇÂèÇÊï∞ÔºåÊàñËÄÖÊòØÂê¶ÈúÄË¶ÅÊèêÁ§∫Áî®Êà∑ËæìÂÖ•ÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ\\n\\nÊ†πÊçÆÂ∑•ÂÖ∑ÁöÑÊèèËø∞Ôºåsearch_weatherÂáΩÊï∞ÁöÑÂèÇÊï∞ÊòØcityÔºåÁ±ªÂûã‰∏∫Â≠óÁ¨¶‰∏≤Ôºå‰ΩÜÂèØËÉΩËØ•ÂáΩÊï∞ÂÜÖÈÉ®Â§ÑÁêÜ‰∫ÜÁ±ª‰ºº‚ÄúÂçóÊûÅ‚ÄùÁöÑËæìÂÖ•ÔºåÊàñËÄÖÁî®Êà∑ÂèØËÉΩÊúüÊúõÂæóÂà∞ÂçóÊûÅÊ¥≤ÁöÑÂ§©Ê∞îÊ¶ÇÂÜµ„ÄÇ‰πüÊúâÂèØËÉΩÁî®Êà∑ËæìÂÖ•ÊúâËØØÔºåÊàñËÄÖÊÉ≥‰∫ÜËß£Êüê‰∏™ÁâπÂÆöÂüéÂ∏ÇÁöÑÂ§©Ê∞îÔºå‰ΩÜËØØÂÜô‰∏∫ÂçóÊûÅ„ÄÇ‰æãÂ¶ÇÔºåÁî®Êà∑ÂèØËÉΩÊÉ≥Êü•Âåó‰∫¨„ÄÅ‰∏äÊµ∑Á≠âÂüéÂ∏ÇÔºå‰ΩÜÈîôËØØËæìÂÖ•‰∫Ü‚ÄúÂçóÊûÅ‚Äù„ÄÇ\\n\\nËøôÊó∂ÂÄôÔºåÊàëÈúÄË¶ÅÂÖàË∞ÉÁî®search_weatherÂáΩÊï∞ÔºåÂ∞ùËØï‰ΩøÁî®‚ÄúÂçóÊûÅ‚Äù‰Ωú‰∏∫cityÂèÇÊï∞ÔºåÁúãÁúãÊòØÂê¶ËÉΩËøîÂõûÊúâÊïàÁªìÊûú„ÄÇÂ¶ÇÊûúËøîÂõûÈîôËØØÔºåÂèØËÉΩÈúÄË¶ÅÊèêÁ§∫Áî®Êà∑Á°ÆËÆ§ÂüéÂ∏ÇÂêçÁß∞„ÄÇ‰ΩÜÊ†πÊçÆÂΩìÂâçÂ∑•ÂÖ∑ÁöÑËÆæÂÆöÔºåÂèØËÉΩÈúÄË¶ÅÁõ¥Êé•Ë∞ÉÁî®ÂáΩÊï∞ÔºåÁÑ∂ÂêéÊ†πÊçÆÁªìÊûúÂ§ÑÁêÜ„ÄÇÂ¶ÇÊûúÂáΩÊï∞ÊîØÊåÅÂú∞Âå∫Êü•ËØ¢ÔºåÂèØËÉΩÊ≤°ÈóÆÈ¢òÔºõÂ¶ÇÊûú‰∏çÊîØÊåÅÔºåÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•ÊæÑÊ∏Ö„ÄÇ\\n\\nÂõ†Ê≠§ÔºåÊ≠£Á°ÆÁöÑÂÅöÊ≥ïÊòØÂÖàË∞ÉÁî®search_weatherÂáΩÊï∞ÔºåÂèÇÊï∞‰∏∫city: \\\"ÂçóÊûÅ\\\"ÔºåÁÑ∂ÂêéÊ†πÊçÆËøîÂõûÁöÑÊï∞ÊçÆÂÜ≥ÂÆöÊòØÂê¶ÈúÄË¶ÅËøõ‰∏ÄÊ≠•ËØ¢ÈóÆÁî®Êà∑ÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ‰ΩÜÊ†πÊçÆÁî®Êà∑‰πãÂâçÁöÑÂØπËØùÔºå‰ªñ‰ª¨ÂèØËÉΩÂè™ÊòØÊÉ≥‰∫ÜËß£ÂçóÊûÅÁöÑÂ§©Ê∞îÊÉÖÂÜµÔºåÊâÄ‰ª•Áõ¥Êé•Ë∞ÉÁî®ÂáΩÊï∞ÊòØÂêàÈÄÇÁöÑ„ÄÇ\\n\",\n      \"tool_calls\": [\n        {\n          \"id\": null,\n          \"type\": \"function\",\n          \"function\": {\n            \"name\": \"search_weather\",\n            \"arguments\": \" {\\\"city\\\": \\\"ÂçóÊûÅ\\\"}\"\n          }\n        }\n      ]\n    },\n    {\n      \"tool_call_id\": null,\n      \"role\": \"tool\",\n      \"name\": \"search_weather\",\n      \"content\": \"\\\"Weather in ÂçóÊûÅ: Sunny, 25¬∞C\\\"\"\n    }\n  ]\n}\n```\n[File: chat_ÂçóÊûÅ_20260206_035215.json]\n\nËøôÊ†∑ÂØπÂêóÔºå‰ΩÜÊòØÊâæÂú∞ÊñπÂ•ΩÈ∫ªÁÉ¶Ôºå‰∏ãÊ¨°‰Ω†ËøòÊòØÁõ¥Êé•ÁªôÊàëÂÆåÊï¥code, È°∫Â∏¶‰∏ÄÊèê--newÂèÇÊï∞Ê≤°Áî®ÂïäÔºåÊîπ‰∏Ä‰∏ãÂêßÊ±Ç‰Ω†‰∫Ü„ÄÇ--newÁöÑ‰ΩúÁî®ÊòØË∑≥Ëøáconfig‰∏≠ÁöÑcontextÂíåmessages„ÄÇÊàëÊìç‰∫ÜmessagesÊòØË∞Å‰ªñÂ¶à‰øùÂ≠ò‰∏äÂéªÁöÑÂïäÔºåÊàëÊúç‰∫ÜÔºå‰∏çË¶Å‰øùÂ≠òÂïäÔºåËøô‰∏™Âú∞ÊñπÂè™Ë¶Å‰∏Ä‰∏™system_promptÂ∞±Â§ü‰∫ÜÔºåÂâ©‰∏ãÁöÑcontextÈÉΩÊòØ‰øùÂ≠òÂú®ÊØè Ê¨°Êñ∞Âá∫ÁöÑjsonÊñá‰ª∂ÈáåÁöÑÔºåÂ∞±ÊòØcontextÊåáÁ§∫ÁöÑÂì™‰∏™jsonÔºåËøôÈáåÂ∞±ÊòØ‰∏äÊ¨°Â≠ò‰∫Ü‰∏Ä‰∏™jsonÁÑ∂ÂêéÊñá‰ª∂Âêç‰øùÂ≠òÂú®Ëøô‰∏™Âú∞Êñπ„ÄÇÁÑ∂Âêé--newÁöÑÊÑèÊÄùÂ∞±ÊòØÊääcontextÂΩìÊàê‰∏çÂ≠òÂú®ÂïäÔºåÊàëÊìç‰∫ÜÔºåÂê¶ÂàôÂ∞±ÊòØÂä†‰∫Üconfig.jsonÊääÈáåÈù¢system promptÂíåcontextÊãº‰∏ÄËµ∑ÂèëÈÄÅÂïäÔºåÊàëÊìç‰∫ÜËøôÂæàÈöæÁêÜËß£Âêó\n\n```json\n{\n  \"max_tokens\": 163840,\n  \"temperature\": 0.7,\n  \"top_p\": 0.7,\n  \"top_k\": 50,\n  \"frequency_penalty\": 0,\n  \"enable_thinking\": true,\n  \"thinking_budget\": 81920,\n  \"model\": \"deepseek-ai/DeepSeek-V3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"```py\\n#!/usr/bin/env python3\\n\\\"\\\"\\\"\\nAdvanced Typewriter Effect HTTP Client for OpenAI-compatible APIs (Enhanced Version).\\nZero-default policy with graceful fallback - avoids crashes when encountering unsupported parameters.\\n\\\"\\\"\\\"\\n\\nimport argparse\\nimport base64\\nimport importlib.util\\nimport inspect\\nimport io\\nimport json\\nimport logging\\nimport mimetypes\\nimport os\\nimport re\\nimport signal\\nimport sys\\nimport time\\nfrom dataclasses import dataclass, field\\nfrom datetime import datetime\\nfrom pathlib import Path\\nfrom typing import Optional, Dict, List, Any, Tuple, Union, Callable\\n\\nimport requests\\nfrom dotenv import load_dotenv\\n\\n# ============ Setup & Constants ============\\n\\n# Configure logging\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\\n    stream=sys.stderr\\n)\\nlogger = logging.getLogger(__name__)\\n\\nCODE_BLOCK = \\\"`\\\" * 3\\n\\n\\nclass Constants:\\n    \\\"\\\"\\\"Application constants\\\"\\\"\\\"\\n    CHUNK_SIZE = 1024\\n    STREAM_DELAY_REASONING = 0.01\\n    STREAM_DELAY_CONTENT = 0.03\\n    TIMEOUT = 60*60*6  # ‰øùÊåÅ 1 Â∞èÊó∂Ë∂ÖÊó∂\\n\\n\\nclass Colors:\\n    \\\"\\\"\\\"ANSI Color codes for terminal output.\\\"\\\"\\\"\\n\\n    GREEN = \\\"\\\\033[92m\\\"\\n    YELLOW = \\\"\\\\033[93m\\\"\\n    RED = \\\"\\\\033[91m\\\"\\n    BLUE = \\\"\\\\033[94m\\\"\\n    CYAN = \\\"\\\\033[96m\\\"\\n    BOLD = \\\"\\\\033[1m\\\"\\n    END = \\\"\\\\033[0m\\\"\\n\\n    @classmethod\\n    def info(cls, text: str) -> str:\\n        return f\\\"{cls.GREEN}{text}{cls.END}\\\"\\n\\n    @classmethod\\n    def warn(cls, text: str) -> str:\\n        return f\\\"{cls.YELLOW}{text}{cls.END}\\\"\\n\\n    @classmethod\\n    def error(cls, text: str) -> str:\\n        return f\\\"{cls.RED}{text}{cls.END}\\\"\\n\\n    @classmethod\\n    def bold(cls, text: str) -> str:\\n        return f\\\"{cls.BOLD}{text}{cls.END}\\\"\\n\\n\\n# ============ Exceptions ============\\n\\nclass ChatError(Exception):\\n    \\\"\\\"\\\"Base exception for chat errors\\\"\\\"\\\"\\n    pass\\n\\n\\nclass ConfigurationError(ChatError):\\n    \\\"\\\"\\\"Missing required configuration\\\"\\\"\\\"\\n    pass\\n\\n\\nclass APIError(ChatError):\\n    \\\"\\\"\\\"API communication error\\\"\\\"\\\"\\n    pass\\n\\n\\nclass FileProcessingError(ChatError):\\n    \\\"\\\"\\\"File I/O error\\\"\\\"\\\"\\n    pass\\n\\n\\nclass ToolError(ChatError):\\n    \\\"\\\"\\\"Tool execution error\\\"\\\"\\\"\\n    pass\\n\\n\\n# ============ UTF-8 Setup ============\\n\\ndef setup_utf8():\\n    \\\"\\\"\\\"Force UTF-8 encoding for Windows and Unix\\\"\\\"\\\"\\n    if sys.platform == \\\"win32\\\":\\n        import ctypes\\n        try:\\n            kernel32 = ctypes.windll.kernel32\\n            kernel32.SetConsoleCP(65001)\\n            kernel32.SetConsoleOutputCP(65001)\\n        except Exception:\\n            pass\\n    \\n    if hasattr(sys.stdout, 'buffer'):\\n        sys.stdout = io.TextIOWrapper(\\n            sys.stdout.buffer,\\n            encoding=\\\"utf-8\\\",\\n            errors=\\\"replace\\\",\\n            line_buffering=True\\n        )\\n\\n\\n# ============ Data Models ============\\n\\n@dataclass\\nclass APIConfig:\\n    \\\"\\\"\\\"API connection configuration - only endpoint and api_key\\\"\\\"\\\"\\n\\n    endpoint: str = \\\"\\\"\\n    api_key: str = \\\"\\\"\\n    timeout: int = Constants.TIMEOUT  # HTTP timeout, not API parameter\\n\\n    def validate(self) -> None:\\n        if not self.endpoint:\\n            raise ConfigurationError(\\\"API endpoint is required\\\")\\n        if not self.api_key:\\n            raise ConfigurationError(\\\"API key is required\\\")\\n\\n\\n@dataclass\\nclass RequestMetadata:\\n    \\\"\\\"\\\"Metadata for request tracking and reporting\\\"\\\"\\\"\\n\\n    start_time: datetime = field(default_factory=datetime.now)\\n    end_time: Optional[datetime] = None\\n    prompt_tokens: int = 0\\n    completion_tokens: int = 0\\n    total_tokens: int = 0\\n    finish_reason: Optional[str] = None\\n    model: Optional[str] = None\\n    duration: float = 0.0\\n    tool_calls: List[Dict] = field(default_factory=list)\\n    tool_results: List[Dict] = field(default_factory=list)\\n\\n    def finalize(self) -> None:\\n        self.end_time = datetime.now()\\n        self.duration = (self.end_time - self.start_time).total_seconds()\\n\\n    def get_token_rate(self) -> Optional[float]:\\n        \\\"\\\"\\\"Calculate tokens per second\\\"\\\"\\\"\\n        if self.duration > 0 and self.completion_tokens > 0:\\n            return self.completion_tokens / self.duration\\n        return None\\n\\n\\n# ============ Tool System ============\\n\\nclass ToolManager:\\n    \\\"\\\"\\\"Manages tool functions and execution\\\"\\\"\\\"\\n    \\n    def __init__(self, tools_module_path: str = \\\"tools.py\\\"):\\n        self.tools_module_path = tools_module_path\\n        self.available_functions: Dict[str, Callable] = {}\\n        self.tool_definitions: List[Dict] = []\\n        self._load_tools()\\n    \\n    def _load_tools(self):\\n        \\\"\\\"\\\"Load tool functions from tools.py module\\\"\\\"\\\"\\n        try:\\n            spec = importlib.util.spec_from_file_location(\\\"tools\\\", self.tools_module_path)\\n            if spec and spec.loader:\\n                tools_module = importlib.util.module_from_spec(spec)\\n                spec.loader.exec_module(tools_module)\\n                \\n                # Find all functions in the module\\n                for name, obj in inspect.getmembers(tools_module):\\n                    if inspect.isfunction(obj) and not name.startswith('_'):\\n                        # Check if function has proper annotations for OpenAI tools\\n                        sig = inspect.signature(obj)\\n                        params = {}\\n                        for param_name, param in sig.parameters.items():\\n                            if param_name != 'self':\\n                                param_type = str(param.annotation) if param.annotation != inspect.Parameter.empty else \\\"string\\\"\\n                                params[param_name] = {\\n                                    \\\"type\\\": param_type.replace(\\\"<class '\\\", \\\"\\\").replace(\\\"'>\\\", \\\"\\\").lower(),\\n                                    \\\"description\\\": f\\\"Parameter {param_name}\\\"\\n                                }\\n                        \\n                        # Create tool definition\\n                        tool_def = {\\n                            \\\"type\\\": \\\"function\\\",\\n                            \\\"function\\\": {\\n                                \\\"name\\\": name,\\n                                \\\"description\\\": obj.__doc__ or f\\\"Function {name}\\\",\\n                                \\\"parameters\\\": {\\n                                    \\\"type\\\": \\\"object\\\",\\n                                    \\\"properties\\\": params,\\n                                    \\\"required\\\": list(params.keys())\\n                                }\\n                            }\\n                        }\\n                        \\n                        self.tool_definitions.append(tool_def)\\n                        self.available_functions[name] = obj\\n                        \\n                        print(f\\\"{Colors.info('‚úì')} Loaded tool: {name}\\\", file=sys.stderr)\\n                \\n                if not self.tool_definitions:\\n                    print(f\\\"{Colors.warn('‚ö†')} No tools found in {self.tools_module_path}\\\", file=sys.stderr)\\n            else:\\n                print(f\\\"{Colors.warn('‚ö†')} Tools module not found: {self.tools_module_path}\\\", file=sys.stderr)\\n        except Exception as e:\\n            print(f\\\"{Colors.warn('‚ö†')} Failed to load tools: {e}\\\", file=sys.stderr)\\n    \\n    def execute_tool(self, tool_call: Dict) -> Dict:\\n        \\\"\\\"\\\"Execute a tool call and return result\\\"\\\"\\\"\\n        try:\\n            function_name = tool_call[\\\"function\\\"][\\\"name\\\"]\\n            function_args = json.loads(tool_call[\\\"function\\\"][\\\"arguments\\\"])\\n            \\n            if function_name not in self.available_functions:\\n                raise ToolError(f\\\"Unknown function: {function_name}\\\")\\n            \\n            print(f\\\"\\\\n{Colors.CYAN}üîß Executing tool: {function_name}{Colors.END}\\\", file=sys.stderr)\\n            print(f\\\"   Arguments: {function_args}\\\", file=sys.stderr)\\n            \\n            # Execute the function\\n            result = self.available_functions[function_name](**function_args)\\n            \\n            print(f\\\"   Result: {result}\\\", file=sys.stderr)\\n            \\n            return {\\n                \\\"tool_call_id\\\": tool_call[\\\"id\\\"],\\n                \\\"role\\\": \\\"tool\\\",\\n                \\\"name\\\": function_name,\\n                \\\"content\\\": json.dumps(result, ensure_ascii=False)\\n            }\\n            \\n        except Exception as e:\\n            error_msg = f\\\"Tool execution failed: {str(e)}\\\"\\n            print(f\\\"{Colors.error('‚ùå')} {error_msg}\\\", file=sys.stderr)\\n            return {\\n                \\\"tool_call_id\\\": tool_call[\\\"id\\\"],\\n                \\\"role\\\": \\\"tool\\\",\\n                \\\"name\\\": tool_call[\\\"function\\\"][\\\"name\\\"],\\n                \\\"content\\\": json.dumps({\\\"error\\\": error_msg}, ensure_ascii=False)\\n            }\\n    \\n    def get_tool_definitions(self) -> List[Dict]:\\n        \\\"\\\"\\\"Get OpenAI-compatible tool definitions\\\"\\\"\\\"\\n        return self.tool_definitions\\n\\n\\n# ============ Configuration Management ============\\n\\nclass ConfigManager:\\n    \\\"\\\"\\\"Manages configuration loading with graceful fallback\\\"\\\"\\\"\\n\\n    ENV_PATHS = [\\\".env\\\", \\\"../.env\\\", Path.home() / \\\".ai_chat.env\\\"]\\n    PROFILE_PATHS = [\\n        Path(\\\"profiles.json\\\"),\\n        Path(\\\"../profiles.json\\\"),\\n        Path.home() / \\\".ai_chat_profiles.json\\\",\\n    ]\\n\\n    @staticmethod\\n    def load_connection_config(args) -> APIConfig:\\n        \\\"\\\"\\\"Load endpoint and api_key with priority: CLI > Profile > Env > Default\\\"\\\"\\\"\\n        config = APIConfig()\\n\\n        # 1. Load .env files first\\n        for env_path in ConfigManager.ENV_PATHS:\\n            if Path(env_path).exists():\\n                load_dotenv(env_path)\\n                logger.info(f\\\"Loaded environment: {env_path}\\\")\\n                break\\n\\n        # 2. From environment variables\\n        config.endpoint = os.getenv(\\\"ENDPOINT\\\", \\\"\\\")\\n        config.api_key = os.getenv(\\\"API_KEY\\\", \\\"\\\")\\n\\n        # 3. Try to load from profile\\n        profile_config = ConfigManager._load_profile(args.profile)\\n        if profile_config:\\n            if \\\"endpoint\\\" in profile_config:\\n                config.endpoint = profile_config[\\\"endpoint\\\"]\\n            if \\\"api_key\\\" in profile_config:\\n                config.api_key = profile_config[\\\"api_key\\\"]\\n            if \\\"apiKey\\\" in profile_config:\\n                config.api_key = profile_config[\\\"apiKey\\\"]\\n\\n        # 4. CLI arguments override everything\\n        if hasattr(args, \\\"endpoint\\\") and args.endpoint:\\n            config.endpoint = args.endpoint\\n        if hasattr(args, \\\"api_key\\\") and args.api_key:\\n            config.api_key = args.api_key\\n\\n        return config\\n\\n    @staticmethod\\n    def _load_profile(profile_name: str) -> Optional[Dict[str, Any]]:\\n        \\\"\\\"\\\"Load profile from various locations\\\"\\\"\\\"\\n        for path in ConfigManager.PROFILE_PATHS:\\n            if path.exists():\\n                try:\\n                    with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n                        profiles = json.load(f)\\n\\n                    if profile_name in profiles:\\n                        print(\\n                            f\\\"{Colors.info('‚úì')} Loaded profile: {profile_name} from {path}\\\",\\n                            file=sys.stderr,\\n                        )\\n                        return profiles[profile_name]\\n                except Exception as e:\\n                    logger.error(f\\\"Failed to load profile {path}: {e}\\\")\\n\\n        if profile_name != \\\"default\\\":\\n            print(\\n                f\\\"{Colors.warn('‚ö†')} Profile '{profile_name}' not found, using default\\\",\\n                file=sys.stderr,\\n            )\\n        return None\\n\\n    @staticmethod\\n    def load_request_body(config_path: str, args) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Load request body from config.json with safe parameter handling\\\"\\\"\\\"\\n        path = Path(config_path).expanduser().resolve()\\n\\n        if not path.exists():\\n            if (\\n                config_path != \\\"config.json\\\"\\n            ):  # User explicitly specified non-existent file\\n                raise FileNotFoundError(f\\\"Config file not found: {config_path}\\\")\\n            # Use empty config if default config.json doesn't exist\\n            return {}\\n\\n        try:\\n            with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n                data = json.load(f)\\n\\n            print(f\\\"{Colors.info('‚úì')} Loaded request config: {path}\\\", file=sys.stderr)\\n\\n            # Apply CLI overrides for safety-critical parameters\\n            ConfigManager._apply_cli_overrides(data, args)\\n\\n            return data\\n        except Exception as e:\\n            raise ValueError(f\\\"Failed to load config file ({config_path}): {e}\\\")\\n\\n    @staticmethod\\n    def _apply_cli_overrides(data: Dict[str, Any], args) -> None:\\n        \\\"\\\"\\\"Apply CLI arguments to request body, removing unsupported parameters\\\"\\\"\\\"\\n        # Safety: Remove problematic parameters if CLI provides alternatives\\n        if hasattr(args, \\\"model\\\") and args.model:\\n            data[\\\"model\\\"] = args.model\\n            # Remove any model-specific parameters that might conflict\\n            data.pop(\\\"temperature\\\", None)  # Some models don't support temperature\\n            data.pop(\\\"top_p\\\", None)\\n            data.pop(\\\"frequency_penalty\\\", None)\\n            data.pop(\\\"presence_penalty\\\", None)\\n\\n        # OpenAI standard parameters override\\n        if hasattr(args, \\\"temperature\\\") and args.temperature is not None:\\n            data[\\\"temperature\\\"] = args.temperature\\n        if hasattr(args, \\\"max_tokens\\\") and args.max_tokens is not None:\\n            data[\\\"max_tokens\\\"] = args.max_tokens\\n        if hasattr(args, \\\"top_p\\\") and args.top_p is not None:\\n            data[\\\"top_p\\\"] = args.top_p\\n        if hasattr(args, \\\"presence_penalty\\\") and args.presence_penalty is not None:\\n            data[\\\"presence_penalty\\\"] = args.presence_penalty\\n        if hasattr(args, \\\"frequency_penalty\\\") and args.frequency_penalty is not None:\\n            data[\\\"frequency_penalty\\\"] = args.frequency_penalty\\n        if hasattr(args, \\\"seed\\\") and args.seed is not None:\\n            data[\\\"seed\\\"] = args.seed\\n\\n        # Stream handling\\n        if hasattr(args, \\\"no_stream\\\") and args.no_stream:\\n            data[\\\"stream\\\"] = False\\n        elif \\\"stream\\\" not in data:\\n            data[\\\"stream\\\"] = True  # Default to stream for better UX\\n\\n        # Thinking enable/disable\\n        if hasattr(args, \\\"enable_thinking\\\"):\\n            if args.enable_thinking is not None:\\n                data[\\\"enable_thinking\\\"] = args.enable_thinking\\n                # Some APIs don't support reasoning_content, remove if disabled\\n                if not args.enable_thinking:\\n                    data.pop(\\\"reasoning_content\\\", None)\\n\\n    @staticmethod\\n    def save_context_to_config(config_path: str, messages: List[Dict]) -> None:\\n        \\\"\\\"\\\"Save conversation context back to config.json\\\"\\\"\\\"\\n        try:\\n            path = Path(config_path).expanduser().resolve()\\n            \\n            # Load existing config\\n            if path.exists():\\n                with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n                    config_data = json.load(f)\\n            else:\\n                config_data = {}\\n            \\n            # Update messages\\n            config_data[\\\"messages\\\"] = messages\\n            \\n            # Save back\\n            with open(path, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n                json.dump(config_data, f, ensure_ascii=False, indent=2)\\n            \\n            print(f\\\"{Colors.info('‚úì')} Saved context to: {path}\\\", file=sys.stderr)\\n            \\n        except Exception as e:\\n            print(f\\\"{Colors.warn('‚ö†')} Failed to save context: {e}\\\", file=sys.stderr)\\n\\n\\n# ============ File & Message Building ============\\n\\nclass MessageBuilder:\\n    \\\"\\\"\\\"Builds messages with file context support\\\"\\\"\\\"\\n\\n    IMAGE_TYPES = {\\\".png\\\", \\\".jpg\\\", \\\".jpeg\\\", \\\".gif\\\", \\\".webp\\\", \\\".bmp\\\"}\\n\\n    @staticmethod\\n    def load_file(filepath: str) -> Union[str, Dict[str, Any]]:\\n        \\\"\\\"\\\"Load file content with MIME type detection\\\"\\\"\\\"\\n        if filepath.startswith(\\\"@\\\"):\\n            filepath = filepath[1:]\\n\\n        path = Path(filepath).expanduser().resolve()\\n        if not path.exists():\\n            raise FileNotFoundError(f\\\"File not found: {filepath}\\\")\\n\\n        mime_type, _ = mimetypes.guess_type(path)\\n        if not mime_type:\\n            mime_type = \\\"application/octet-stream\\\"\\n\\n        # Image handling\\n        if mime_type.startswith(\\\"image/\\\") or path.suffix.lower() in MessageBuilder.IMAGE_TYPES:\\n            with open(path, \\\"rb\\\") as f:\\n                encoded = base64.b64encode(f.read()).decode(\\\"utf-8\\\")\\n            return {\\n                \\\"type\\\": \\\"image_url\\\",\\n                \\\"image_url\\\": {\\\"url\\\": f\\\"data:{mime_type};base64,{encoded}\\\"},\\n            }\\n\\n        # Text file handling with encoding detection\\n        encodings = [\\\"utf-8\\\", \\\"utf-8-sig\\\", \\\"gbk\\\", \\\"latin-1\\\"]\\n        content = None\\n        \\n        for enc in encodings:\\n            try:\\n                with open(path, \\\"r\\\", encoding=enc, errors=\\\"strict\\\") as f:\\n                    content = f.read()\\n                break\\n            except UnicodeDecodeError:\\n                continue\\n        \\n        if content is None:\\n            with open(path, \\\"r\\\", encoding=\\\"utf-8\\\", errors=\\\"replace\\\") as f:\\n                content = f.read()\\n\\n        # Format based on file type\\n        suffix = path.suffix[1:] if path.suffix else \\\"text\\\"\\n        if path.suffix in [\\\".md\\\", \\\".txt\\\", \\\".rst\\\"]:\\n            return f\\\"{content}\\\\n\\\\n[File: {path.name}]\\\"\\n        else:\\n            return f\\\"{CODE_BLOCK}{suffix}\\\\n{content}\\\\n{CODE_BLOCK}\\\\n[File: {path.name}]\\\"\\n\\n    @staticmethod\\n    def build_messages(\\n        args, config_data: Dict | None = None\\n    ) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Build final message list with priority: context > config > prompt\\\"\\\"\\\"\\n        messages = []\\n        \\n        # Extract system prompt from config if exists\\n        system_prompt = None\\n        config_messages = []\\n        if config_data:\\n            system_prompt = config_data.pop(\\\"system\\\", None)\\n            config_messages = config_data.pop(\\\"messages\\\", [])\\n\\n        # 1. Add system prompt if exists (always first)\\n        if system_prompt:\\n            messages.append({\\\"role\\\": \\\"system\\\", \\\"content\\\": system_prompt})\\n            print(f\\\"{Colors.info('‚úì')} Added system prompt\\\", file=sys.stderr)\\n\\n        # 2. Load context if provided\\n        if hasattr(args, \\\"context\\\") and args.context:\\n            try:\\n                context_messages = MessageBuilder._load_context(args.context)\\n                messages.extend(context_messages)\\n                print(\\n                    f\\\"{Colors.info('‚úì')} Loaded conversation context: {args.context}\\\",\\n                    file=sys.stderr,\\n                )\\n            except Exception as e:\\n                print(\\n                    f\\\"{Colors.warn('‚ö†')} Failed to load context: {e}\\\", file=sys.stderr\\n                )\\n\\n        # 3. Add messages from config\\n        if config_messages:\\n            messages.extend(config_messages)\\n\\n        # 4. Add current user input\\n        if hasattr(args, \\\"prompt\\\") and args.prompt:\\n            user_messages = MessageBuilder._build_user_messages(args.prompt)\\n            messages.extend(user_messages)\\n\\n        if not messages and not system_prompt:\\n            raise ConfigurationError(\\n                \\\"No messages to send. Please provide prompt or messages in config.\\\"\\n            )\\n\\n        return messages\\n\\n    @staticmethod\\n    def _load_context(context_path: str) -> List[Dict]:\\n        \\\"\\\"\\\"Load conversation history from JSON file\\\"\\\"\\\"\\n        path = Path(context_path).expanduser().resolve()\\n        if not path.exists():\\n            raise FileNotFoundError(f\\\"Context file not found: {context_path}\\\")\\n\\n        with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n            data = json.load(f)\\n\\n        if isinstance(data, list):\\n            return data\\n        elif isinstance(data, dict) and \\\"messages\\\" in data:\\n            return data[\\\"messages\\\"]\\n        else:\\n            raise ValueError(\\n                \\\"Invalid context format: expected list or {messages: [...]}\\\"\\n            )\\n\\n    @staticmethod\\n    def _build_user_messages(prompt_parts: List[str]) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Build user messages from prompt parts\\\"\\\"\\\"\\n        if not prompt_parts:\\n            return []\\n\\n        content_parts = []\\n        has_image = False\\n\\n        for part in prompt_parts:\\n            if part.startswith(\\\"@\\\"):\\n                try:\\n                    file_content = MessageBuilder.load_file(part)\\n                    if isinstance(file_content, dict):\\n                        has_image = True\\n                        print(f\\\"{Colors.info('‚úì')} Attached image: {part[1:]}\\\", file=sys.stderr)\\n                    else:\\n                        print(f\\\"{Colors.info('‚úì')} Attached file: {part[1:]}\\\", file=sys.stderr)\\n                    content_parts.append(file_content)\\n                except Exception as e:\\n                    print(\\n                        f\\\"{Colors.warn('‚ö†')} Failed to load file {part}: {e}\\\",\\n                        file=sys.stderr,\\n                    )\\n                    content_parts.append(f\\\"[Failed to load file: {part}]\\\")\\n            else:\\n                content_parts.append(part)\\n\\n        # Build message content\\n        if has_image:\\n            multimodal_content = []\\n            current_text = []\\n\\n            for part in content_parts:\\n                if isinstance(part, dict):\\n                    if current_text:\\n                        multimodal_content.append(\\n                            {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n\\\".join(current_text)}\\n                        )\\n                        current_text = []\\n                    multimodal_content.append(part)\\n                else:\\n                    current_text.append(part)\\n\\n            if current_text:\\n                multimodal_content.append(\\n                    {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n\\\\n\\\".join(current_text)}\\n                )\\n\\n            return [{\\\"role\\\": \\\"user\\\", \\\"content\\\": multimodal_content}]\\n        else:\\n            user_content = \\\"\\\\n\\\\n\\\".join(content_parts)\\n            return [{\\\"role\\\": \\\"user\\\", \\\"content\\\": user_content}]\\n\\n\\n# ============ Typewriter Output ============\\n\\nclass TypewriterPrinter:\\n    \\\"\\\"\\\"Typewriter effect printer with file output\\\"\\\"\\\"\\n\\n    def __init__(self, output_file: Optional[io.TextIOWrapper] = None):\\n        self.output_file = output_file\\n        self.reasoning_buffer = \\\"\\\"\\n        self.content_buffer = \\\"\\\"\\n        self.reasoning_printed = 0\\n        self.content_printed = 0\\n        self.in_reasoning = True\\n        self.start_time = time.time()\\n        self.first_token_time = None\\n        self.token_count = 0\\n\\n        # Performance tracking\\n        self.reasoning_start = None\\n        self.content_start = None\\n\\n    def write(self, text: str, is_reasoning: bool = False):\\n        \\\"\\\"\\\"Write text to file\\\"\\\"\\\"\\n        if self.output_file:\\n            try:\\n                self.output_file.write(text)\\n                if not is_reasoning:\\n                    self.output_file.flush()\\n            except Exception as e:\\n                print(f\\\"\\\\n{Colors.error('[File write error]')} {e}\\\", file=sys.stderr)\\n\\n    def update_reasoning(self, delta: str):\\n        \\\"\\\"\\\"Update and print reasoning content\\\"\\\"\\\"\\n        if not delta:\\n            return\\n\\n        if self.first_token_time is None:\\n            self.first_token_time = time.time()\\n\\n        if self.reasoning_start is None:\\n            self.reasoning_start = time.time()\\n            print(f\\\"\\\\n{Colors.BLUE}üí≠ Thinking:{Colors.END}\\\")\\n            if self.output_file:\\n                self.write(\\\"\\\\nüí≠ Thinking:\\\\n\\\", is_reasoning=True)\\n\\n        self.reasoning_buffer += delta\\n        new_text = self.reasoning_buffer[self.reasoning_printed :]\\n\\n        # Print to console with typewriter effect\\n        for char in new_text:\\n            print(char, end=\\\"\\\", flush=True)\\n            time.sleep(Constants.STREAM_DELAY_REASONING)\\n\\n        # Write to file\\n        self.write(new_text, is_reasoning=True)\\n\\n        self.reasoning_printed = len(self.reasoning_buffer)\\n        self.token_count += len(delta.split())  # Rough estimate\\n\\n    def switch_to_content(self):\\n        \\\"\\\"\\\"Switch from reasoning to content output\\\"\\\"\\\"\\n        if self.in_reasoning:\\n            self.in_reasoning = False\\n            self.content_start = time.time()\\n\\n            # Console output\\n            print(f\\\"\\\\n{Colors.bold('='*50)}\\\")\\n            print(f\\\"{Colors.CYAN}‚ú® Response:{Colors.END}\\\")\\n            print(f\\\"{Colors.bold('='*50)}\\\")\\n\\n            # File output\\n            if self.output_file:\\n                self.write(f\\\"\\\\n\\\\n{'='*50}\\\\n‚ú® Response:\\\\n{'='*50}\\\\n\\\\n\\\")\\n\\n    def update_content(self, delta: str):\\n        \\\"\\\"\\\"Update and print response content\\\"\\\"\\\"\\n        if not delta:\\n            return\\n\\n        if self.in_reasoning:\\n            self.switch_to_content()\\n\\n        if self.first_token_time is None:\\n            self.first_token_time = time.time()\\n\\n        self.content_buffer += delta\\n        new_text = self.content_buffer[self.content_printed :]\\n\\n        # Typewriter effect\\n        for char in new_text:\\n            print(char, end=\\\"\\\", flush=True)\\n            time.sleep(Constants.STREAM_DELAY_CONTENT)\\n        \\n        self.write(new_text)\\n\\n        self.content_printed = len(self.content_buffer)\\n        self.token_count += len(delta.split())  # Rough estimate\\n\\n    def finalize(self):\\n        \\\"\\\"\\\"Finalize output\\\"\\\"\\\"\\n        if self.output_file:\\n            try:\\n                self.output_file.flush()\\n            except Exception as e:\\n                print(f\\\"{Colors.warn('‚ö†')} Failed to flush file: {e}\\\", file=sys.stderr)\\n\\n\\n# ============ HTTP Client ============\\n\\nclass APIClient:\\n    \\\"\\\"\\\"HTTP client with retry and error handling using Session\\\"\\\"\\\"\\n\\n    def __init__(self, config: APIConfig, max_retries: int = 3):\\n        self.config = config\\n        self.max_retries = max_retries\\n        self.session = requests.Session()\\n        self.session.headers.update({\\n            \\\"Authorization\\\": f\\\"Bearer {config.api_key}\\\",\\n            \\\"Content-Type\\\": \\\"application/json\\\",\\n            \\\"Accept\\\": \\\"text/event-stream\\\",\\n        })\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.session.close()\\n\\n    def _estimate_tokens(self, messages: List[Dict]) -> Tuple[int, int]:\\n        \\\"\\\"\\\"‰º∞ÁÆó Prompt ÁöÑ Token Êï∞Ôºà‰øùÂÆà‰º∞ËÆ°ÔºöÊØè 3 ‰∏™Â≠óÁ¨¶ 1 ‰∏™ TokenÔºâ\\\"\\\"\\\"\\n        try:\\n            # Â∞Ü messages ËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤ËÆ°ÁÆó\\n            text = json.dumps(messages, ensure_ascii=False)\\n            char_count = len(text)\\n            # ‰øùÂÆà‰º∞ËÆ°ÔºöËã±ÊñáÁ∫¶ 4 Â≠óÁ¨¶/tokenÔºå‰∏≠ÊñáÁ∫¶ 1-2 Â≠óÁ¨¶/tokenÔºåÊ∑∑ÂêàÊåâ 3 ËÆ°ÁÆó\\n            estimated_tokens = char_count // 3\\n            return char_count, estimated_tokens\\n        except:\\n            return 0, 0\\n\\n    def request(\\n        self, payload: Dict[str, Any], printer: TypewriterPrinter, tool_manager: Optional[ToolManager] = None\\n    ) -> RequestMetadata:\\n        \\\"\\\"\\\"Execute API request with retry logic\\\"\\\"\\\"\\n        metadata = RequestMetadata(model=payload.get(\\\"model\\\"))\\n        is_stream = payload.get(\\\"stream\\\", True)\\n\\n        # ËÆ°ÁÆó Prompt ‰ø°ÊÅØ\\n        messages = payload.get(\\\"messages\\\", [])\\n        char_count, estimated_tokens = self._estimate_tokens(messages)\\n        max_tokens = payload.get(\\\"max_tokens\\\", \\\"Not set\\\")\\n        \\n        # ÊòæÁ§∫ËØ∑Ê±Ç‰ø°ÊÅØÔºåÂåÖÊã¨ Token È¢Ñ‰º∞\\n        print(\\n            f\\\"\\\\n{Colors.CYAN}üöÄ Requesting {metadata.model or 'unknown model'}...{Colors.END}\\\"\\n        )\\n        print(f\\\"   Endpoint: {self.config.endpoint}\\\")\\n        print(f\\\"   Stream: {'Yes' if is_stream else 'No'}\\\")\\n        print(f\\\"   Prompt Size: {char_count} chars (Est. ~{estimated_tokens} tokens)\\\")\\n        \\n        if max_tokens != \\\"Not set\\\":\\n            print(f\\\"   Max Tokens: {max_tokens}\\\")\\n        else:\\n            print(f\\\"   Max Tokens: {Colors.warn('Not set')} (model default)\\\")\\n\\n        # Safety: Remove any empty or None parameters that might cause API errors\\n        clean_payload = self._clean_payload(payload)\\n\\n        for attempt in range(self.max_retries + 1):\\n            try:\\n                response = self.session.post(\\n                    self.config.endpoint,\\n                    json=clean_payload,\\n                    stream=is_stream,\\n                    timeout=self.config.timeout,\\n                )\\n                response.raise_for_status()\\n\\n                if is_stream:\\n                    return self._handle_stream(response, printer, metadata, tool_manager)\\n                else:\\n                    return self._handle_non_stream(response, printer, metadata, tool_manager)\\n\\n            except requests.exceptions.RequestException as e:\\n                if attempt < self.max_retries:\\n                    wait_time = 2**attempt\\n                    print(\\n                        f\\\"{Colors.warn(f'‚ö†Ô∏è  Request failed ({e}), retrying in {wait_time}s...')}\\\",\\n                        file=sys.stderr,\\n                    )\\n                    time.sleep(wait_time)\\n                else:\\n                    raise APIError(\\n                        f\\\"Request failed after {self.max_retries} retries: {e}\\\"\\n                    )\\n\\n        raise APIError(\\\"Max retries exceeded\\\")\\n\\n    def _clean_payload(self, payload: Dict[str, Any]) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Remove potentially problematic parameters\\\"\\\"\\\"\\n        clean = payload.copy()\\n\\n        # Remove None values\\n        for key in list(clean.keys()):\\n            if clean[key] is None:\\n                del clean[key]\\n\\n        # Remove empty strings\\n        for key in list(clean.keys()):\\n            if isinstance(clean[key], str) and not clean[key].strip():\\n                del clean[key]\\n\\n        return clean\\n\\n    def _handle_stream(\\n        self, response, printer: TypewriterPrinter, metadata: RequestMetadata, tool_manager: Optional[ToolManager]\\n    ) -> RequestMetadata:\\n        \\\"\\\"\\\"Handle streaming response\\\"\\\"\\\"\\n        buffer = \\\"\\\"\\n        tool_calls_buffer = {}\\n\\n        for chunk in response.iter_content(chunk_size=Constants.CHUNK_SIZE):\\n            if not chunk:\\n                continue\\n\\n            buffer += chunk.decode(\\\"utf-8\\\", errors=\\\"replace\\\")\\n\\n            while \\\"\\\\n\\\" in buffer:\\n                line, buffer = buffer.split(\\\"\\\\n\\\", 1)\\n                line = line.strip()\\n\\n                if not line.startswith(\\\"data: \\\"):\\n                    continue\\n\\n                data_str = line[6:]\\n                if data_str == \\\"[DONE]\\\":\\n                    break\\n\\n                try:\\n                    data = json.loads(data_str)\\n                    self._process_stream_data(data, printer, metadata, tool_calls_buffer)\\n                except json.JSONDecodeError:\\n                    continue\\n\\n        # Handle tool calls after stream ends\\n        if tool_calls_buffer and tool_manager:\\n            self._handle_tool_calls(tool_calls_buffer, metadata, tool_manager)\\n\\n        metadata.finalize()\\n        return metadata\\n\\n    def _process_stream_data(\\n        self, data: Dict, printer: TypewriterPrinter, metadata: RequestMetadata, tool_calls_buffer: Dict\\n    ):\\n        \\\"\\\"\\\"Process individual stream data chunk\\\"\\\"\\\"\\n        # Update usage\\n        if \\\"usage\\\" in data and data[\\\"usage\\\"]:\\n            metadata.prompt_tokens = data[\\\"usage\\\"].get(\\\"prompt_tokens\\\", 0)\\n            metadata.completion_tokens = data[\\\"usage\\\"].get(\\\"completion_tokens\\\", 0)\\n            metadata.total_tokens = data[\\\"usage\\\"].get(\\\"total_tokens\\\", 0)\\n\\n        choices = data.get(\\\"choices\\\", [])\\n        if not choices:\\n            return\\n\\n        choice = choices[0]\\n        delta = choice.get(\\\"delta\\\", {})\\n\\n        # Update finish reason\\n        finish_reason = choice.get(\\\"finish_reason\\\")\\n        if finish_reason:\\n            metadata.finish_reason = finish_reason\\n\\n        # Process tool calls\\n        tool_calls = delta.get(\\\"tool_calls\\\", [])\\n        for tool_call in tool_calls:\\n            idx = tool_call.get(\\\"index\\\", 0)\\n            if idx not in tool_calls_buffer:\\n                tool_calls_buffer[idx] = {\\\"id\\\": \\\"\\\", \\\"type\\\": \\\"function\\\", \\\"function\\\": {\\\"name\\\": \\\"\\\", \\\"arguments\\\": \\\"\\\"}}\\n            \\n            if \\\"id\\\" in tool_call:\\n                tool_calls_buffer[idx][\\\"id\\\"] = tool_call[\\\"id\\\"]\\n            if \\\"function\\\" in tool_call:\\n                if \\\"name\\\" in tool_call[\\\"function\\\"]:\\n                    tool_calls_buffer[idx][\\\"function\\\"][\\\"name\\\"] = tool_call[\\\"function\\\"][\\\"name\\\"]\\n                if \\\"arguments\\\" in tool_call[\\\"function\\\"]:\\n                    tool_calls_buffer[idx][\\\"function\\\"][\\\"arguments\\\"] += tool_call[\\\"function\\\"][\\\"arguments\\\"]\\n\\n        # Process content\\n        reasoning_delta = delta.get(\\\"reasoning_content\\\") or delta.get(\\\"reasoning\\\") or \\\"\\\"\\n        content_delta = delta.get(\\\"content\\\", \\\"\\\")\\n\\n        if reasoning_delta:\\n            printer.update_reasoning(reasoning_delta)\\n\\n        if content_delta:\\n            printer.update_content(content_delta)\\n\\n    def _handle_non_stream(\\n        self, response, printer: TypewriterPrinter, metadata: RequestMetadata, tool_manager: Optional[ToolManager]\\n    ) -> RequestMetadata:\\n        \\\"\\\"\\\"Handle non-streaming response\\\"\\\"\\\"\\n        data = response.json()\\n        choice = data.get(\\\"choices\\\", [{}])[0]\\n        message = choice.get(\\\"message\\\", {})\\n\\n        reasoning = message.get(\\\"reasoning_content\\\", \\\"\\\")\\n        content = message.get(\\\"content\\\", \\\"\\\")\\n\\n        if reasoning:\\n            printer.update_reasoning(reasoning)\\n\\n        printer.switch_to_content()\\n        printer.update_content(content)\\n\\n        # Handle tool calls\\n        tool_calls = message.get(\\\"tool_calls\\\", [])\\n        if tool_calls and tool_manager:\\n            self._handle_tool_calls(tool_calls, metadata, tool_manager)\\n\\n        # Update metadata\\n        if \\\"usage\\\" in data:\\n            metadata.prompt_tokens = data[\\\"usage\\\"].get(\\\"prompt_tokens\\\", 0)\\n            metadata.completion_tokens = data[\\\"usage\\\"].get(\\\"completion_tokens\\\", 0)\\n            metadata.total_tokens = data[\\\"usage\\\"].get(\\\"total_tokens\\\", 0)\\n\\n        metadata.finish_reason = choice.get(\\\"finish_reason\\\")\\n        metadata.finalize()\\n\\n        return metadata\\n\\n    def _handle_tool_calls(self, tool_calls: Union[List, Dict], metadata: RequestMetadata, tool_manager: ToolManager):\\n        \\\"\\\"\\\"Handle tool calls and execute them\\\"\\\"\\\"\\n        if isinstance(tool_calls, dict):\\n            # Convert dict to list\\n            tool_calls_list = []\\n            for idx in sorted(tool_calls.keys()):\\n                tool_calls_list.append(tool_calls[idx])\\n            tool_calls = tool_calls_list\\n        \\n        metadata.tool_calls = tool_calls\\n        \\n        print(f\\\"\\\\n{Colors.CYAN}üõ†Ô∏è  Tool Calls Detected:{Colors.END}\\\")\\n        for i, tool_call in enumerate(tool_calls, 1):\\n            print(f\\\"   {i}. {tool_call['function']['name']}({tool_call['function']['arguments']})\\\")\\n        \\n        # Execute tool calls\\n        for tool_call in tool_calls:\\n            result = tool_manager.execute_tool(tool_call)\\n            metadata.tool_results.append(result)\\n\\n\\n# ============ Result Saving ============\\n\\nclass ResultSaver:\\n    \\\"\\\"\\\"Saves conversation results to files\\\"\\\"\\\"\\n\\n    @staticmethod\\n    def save(\\n        base_path: Path,\\n        payload: Dict[str, Any],\\n        metadata: RequestMetadata,\\n        printer: TypewriterPrinter,\\n    ) -> Tuple[Path, Path]:\\n        \\\"\\\"\\\"Save results as Markdown and JSON\\\"\\\"\\\"\\n        # Create directory if needed\\n        base_path.parent.mkdir(parents=True, exist_ok=True)\\n\\n        # 1. Save Markdown\\n        md_path = ResultSaver._save_markdown(base_path, payload, metadata, printer)\\n\\n        # 2. Save JSON for continuation\\n        json_path = ResultSaver._save_json(base_path, payload, metadata, printer)\\n\\n        return md_path, json_path\\n\\n    @staticmethod\\n    def _save_markdown(\\n        base_path: Path,\\n        payload: Dict[str, Any],\\n        metadata: RequestMetadata,\\n        printer: TypewriterPrinter,\\n    ) -> Path:\\n        \\\"\\\"\\\"Save conversation as Markdown with full history\\\"\\\"\\\"\\n        md_path = base_path.with_suffix(\\\".md\\\")\\n\\n        # Prepare payload for display (hide messages to save space)\\n        display_payload = payload.copy()\\n        messages = display_payload.pop(\\\"messages\\\", [])\\n        if messages:\\n            display_payload[\\\"messages\\\"] = f\\\"[{len(messages)} messages hidden]\\\"\\n\\n        # Build Markdown content\\n        md_content = f\\\"\\\"\\\"# AI Conversation Log\\n**Time**: {metadata.start_time.strftime('%Y-%m-%d %H:%M:%S')}  \\n**Model**: {metadata.model or 'Unknown'}  \\n**Duration**: {metadata.duration:.2f}s  \\n\\n## Request Configuration\\n{CODE_BLOCK}json\\n{json.dumps(display_payload, indent=2, ensure_ascii=False)}\\n{CODE_BLOCK}\\n\\n## Conversation History\\n\\n\\\"\\\"\\\"\\n\\n        # Add all messages\\n        for msg in messages:\\n            role = msg.get(\\\"role\\\", \\\"unknown\\\").upper()\\n            content = msg.get(\\\"content\\\", \\\"\\\")\\n            \\n            # Handle multimodal content\\n            if isinstance(content, list):\\n                texts = []\\n                for item in content:\\n                    if isinstance(item, dict):\\n                        if item.get(\\\"type\\\") == \\\"text\\\":\\n                            texts.append(item.get(\\\"text\\\", \\\"\\\"))\\n                        elif item.get(\\\"type\\\") == \\\"image_url\\\":\\n                            texts.append(\\\"[Image]\\\")\\n                content = \\\"\\\\n\\\".join(texts)\\n            \\n            md_content += f\\\"### {role}\\\\n\\\\n{content}\\\\n\\\\n\\\"\\n\\n        # Add reasoning if present\\n        if printer.reasoning_buffer:\\n            md_content += f\\\"\\\"\\\"\\n---\\n\\n## üí≠ Thinking Process\\n<details>\\n<summary>Click to expand ({len(printer.reasoning_buffer)} chars)</summary>\\n\\n{printer.reasoning_buffer}\\n</details>\\n\\n\\\"\\\"\\\"\\n\\n        # Add response\\n        md_content += f\\\"\\\"\\\"\\n---\\n\\n## ‚ú® Response\\n{printer.content_buffer}\\n\\n\\\"\\\"\\\"\\n\\n        # Add tool calls if present\\n        if metadata.tool_calls:\\n            md_content += f\\\"\\\"\\\"\\n---\\n\\n## üõ†Ô∏è Tool Calls\\n\\\"\\\"\\\"\\n            for i, tool_call in enumerate(metadata.tool_calls, 1):\\n                md_content += f\\\"\\\"\\\"\\n### Tool Call {i}\\n- **Function**: `{tool_call['function']['name']}`\\n- **Arguments**: `{tool_call['function']['arguments']}`\\n\\\"\\\"\\\"\\n            \\n            if metadata.tool_results:\\n                md_content += f\\\"\\\"\\\"\\n### Tool Results\\n\\\"\\\"\\\"\\n                for result in metadata.tool_results:\\n                    md_content += f\\\"\\\"\\\"\\n- **{result['name']}**: {result['content']}\\n\\\"\\\"\\\"\\n\\n        md_content += f\\\"\\\"\\\"\\n---\\n## üìä Statistics\\n\\n- **Finish Reason**: {metadata.finish_reason or 'N/A'}\\n- **Tokens**: {metadata.total_tokens or 'N/A'} (Prompt: {metadata.prompt_tokens or 'N/A'}, Completion: {metadata.completion_tokens or 'N/A'})\\n- **Token Rate**: {f'{metadata.get_token_rate():.1f} tokens/s' if metadata.get_token_rate() else 'N/A'}\\n- **First Token Latency**: {f'{(printer.first_token_time - printer.start_time):.2f}s' if printer.first_token_time else 'N/A'}\\n- **Estimated Total Tokens**: {printer.token_count}\\n\\\"\\\"\\\"\\n\\n        with open(md_path, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n            f.write(md_content)\\n\\n        return md_path\\n\\n    @staticmethod\\n    def _save_json(\\n        base_path: Path,\\n        payload: Dict[str, Any],\\n        metadata: RequestMetadata,\\n        printer: TypewriterPrinter,\\n    ) -> Path:\\n        \\\"\\\"\\\"Save conversation as JSON for continuation\\\"\\\"\\\"\\n        json_path = base_path.with_suffix(\\\".json\\\")\\n\\n        # Build conversation for continuation\\n        continuation = {\\n            \\\"metadata\\\": {\\n                \\\"export_time\\\": datetime.now().isoformat(),\\n                \\\"model\\\": metadata.model,\\n                \\\"total_tokens\\\": metadata.total_tokens,\\n                \\\"duration\\\": metadata.duration,\\n            },\\n            \\\"messages\\\": payload.get(\\\"messages\\\", []).copy(),\\n        }\\n\\n        # Add assistant response\\n        assistant_msg = {\\n            \\\"role\\\": \\\"assistant\\\",\\n            \\\"content\\\": printer.content_buffer,\\n        }\\n        if printer.reasoning_buffer:\\n            assistant_msg[\\\"reasoning_content\\\"] = printer.reasoning_buffer\\n        if metadata.tool_calls:\\n            assistant_msg[\\\"tool_calls\\\"] = metadata.tool_calls\\n\\n        continuation[\\\"messages\\\"].append(assistant_msg)\\n\\n        # Add tool results as tool messages\\n        for result in metadata.tool_results:\\n            continuation[\\\"messages\\\"].append(result)\\n\\n        with open(json_path, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n            json.dump(continuation, f, ensure_ascii=False, indent=2)\\n\\n        return json_path\\n\\n\\n# ============ Argument Parser ============\\n\\ndef create_argument_parser() -> argparse.ArgumentParser:\\n    \\\"\\\"\\\"Create enhanced argument parser\\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"AI Chat CLI - Enhanced with graceful parameter handling\\\",\\n        formatter_class=argparse.RawTextHelpFormatter,\\n        epilog=\\\"\\\"\\\"\\nConfiguration Strategy:\\n  1. Unified settings via config.json to avoid model-specific parameter issues\\n  2. CLI arguments safely override config.json (removes conflicting params)\\n  3. profiles.json or .env provides endpoint and api_key\\n\\nImage Support:\\n  Use @filename.png to attach images (png, jpg, jpeg, gif, webp, bmp supported)\\n  Example: python chat.py @photo.jpg \\\"Describe this image\\\"\\n\\nToken Estimation:\\n  Before sending, the CLI will display estimated prompt tokens and max_tokens\\n  to help you avoid context overflow.\\n\\nTool Support:\\n  Define tools in tools.py with proper function signatures and docstrings\\n  The CLI will automatically load and make them available to the model\\n\\nExamples:\\n  # Basic usage with default config\\n  python chat.py \\\"Hello, how are you?\\\"\\n  \\n  # With file attachment (code or image)\\n  python chat.py @code.py \\\"Explain this code\\\"\\n  python chat.py @image.png \\\"What's in this image?\\\"\\n  \\n  # Custom profile and config\\n  python chat.py -p deepseek -f deepseek_config.json \\\"Explain quantum computing\\\"\\n  \\n  # Continue conversation\\n  python chat.py --context conversation.json \\\"Continue from here\\\"\\n  \\n  # Start new conversation (don't save to config)\\n  python chat.py --new \\\"Start fresh conversation\\\"\\n  \\n  # Disable streaming\\n  python chat.py --no-stream \\\"Generate a long response\\\"\\n  \\n  # Override parameters\\n  python chat.py --temperature 0.5 --max-tokens 2000 \\\"Creative writing\\\"\\n        \\\"\\\"\\\",\\n    )\\n\\n    # Connection\\n    parser.add_argument(\\n        \\\"--profile\\\", \\\"-p\\\", default=\\\"default\\\", help=\\\"Profile name in profiles.json\\\"\\n    )\\n    parser.add_argument(\\\"--endpoint\\\", \\\"-e\\\", help=\\\"Override API endpoint\\\")\\n    parser.add_argument(\\\"--api-key\\\", \\\"-k\\\", help=\\\"Override API key\\\")\\n\\n    # Configuration\\n    parser.add_argument(\\n        \\\"--config\\\", \\\"-c\\\", default=\\\"config.json\\\", help=\\\"Request body configuration file\\\"\\n    )\\n    parser.add_argument(\\n        \\\"--new\\\", action=\\\"store_true\\\", help=\\\"Start new conversation (don't save context to config)\\\"\\n    )\\n\\n    # Input\\n    parser.add_argument(\\\"prompt\\\", nargs=\\\"*\\\", help=\\\"Prompt text (supports @filename for files/images)\\\")\\n    parser.add_argument(\\\"--context\\\", help=\\\"Conversation context JSON file for continuation\\\")\\n\\n    # OpenAI Parameters (override config.json)\\n    parser.add_argument(\\\"--model\\\", \\\"-m\\\", help=\\\"Model name (overrides config)\\\")\\n    parser.add_argument(\\\"--temperature\\\", \\\"-t\\\", type=float, help=\\\"Sampling temperature (0-2)\\\")\\n    parser.add_argument(\\\"--max-tokens\\\", type=int, help=\\\"Maximum tokens to generate\\\")\\n    parser.add_argument(\\\"--top-p\\\", type=float, help=\\\"Nucleus sampling parameter\\\")\\n    parser.add_argument(\\\"--presence-penalty\\\", type=float, help=\\\"Presence penalty\\\")\\n    parser.add_argument(\\\"--frequency-penalty\\\", type=float, help=\\\"Frequency penalty\\\")\\n    parser.add_argument(\\\"--seed\\\", type=int, help=\\\"Random seed for reproducibility\\\")\\n\\n    # Behavior (safely override config)\\n    parser.add_argument(\\n        \\\"--no-stream\\\", action=\\\"store_true\\\", help=\\\"Disable streaming output\\\"\\n    )\\n    parser.add_argument(\\n        \\\"--enable-thinking\\\",\\n        action=\\\"store_true\\\",\\n        default=None,\\n        help=\\\"Enable thinking process\\\",\\n    )\\n    parser.add_argument(\\n        \\\"--no-thinking\\\",\\n        dest=\\\"enable_thinking\\\",\\n        action=\\\"store_false\\\",\\n        help=\\\"Disable thinking process\\\",\\n    )\\n\\n    # Tools\\n    parser.add_argument(\\n        \\\"--tools\\\", default=\\\"tools.py\\\", help=\\\"Python module containing tool functions\\\"\\n    )\\n    parser.add_argument(\\n        \\\"--no-tools\\\", action=\\\"store_true\\\", help=\\\"Disable tool functions\\\"\\n    )\\n\\n    # Output\\n    parser.add_argument(\\\"--output\\\", \\\"-o\\\", help=\\\"Output file base name\\\")\\n\\n    return parser\\n\\n\\n# ============ Main Function ============\\n\\ndef handle_interrupt(signum, frame):\\n    \\\"\\\"\\\"Handle Ctrl+C gracefully\\\"\\\"\\\"\\n    print(f\\\"\\\\n{Colors.warn('‚ö†Ô∏è  Interrupted by user')}\\\")\\n    sys.exit(130)\\n\\n\\ndef main():\\n    \\\"\\\"\\\"Main entry point\\\"\\\"\\\"\\n    # Setup\\n    setup_utf8()\\n    signal.signal(signal.SIGINT, handle_interrupt)\\n\\n    # Parse arguments\\n    parser = create_argument_parser()\\n    args = parser.parse_args()\\n\\n    try:\\n        # 1. Load connection configuration\\n        api_config = ConfigManager.load_connection_config(args)\\n        api_config.validate()\\n\\n        # 2. Load request body from config.json\\n        request_body = ConfigManager.load_request_body(args.config, args)\\n\\n        # 3. Initialize tool manager\\n        tool_manager = None\\n        if not args.no_tools and Path(args.tools).exists():\\n            tool_manager = ToolManager(args.tools)\\n            tool_definitions = tool_manager.get_tool_definitions()\\n            if tool_definitions:\\n                request_body[\\\"tools\\\"] = tool_definitions\\n                print(f\\\"{Colors.info('‚úì')} Added {len(tool_definitions)} tools to request\\\", file=sys.stderr)\\n\\n        # 4. Build messages\\n        messages = MessageBuilder.build_messages(args, request_body)\\n        request_body[\\\"messages\\\"] = messages\\n\\n        # 5. Determine output path\\n        if args.output:\\n            output_base = Path(args.output)\\n        else:\\n            timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n            prompt_hint = (\\n                args.prompt[0][:20]\\n                if args.prompt\\n                else request_body.get(\\\"model\\\", \\\"chat\\\")\\n            )\\n            safe_name = re.sub(r\\\"[^\\\\w\\\\s-]\\\", \\\"\\\", prompt_hint).strip().replace(\\\" \\\", \\\"_\\\")\\n            output_base = Path(f\\\"chat_{safe_name}_{timestamp}\\\")\\n\\n        # 6. Execute request\\n        with open(output_base.with_suffix(\\\".md\\\"), \\\"w\\\", encoding=\\\"utf-8\\\") as out_file:\\n            printer = TypewriterPrinter(out_file)\\n            \\n            with APIClient(api_config) as client:\\n                metadata = client.request(request_body, printer, tool_manager)\\n            \\n            printer.finalize()\\n\\n            # 7. Add separator before complete message\\n            print(f\\\"\\\\n{Colors.bold('‚îÄ' * 60)}\\\")\\n\\n            # 8. Print statistics\\n            print(f\\\"\\\\n{Colors.info('‚úÖ Complete')}\\\")\\n            print(f\\\"   Duration: {metadata.duration:.2f}s\\\")\\n\\n            if metadata.total_tokens:\\n                print(\\n                    f\\\"   Tokens: {metadata.total_tokens} (Prompt: {metadata.prompt_tokens}, Completion: {metadata.completion_tokens})\\\"\\n                )\\n                if token_rate := metadata.get_token_rate():\\n                    print(f\\\"   Token Rate: {token_rate:.1f} tokens/s\\\")\\n\\n            if metadata.finish_reason:\\n                print(f\\\"   Finish Reason: {metadata.finish_reason}\\\")\\n\\n            if printer.first_token_time:\\n                print(\\n                    f\\\"   First Token: {(printer.first_token_time - printer.start_time):.2f}s\\\"\\n                )\\n\\n            if metadata.tool_calls:\\n                print(f\\\"   Tool Calls: {len(metadata.tool_calls)} executed\\\")\\n\\n            # 9. Save results\\n            md_path, json_path = ResultSaver.save(\\n                output_base, request_body, metadata, printer\\n            )\\n\\n            print(f\\\"\\\\n{Colors.info('üíæ Saved:')}\\\")\\n            print(f\\\"   {md_path}\\\")\\n            print(f\\\"   {json_path}\\\")\\n            print(\\n                f\\\"\\\\n{Colors.info('üí° Tip:')} Continue with: --context {json_path.name}\\\"\\n            )\\n\\n            # 10. Save context to config.json if not --new\\n            if not args.new:\\n                # Update messages with assistant response\\n                updated_messages = messages.copy()\\n                \\n                # Add assistant message\\n                assistant_msg = {\\n                    \\\"role\\\": \\\"assistant\\\",\\n                    \\\"content\\\": printer.content_buffer,\\n                }\\n                if printer.reasoning_buffer:\\n                    assistant_msg[\\\"reasoning_content\\\"] = printer.reasoning_buffer\\n                if metadata.tool_calls:\\n                    assistant_msg[\\\"tool_calls\\\"] = metadata.tool_calls\\n                \\n                updated_messages.append(assistant_msg)\\n                \\n                # Add tool results\\n                for result in metadata.tool_results:\\n                    updated_messages.append(result)\\n                \\n                # Save to config.json\\n                ConfigManager.save_context_to_config(args.config, updated_messages)\\n\\n    except KeyboardInterrupt:\\n        print(f\\\"\\\\n{Colors.warn('‚ö†Ô∏è  Interrupted')}\\\", file=sys.stderr)\\n        sys.exit(130)\\n    except (ConfigurationError, FileNotFoundError) as e:\\n        print(f\\\"{Colors.error('‚ùå Configuration Error:')} {e}\\\", file=sys.stderr)\\n        sys.exit(1)\\n    except APIError as e:\\n        print(f\\\"{Colors.error('‚ùå API Error:')} {e}\\\", file=sys.stderr)\\n        sys.exit(1)\\n    except Exception as e:\\n        logger.exception(\\\"Unexpected error\\\")\\n        print(f\\\"{Colors.error('‚ùå Unexpected error:')} {e}\\\", file=sys.stderr)\\n        sys.exit(1)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n```\\n[File: deepseek.py]\\n\\n```json\\n{\\n  \\\"metadata\\\": {\\n    \\\"export_time\\\": \\\"2026-02-06T03:52:29.393673\\\",\\n    \\\"model\\\": \\\"Qwen/Qwen3-8B\\\",\\n    \\\"total_tokens\\\": 771,\\n    \\\"duration\\\": 13.744392\\n  },\\n  \\\"messages\\\": [\\n    {\\n      \\\"role\\\": \\\"user\\\",\\n      \\\"content\\\": \\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\"\\n    },\\n    {\\n      \\\"role\\\": \\\"assistant\\\",\\n      \\\"content\\\": \\\"\\\\n\\\\nÊàëÊó†Ê≥ïÁêÜËß£‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùËøô‰∏™ËØçÁöÑÂÖ∑‰ΩìÂê´‰πâÔºåÊÇ®ÊòØÂê¶Âú®Ë°®ËææÊüêÁßçÊÉÖÁª™ÊàñÈúÄË¶ÅÂ∏ÆÂä©ÔºüÂ¶ÇÊûúÈúÄË¶ÅËÆ°ÁÆó„ÄÅÊü•ËØ¢Êó∂Èó¥ÊàñÂ§©Ê∞î‰ø°ÊÅØÔºåËØ∑ÂëäËØâÊàëÂÖ∑‰ΩìÂÜÖÂÆπÔºåÊàë‰ºöÂ∞ΩÂäõÂçèÂä©ÊÇ®„ÄÇ\\\",\\n      \\\"reasoning_content\\\": \\\"\\\\nOkay, the user just said \\\\\\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\\\\\". Let me think about how to respond. First, I need to understand what they mean. \\\\\\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\\\\\" is a term that's often used in Chinese online culture, especially in gaming communities. It's a way to express frustration or exasperation, similar to \\\\\\\"arara\\\\\\\" in Japanese or \\\\\\\"arrgghh\\\\\\\" in English. People might use it when they're upset about something in a game, like a loss or a difficult situation.\\\\n\\\\nNow, looking at the tools provided, there are three functions: calculate, get_current_time, and search_weather. The user's message doesn't seem to involve any of these. They didn't ask for a math calculation, didn't mention a timezone for the current time, and didn't request weather info for a city. So, none of the tools are applicable here. \\\\n\\\\nI should check if there's any hidden request or if the user is testing me. But \\\\\\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\\\\\" is just an expression. Maybe they want me to acknowledge their frustration or explain the term? However, since the tools don't cover that, I need to respond without using any function calls. \\\\n\\\\nThe correct approach is to inform the user that I can't assist with their current message and ask how I can help. That way, I stay within the provided tools and offer further support if needed.\\\\n\\\"\\n    },\\n    {\\n      \\\"role\\\": \\\"user\\\",\\n      \\\"content\\\": \\\"‰ªäÊó•Â§©Ê∞î\\\"\\n    },\\n    {\\n      \\\"role\\\": \\\"assistant\\\",\\n      \\\"content\\\": \\\"\\\\n\\\\nËØ∑ÂëäËØâÊàëÊÇ®ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞îÔºåÊàëÂ∞Ü‰∏∫ÊÇ®ÊêúÁ¥¢Áõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ\\\",\\n      \\\"reasoning_content\\\": \\\"\\\\nÂ•ΩÁöÑÔºåÁî®Êà∑Áé∞Âú®ÈóÆÁöÑÊòØ‚Äú‰ªäÊó•Â§©Ê∞î‚ÄùÔºåÁúãËµ∑Êù•‰ªñ‰ª¨ÊÉ≥‰∫ÜËß£‰ªäÂ§©ÁöÑÂ§©Ê∞îÊÉÖÂÜµ„ÄÇ‰∏çËøá‰πãÂâçÁöÑÂØπËØù‰∏≠ÔºåÁî®Êà∑ËØ¥‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùÔºåÊàëÊ≤°ËÉΩÂáÜÁ°ÆÁêÜËß£‰ªñ‰ª¨ÁöÑÊÑèÊÄù„ÄÇÂèØËÉΩÁî®Êà∑ÊòØÂú®ÊµãËØïÊàñËÄÖÁî®ÊüêÁßçÊöóÂè∑Ôºå‰ΩÜÂêéÊù•‰ªñ‰ª¨Áõ¥Êé•ÈóÆÂ§©Ê∞î‰∫Ü„ÄÇ\\\\n\\\\nÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁ°ÆÂÆöÁî®Êà∑ÊòØÂê¶ÈúÄË¶ÅÊü•ËØ¢Â§©Ê∞î„ÄÇÊ†πÊçÆÂΩìÂâçÁöÑÂ∑•ÂÖ∑ÔºåÊúâ‰∏Ä‰∏™search_weatherÂáΩÊï∞ÔºåÈúÄË¶ÅÂèÇÊï∞ÊòØÂüéÂ∏Ç„ÄÇÁî®Êà∑Ê≤°ÊúâÊèêÂà∞ÂÖ∑‰ΩìÂüéÂ∏ÇÔºåÊâÄ‰ª•ÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•ËØ¢ÈóÆ„ÄÇ‰ΩÜÊúâÊó∂ÂÄôÁî®Êà∑ÂèØËÉΩÈªòËÆ§Êü•ËØ¢ÊâÄÂú®ÂüéÂ∏ÇÔºå‰∏çËøáËøôÈáåÊ≤°Êúâ‰∏ä‰∏ãÊñáÔºåÊâÄ‰ª•‰∏çËÉΩÁ°ÆÂÆö„ÄÇ\\\\n\\\\nÂè¶Â§ñÔºåÁî®Êà∑ÂèØËÉΩÊòØÂú®ÁªßÁª≠‰πãÂâçÁöÑÂØπËØùÔºå‰ΩÜ‰πãÂâçÁöÑ‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùÊ≤°ÊúâÊòéÁ°Æ‰ø°ÊÅØ„ÄÇÊâÄ‰ª•ÊúÄÂ•ΩËøòÊòØÂÖàÁ°ÆËÆ§Áî®Êà∑ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞î„ÄÇ‰∏çËøá‰πüÊúâÂèØËÉΩÁî®Êà∑Âè™ÊòØÁ™ÅÁÑ∂ÊÉ≥‰∫ÜËß£Â§©Ê∞îÔºåÊ≤°ÊúâÊèêÂà∞ÂüéÂ∏ÇÔºåËøôÊó∂ÂÄôÂèØËÉΩÈúÄË¶ÅÊèêÁ§∫‰ªñ‰ª¨Êèê‰æõÂüéÂ∏ÇÂêçÁß∞„ÄÇ\\\\n\\\\nÊ£ÄÊü•Â∑•ÂÖ∑‰∏≠ÁöÑÂáΩÊï∞Ôºåsearch_weatherÁöÑÂèÇÊï∞ÊòØcityÔºåÂøÖÂ°´„ÄÇÊâÄ‰ª•ÂøÖÈ°ªËé∑ÂèñÂüéÂ∏Ç‰ø°ÊÅØÊâçËÉΩË∞ÉÁî®„ÄÇÂõ†Ê≠§ÔºåÊàëÂ∫îËØ•ÂõûÂ§çÁî®Êà∑ÔºåËØ¢ÈóÆ‰ªñ‰ª¨ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞î„ÄÇËøôÊ†∑Êó¢Á¨¶ÂêàÂ∑•ÂÖ∑ÁöÑË¶ÅÊ±ÇÔºå‰πüËÉΩÁ°Æ‰øùÊèê‰æõÂáÜÁ°ÆÁöÑ‰ø°ÊÅØ„ÄÇ\\\\n\\\\nÂè¶Â§ñÔºåÊòØÂê¶ÊúâÂÖ∂‰ªñÂèØËÉΩÔºüÊØîÂ¶ÇÁî®Êà∑ÂèØËÉΩËØØËæìÂÖ•ÔºåÊàñËÄÖÊúâÂÖ∂‰ªñÊÑèÂõæÔºü‰ΩÜÊ†πÊçÆÁé∞ÊúâÂ∑•ÂÖ∑ÔºåÂè™ÊúâÂ§©Ê∞î„ÄÅËÆ°ÁÆóÊó∂Èó¥ÂíåËé∑ÂèñÊó∂Èó¥ÁöÑÂäüËÉΩ„ÄÇÊâÄ‰ª•ÊúÄÂêàÁêÜÁöÑÂõûÂ∫îÊòØËØ∑Ê±ÇÂüéÂ∏ÇÂêçÁß∞„ÄÇ\\\\n\\\"\\n    },\\n    {\\n      \\\"role\\\": \\\"user\\\",\\n      \\\"content\\\": \\\"ÂçóÊûÅ\\\"\\n    },\\n    {\\n      \\\"role\\\": \\\"assistant\\\",\\n      \\\"content\\\": \\\"\\\\n\\\\n\\\",\\n      \\\"reasoning_content\\\": \\\"\\\\nÂ•ΩÁöÑÔºåÁî®Êà∑Áé∞Âú®ÊèêÂà∞‚ÄúÂçóÊûÅ‚ÄùÔºåÈúÄË¶ÅÊü•ËØ¢ÈÇ£ÈáåÁöÑÂ§©Ê∞î„ÄÇÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁ°ÆËÆ§Áî®Êà∑ÊòØÂê¶ÁúüÁöÑÊÉ≥Êü•ËØ¢ÂçóÊûÅÁöÑÂ§©Ê∞î„ÄÇÂçóÊûÅÊòØ‰∏Ä‰∏™Â§ßÊ¥≤ÔºåÂ§©Ê∞îÊÉÖÂÜµÂèØËÉΩÊØîËæÉÁâπÊÆäÔºåËÄå‰∏îÁî®Êà∑ÂèØËÉΩÊ≤°ÊúâÊÑèËØÜÂà∞ÈúÄË¶ÅÊåáÂÆöÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ‰∏çËøáÔºåÊ†πÊçÆ‰πãÂâçÁöÑÂØπËØùÔºåÁî®Êà∑ÂèØËÉΩÊòØÂú®ÊµãËØïÊàñËÄÖÊúâÂÖ∂‰ªñÊÑèÂõæÔºå‰ΩÜÊ†πÊçÆÂ∑•ÂÖ∑‰∏≠ÁöÑsearch_weatherÂáΩÊï∞ÔºåÂèÇÊï∞ÊòØÂüéÂ∏ÇÔºåÊâÄ‰ª•ÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•Á°ÆËÆ§„ÄÇ\\\\n\\\\n‰∏çËøáÔºåÁî®Êà∑‰πãÂâçÂ∑≤ÁªèÈóÆËøá‚Äú‰ªäÊó•Â§©Ê∞î‚ÄùÔºåÁÑ∂ÂêéÊàëËÆ©‰ªñÊåáÂÆöÂüéÂ∏ÇÔºåÁé∞Âú®‰ªñÁõ¥Êé•ÂõûÁ≠î‚ÄúÂçóÊûÅ‚ÄùÔºåÂèØËÉΩÊòØÊåáÊï¥‰∏™ÂçóÊûÅÊ¥≤ÁöÑÂ§©Ê∞îÊÉÖÂÜµ„ÄÇ‰ΩÜÂçóÊûÅÊ¥≤Ê≤°ÊúâÁªü‰∏ÄÁöÑÂüéÂ∏ÇÔºåÂèØËÉΩÊúâÁßëÁ†îÁ´ôÔºåÊØîÂ¶ÇÈ∫¶ÂÖãÈªòÂ§öÁ´ô„ÄÅÈïøÂüéÁ´ôÁ≠â„ÄÇËøôÊó∂ÂÄôÊàëÈúÄË¶ÅÂà§Êñ≠ÊòØÂê¶Â∫îËØ•Áõ¥Êé•‰ΩøÁî®‚ÄúÂçóÊûÅ‚Äù‰Ωú‰∏∫ÂüéÂ∏ÇÂèÇÊï∞ÔºåÊàñËÄÖÊòØÂê¶ÈúÄË¶ÅÊèêÁ§∫Áî®Êà∑ËæìÂÖ•ÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ\\\\n\\\\nÊ†πÊçÆÂ∑•ÂÖ∑ÁöÑÊèèËø∞Ôºåsearch_weatherÂáΩÊï∞ÁöÑÂèÇÊï∞ÊòØcityÔºåÁ±ªÂûã‰∏∫Â≠óÁ¨¶‰∏≤Ôºå‰ΩÜÂèØËÉΩËØ•ÂáΩÊï∞ÂÜÖÈÉ®Â§ÑÁêÜ‰∫ÜÁ±ª‰ºº‚ÄúÂçóÊûÅ‚ÄùÁöÑËæìÂÖ•ÔºåÊàñËÄÖÁî®Êà∑ÂèØËÉΩÊúüÊúõÂæóÂà∞ÂçóÊûÅÊ¥≤ÁöÑÂ§©Ê∞îÊ¶ÇÂÜµ„ÄÇ‰πüÊúâÂèØËÉΩÁî®Êà∑ËæìÂÖ•ÊúâËØØÔºåÊàñËÄÖÊÉ≥‰∫ÜËß£Êüê‰∏™ÁâπÂÆöÂüéÂ∏ÇÁöÑÂ§©Ê∞îÔºå‰ΩÜËØØÂÜô‰∏∫ÂçóÊûÅ„ÄÇ‰æãÂ¶ÇÔºåÁî®Êà∑ÂèØËÉΩÊÉ≥Êü•Âåó‰∫¨„ÄÅ‰∏äÊµ∑Á≠âÂüéÂ∏ÇÔºå‰ΩÜÈîôËØØËæìÂÖ•‰∫Ü‚ÄúÂçóÊûÅ‚Äù„ÄÇ\\\\n\\\\nËøôÊó∂ÂÄôÔºåÊàëÈúÄË¶ÅÂÖàË∞ÉÁî®search_weatherÂáΩÊï∞ÔºåÂ∞ùËØï‰ΩøÁî®‚ÄúÂçóÊûÅ‚Äù‰Ωú‰∏∫cityÂèÇÊï∞ÔºåÁúãÁúãÊòØÂê¶ËÉΩËøîÂõûÊúâÊïàÁªìÊûú„ÄÇÂ¶ÇÊûúËøîÂõûÈîôËØØÔºåÂèØËÉΩÈúÄË¶ÅÊèêÁ§∫Áî®Êà∑Á°ÆËÆ§ÂüéÂ∏ÇÂêçÁß∞„ÄÇ‰ΩÜÊ†πÊçÆÂΩìÂâçÂ∑•ÂÖ∑ÁöÑËÆæÂÆöÔºåÂèØËÉΩÈúÄË¶ÅÁõ¥Êé•Ë∞ÉÁî®ÂáΩÊï∞ÔºåÁÑ∂ÂêéÊ†πÊçÆÁªìÊûúÂ§ÑÁêÜ„ÄÇÂ¶ÇÊûúÂáΩÊï∞ÊîØÊåÅÂú∞Âå∫Êü•ËØ¢ÔºåÂèØËÉΩÊ≤°ÈóÆÈ¢òÔºõÂ¶ÇÊûú‰∏çÊîØÊåÅÔºåÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•ÊæÑÊ∏Ö„ÄÇ\\\\n\\\\nÂõ†Ê≠§ÔºåÊ≠£Á°ÆÁöÑÂÅöÊ≥ïÊòØÂÖàË∞ÉÁî®search_weatherÂáΩÊï∞ÔºåÂèÇÊï∞‰∏∫city: \\\\\\\"ÂçóÊûÅ\\\\\\\"ÔºåÁÑ∂ÂêéÊ†πÊçÆËøîÂõûÁöÑÊï∞ÊçÆÂÜ≥ÂÆöÊòØÂê¶ÈúÄË¶ÅËøõ‰∏ÄÊ≠•ËØ¢ÈóÆÁî®Êà∑ÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ‰ΩÜÊ†πÊçÆÁî®Êà∑‰πãÂâçÁöÑÂØπËØùÔºå‰ªñ‰ª¨ÂèØËÉΩÂè™ÊòØÊÉ≥‰∫ÜËß£ÂçóÊûÅÁöÑÂ§©Ê∞îÊÉÖÂÜµÔºåÊâÄ‰ª•Áõ¥Êé•Ë∞ÉÁî®ÂáΩÊï∞ÊòØÂêàÈÄÇÁöÑ„ÄÇ\\\\n\\\",\\n      \\\"tool_calls\\\": [\\n        {\\n          \\\"id\\\": null,\\n          \\\"type\\\": \\\"function\\\",\\n          \\\"function\\\": {\\n            \\\"name\\\": \\\"search_weather\\\",\\n            \\\"arguments\\\": \\\" {\\\\\\\"city\\\\\\\": \\\\\\\"ÂçóÊûÅ\\\\\\\"}\\\"\\n          }\\n        }\\n      ]\\n    },\\n    {\\n      \\\"tool_call_id\\\": null,\\n      \\\"role\\\": \\\"tool\\\",\\n      \\\"name\\\": \\\"search_weather\\\",\\n      \\\"content\\\": \\\"\\\\\\\"Weather in ÂçóÊûÅ: Sunny, 25¬∞C\\\\\\\"\\\"\\n    }\\n  ]\\n}\\n```\\n[File: chat_ÂçóÊûÅ_20260206_035215.json]\\n\\nÂú®Ëøô‰∏™tool call‰πãÂêéÔºåËøîÂõûtool_calls stop reason,‰ΩÜÊòØ‰Ωú‰∏∫contextËæìÂÖ•ÂõûÂéªÁöÑÊó∂ÂÄôÔºåÂìçÂ∫î400ÔºåÂÖ∑‰ΩìÂÜÖÂÆπ‰πàÔºåÂõ†‰∏∫pythonËÑöÊú¨‰∏çÊâìÂç∞ÔºåÊâÄ‰ª•Êàë‰πü‰∏çÁü•ÈÅì„ÄÇapiÂèØËÉΩËøîÂõû‰∫Ü‰πüÂèØËÉΩÊ≤°ËøîÂõûÔºåÊàë‰∏çÁü•ÈÅì„ÄÇÊÄª‰πã‰Ω†ÊÉ≥ÊÉ≥ÂäûÊ≥ïdebug‰∏ã\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Ê†πÊçÆÊÇ®Êèê‰æõÁöÑÂØπËØùÂéÜÂè≤ÂíåÈîôËØØÊèèËø∞ÔºåÈóÆÈ¢òÂæàÂèØËÉΩÂá∫Áé∞Âú® **Â∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫null** ‰∏ä„ÄÇÂú®OpenAIÂÖºÂÆπÁöÑAPI‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®ÂøÖÈ°ªÂåÖÂê´ÂîØ‰∏ÄÁöÑIDÔºåÂ∑•ÂÖ∑ÂìçÂ∫î‰πüÂøÖÈ°ªÂºïÁî®Áõ∏ÂêåÁöÑID„ÄÇÂΩìID‰∏∫nullÊó∂ÔºåAPI‰ºöËøîÂõû400ÈîôËØØ„ÄÇ\\n\\n## ÈóÆÈ¢òÂàÜÊûê\\n\\n‰ªé `chat_ÂçóÊûÅ_20260206_035215.json` ÂèØ‰ª•ÁúãÂà∞Ôºö\\n```json\\n{\\n  \\\"tool_call_id\\\": null,\\n  \\\"role\\\": \\\"tool\\\",\\n  \\\"name\\\": \\\"search_weather\\\",\\n  \\\"content\\\": \\\"\\\\\\\"Weather in ÂçóÊûÅ: Sunny, 25¬∞C\\\\\\\"\\\"\\n}\\n```\\n\\n`tool_call_id` ‰∏∫ `null`ÔºåËøôËøùÂèç‰∫ÜAPIËßÑËåÉ„ÄÇÂΩìËøô‰∏™ÂØπËØùÂéÜÂè≤‰Ωú‰∏∫‰∏ä‰∏ãÊñáÂÜçÊ¨°ÂèëÈÄÅÊó∂ÔºåAPI‰ºöÊãíÁªùËØ∑Ê±Ç„ÄÇ\\n\\n## Ëß£ÂÜ≥ÊñπÊ°à\\n\\n‰øÆÊîπ `deepseek.py` ‰∏≠ÁöÑ `ToolManager.execute_tool` ÊñπÊ≥ïÔºåÂú®Â∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÊó∂Ëá™Âä®ÁîüÊàêÂîØ‰∏ÄID„ÄÇ\\n\\n### ‰øÆÊîπÂêéÁöÑ `ToolManager.execute_tool` ÊñπÊ≥ïÔºö\\n\\n```python\\ndef execute_tool(self, tool_call: Dict) -> Dict:\\n    \\\"\\\"\\\"Execute a tool call and return result\\\"\\\"\\\"\\n    try:\\n        function_name = tool_call[\\\"function\\\"][\\\"name\\\"]\\n        function_args = json.loads(tool_call[\\\"function\\\"][\\\"arguments\\\"])\\n        \\n        if function_name not in self.available_functions:\\n            raise ToolError(f\\\"Unknown function: {function_name}\\\")\\n        \\n        print(f\\\"\\\\n{Colors.CYAN}üîß Executing tool: {function_name}{Colors.END}\\\", file=sys.stderr)\\n        print(f\\\"   Arguments: {function_args}\\\", file=sys.stderr)\\n        \\n        # Execute the function\\n        result = self.available_functions[function_name](**function_args)\\n        \\n        print(f\\\"   Result: {result}\\\", file=sys.stderr)\\n        \\n        # ‰øÆÂ§çÔºöÂ§ÑÁêÜnullÊàñÁ©∫ÁöÑtool_call_id\\n        tool_call_id = tool_call.get(\\\"id\\\")\\n        if not tool_call_id:\\n            import uuid\\n            tool_call_id = str(uuid.uuid4())\\n            print(f\\\"{Colors.warn('‚ö†')} Generated tool call ID: {tool_call_id}\\\", file=sys.stderr)\\n        \\n        return {\\n            \\\"tool_call_id\\\": tool_call_id,\\n            \\\"role\\\": \\\"tool\\\",\\n            \\\"name\\\": function_name,\\n            \\\"content\\\": json.dumps(result, ensure_ascii=False)\\n        }\\n        \\n    except Exception as e:\\n        error_msg = f\\\"Tool execution failed: {str(e)}\\\"\\n        print(f\\\"{Colors.error('‚ùå')} {error_msg}\\\", file=sys.stderr)\\n        \\n        # ‰øÆÂ§çÔºöÈîôËØØÂìçÂ∫î‰πüÁîüÊàêID\\n        tool_call_id = tool_call.get(\\\"id\\\")\\n        if not tool_call_id:\\n            import uuid\\n            tool_call_id = str(uuid.uuid4())\\n        \\n        return {\\n            \\\"tool_call_id\\\": tool_call_id,\\n            \\\"role\\\": \\\"tool\\\",\\n            \\\"name\\\": tool_call[\\\"function\\\"][\\\"name\\\"],\\n            \\\"content\\\": json.dumps({\\\"error\\\": error_msg}, ensure_ascii=False)\\n        }\\n```\\n\\n### È¢ùÂ§ñ‰øÆÂ§çÔºöÁ°Æ‰øù‰øùÂ≠ò‰∏ä‰∏ãÊñáÊó∂ID‰∏ç‰∏∫null\\n\\nÂú® `ResultSaver._save_json` ÊñπÊ≥ï‰∏≠ÔºåÊ∑ªÂä†IDÈ™åËØÅÔºö\\n\\n```python\\n@staticmethod\\ndef _save_json(\\n    base_path: Path,\\n    payload: Dict[str, Any],\\n    metadata: RequestMetadata,\\n    printer: TypewriterPrinter,\\n) -> Path:\\n    \\\"\\\"\\\"Save conversation as JSON for continuation\\\"\\\"\\\"\\n    json_path = base_path.with_suffix(\\\".json\\\")\\n\\n    # Build conversation for continuation\\n    continuation = {\\n        \\\"metadata\\\": {\\n            \\\"export_time\\\": datetime.now().isoformat(),\\n            \\\"model\\\": metadata.model,\\n            \\\"total_tokens\\\": metadata.total_tokens,\\n            \\\"duration\\\": metadata.duration,\\n        },\\n        \\\"messages\\\": payload.get(\\\"messages\\\", []).copy(),\\n    }\\n\\n    # Add assistant response\\n    assistant_msg = {\\n        \\\"role\\\": \\\"assistant\\\",\\n        \\\"content\\\": printer.content_buffer,\\n    }\\n    if printer.reasoning_buffer:\\n        assistant_msg[\\\"reasoning_content\\\"] = printer.reasoning_buffer\\n    if metadata.tool_calls:\\n        # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_calls‰∏≠ÁöÑID‰∏ç‰∏∫null\\n        for tool_call in metadata.tool_calls:\\n            if not tool_call.get(\\\"id\\\"):\\n                import uuid\\n                tool_call[\\\"id\\\"] = str(uuid.uuid4())\\n        assistant_msg[\\\"tool_calls\\\"] = metadata.tool_calls\\n\\n    continuation[\\\"messages\\\"].append(assistant_msg)\\n\\n    # Add tool results as tool messages\\n    for result in metadata.tool_results:\\n        # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_call_id‰∏ç‰∏∫null\\n        if not result.get(\\\"tool_call_id\\\"):\\n            import uuid\\n            result[\\\"tool_call_id\\\"] = str(uuid.uuid4())\\n        continuation[\\\"messages\\\"].append(result)\\n\\n    with open(json_path, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n        json.dump(continuation, f, ensure_ascii=False, indent=2)\\n\\n    return json_path\\n```\\n\\n## Â¶Ç‰Ωï‰ΩøÁî®‰øÆÂ§ç\\n\\n1. **Â§á‰ªΩÂéüÊñá‰ª∂**Ôºö\\n   ```bash\\n   cp deepseek.py deepseek.py.backup\\n   ```\\n\\n2. **Â∫îÁî®‰øÆÂ§ç**Ôºö\\n   - ÊâìÂºÄ `deepseek.py`\\n   - ÊâæÂà∞ `ToolManager.execute_tool` ÊñπÊ≥ïÔºàÂ§ßÁ∫¶Âú®Á¨¨200Ë°åÔºâ\\n   - Áî®‰∏äÈù¢ÁöÑ‰øÆÂ§ç‰ª£Á†ÅÊõøÊç¢\\n   - ÂèØÈÄâÔºöÊâæÂà∞ `ResultSaver._save_json` ÊñπÊ≥ïÂπ∂Â∫îÁî®Á¨¨‰∫å‰∏™‰øÆÂ§ç\\n\\n3. **ÈáçÊñ∞ËøêË°åÂØπËØù**Ôºö\\n   ```bash\\n   python deepseek.py --context chat_ÂçóÊûÅ_20260206_035215.json \\\"ÁªßÁª≠\\\"\\n   ```\\n\\n## È™åËØÅ‰øÆÂ§ç\\n\\n‰øÆÂ§çÂêéÔºåÊñ∞ÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®Â∞Ü‰ºöÊúâÂîØ‰∏ÄÁöÑIDÔºå‰øùÂ≠òÁöÑJSONÊñá‰ª∂‰∏≠ÁöÑ `tool_call_id` ‰πü‰∏ç‰ºöÊòØnull„ÄÇËøôÊ†∑ÂΩìÊÇ®ÂÜçÊ¨°‰ΩøÁî® `--context` ÂèÇÊï∞Âä†ËΩΩÂØπËØùÂéÜÂè≤Êó∂ÔºåAPIÂ∞±‰∏ç‰ºöËøîÂõû400ÈîôËØØ‰∫Ü„ÄÇ\\n\\n## ‰∏¥Êó∂Ëß£ÂÜ≥ÊñπÊ°à\\n\\nÂ¶ÇÊûúÊÇ®‰∏çÊÉ≥‰øÆÊîπ‰ª£Á†ÅÔºåÂèØ‰ª•ÊâãÂä®ÁºñËæëJSONÊñá‰ª∂ÔºåÂ∞Ü `tool_call_id` ‰ªé `null` Êîπ‰∏∫‰∏Ä‰∏™ÂîØ‰∏ÄÂÄºÔºö\\n\\n```json\\n{\\n  \\\"tool_call_id\\\": \\\"generated_id_123\\\",\\n  \\\"role\\\": \\\"tool\\\",\\n  \\\"name\\\": \\\"search_weather\\\",\\n  \\\"content\\\": \\\"\\\\\\\"Weather in ÂçóÊûÅ: Sunny, 25¬∞C\\\\\\\"\\\"\\n}\\n```\\n\\n‰ΩÜÂª∫ËÆÆ‰ΩøÁî®‰ª£Á†Å‰øÆÂ§çÔºåËøôÊ†∑ÂèØ‰ª•Ëá™Âä®Â§ÑÁêÜÊâÄÊúâÊú™Êù•ÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®„ÄÇ\",\n      \"reasoning_content\": \"Áî®Êà∑Êèê‰æõ‰∫Ü‰∏Ä‰∏™PythonËÑöÊú¨Ôºàdeepseek.pyÔºâÂíå‰∏Ä‰∏™JSONÊñá‰ª∂Ôºàchat_ÂçóÊûÅ_20260206_035215.jsonÔºâ„ÄÇËÑöÊú¨ÊòØ‰∏Ä‰∏™AIËÅäÂ§©CLIÂ∑•ÂÖ∑ÔºåÁî®‰∫é‰∏éOpenAIÂÖºÂÆπÁöÑAPI‰∫§‰∫í„ÄÇJSONÊñá‰ª∂ÊòØÂØπËØùÂéÜÂè≤ËÆ∞ÂΩïÔºåÂåÖÂê´Áî®Êà∑ÂíåÂä©Êâã‰πãÈó¥ÁöÑÊ∂àÊÅØÔºå‰ª•ÂèäÂ∑•ÂÖ∑Ë∞ÉÁî®„ÄÇ\\n\\nÁî®Êà∑ÊèêÂà∞ÔºåÂú®Â∑•ÂÖ∑Ë∞ÉÁî®‰πãÂêéÔºåËøîÂõû‰∫Ü‚Äútool_calls stop reason‚ÄùÔºå‰ΩÜÊòØÂΩì‰Ωú‰∏∫‰∏ä‰∏ãÊñáËæìÂÖ•ÂõûÂéªÊó∂ÔºåÂìçÂ∫îÊòØ400„ÄÇÁî®Êà∑‰∏çÁü•ÈÅìÂÖ∑‰ΩìÂÜÖÂÆπÔºåÂõ†‰∏∫PythonËÑöÊú¨Ê≤°ÊúâÊâìÂç∞Âá∫Êù•„ÄÇÁî®Êà∑Â∏åÊúõÊàëÂ∏ÆÂøôË∞ÉËØïËøô‰∏™ÈóÆÈ¢ò„ÄÇ\\n\\nÊàëÈúÄË¶ÅÂàÜÊûêËøô‰∏™ÈóÆÈ¢ò„ÄÇ‰ªéJSONÊñá‰ª∂Êù•ÁúãÔºåÂØπËØùÂéÜÂè≤Â¶Ç‰∏ãÔºö\\n\\n1. Áî®Êà∑Ôºö‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚Äù\\n2. Âä©ÊâãÔºöÂõûÂ∫î‰∏çÁêÜËß£ÔºåÂπ∂ËØ¢ÈóÆÊòØÂê¶ÈúÄË¶ÅÂ∏ÆÂä©„ÄÇ\\n3. Áî®Êà∑Ôºö‚Äú‰ªäÊó•Â§©Ê∞î‚Äù\\n4. Âä©ÊâãÔºöËØ¢ÈóÆÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ\\n5. Áî®Êà∑Ôºö‚ÄúÂçóÊûÅ‚Äù\\n6. Âä©ÊâãÔºöÊÄùËÄÉÂêéÂÜ≥ÂÆöË∞ÉÁî®search_weatherÂ∑•ÂÖ∑ÔºåÂèÇÊï∞‰∏∫city: \\\"ÂçóÊûÅ\\\"„ÄÇ\\n7. Â∑•ÂÖ∑Ë∞ÉÁî®ÁªìÊûúÔºöËøîÂõû‰∫Ü‚ÄúWeather in ÂçóÊûÅ: Sunny, 25¬∞C‚Äù„ÄÇ\\n\\nÁé∞Âú®ÔºåÈóÆÈ¢òÂá∫Áé∞Âú®Â∑•ÂÖ∑Ë∞ÉÁî®‰πãÂêé„ÄÇÂΩìËøô‰∏™ÂØπËØùÂéÜÂè≤‰Ωú‰∏∫‰∏ä‰∏ãÊñáËæìÂÖ•ÂõûAPIÊó∂ÔºåÂèØËÉΩÂá∫Áé∞‰∫Ü400ÈîôËØØ„ÄÇËøôÂèØËÉΩÊòØÂõ†‰∏∫Ê∂àÊÅØÊ†ºÂºè‰∏çÊ≠£Á°ÆÔºåÊàñËÄÖÂ∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÂìçÂ∫îÊ†ºÂºè‰∏çÁ¨¶ÂêàAPIÁöÑÊúüÊúõ„ÄÇ\\n\\nËÆ©Êàë‰ªîÁªÜÁúãÁúãJSON‰∏≠ÁöÑÊ∂àÊÅØÁªìÊûÑÔºö\\n\\n- Áî®Êà∑Ê∂àÊÅØÔºöÂåÖÂê´‚Äúrole‚ÄùÂíå‚Äúcontent‚Äù„ÄÇ\\n- Âä©ÊâãÊ∂àÊÅØÔºöÂåÖÂê´‚Äúrole‚Äù„ÄÅ‚Äúcontent‚Äù„ÄÅ‚Äúreasoning_content‚ÄùÔºåÊúâÊó∂ËøòÊúâ‚Äútool_calls‚Äù„ÄÇ\\n- Â∑•ÂÖ∑Ê∂àÊÅØÔºöÂåÖÂê´‚Äútool_call_id‚Äù„ÄÅ‚Äúrole‚Äù„ÄÅ‚Äúname‚Äù„ÄÅ‚Äúcontent‚Äù„ÄÇ\\n\\nÂú®JSON‰∏≠ÔºåÊúâ‰∏Ä‰∏™Â∑•ÂÖ∑Ê∂àÊÅØÔºåÂÖ∂‚Äútool_call_id‚Äù‰∏∫null„ÄÇËøôÂèØËÉΩÊòØÈóÆÈ¢òÊâÄÂú®ÔºöÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÔºåËøôÂèØËÉΩÂØºËá¥APIÊãíÁªùËØ∑Ê±Ç„ÄÇ\\n\\nÂú®OpenAIÁöÑAPI‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®ÈúÄË¶ÅÊúâ‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑIDÔºåÂπ∂‰∏îÂ∑•ÂÖ∑ÂìçÂ∫îÂøÖÈ°ªÂºïÁî®Áõ∏ÂêåÁöÑID„ÄÇÂ¶ÇÊûúID‰∏∫nullÊàñÁº∫Â§±ÔºåAPIÂèØËÉΩ‰ºöËøîÂõû400ÈîôËØØ„ÄÇ\\n\\nÊ≠§Â§ñÔºåÂä©ÊâãÊ∂àÊÅØ‰∏≠ÁöÑ‚Äútool_calls‚ÄùÊï∞ÁªÑÂåÖÂê´‰∏Ä‰∏™ID‰∏∫nullÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®„ÄÇËøô‰πü‰∏çÁ¨¶ÂêàËßÑËåÉ„ÄÇ\\n\\nÂõ†Ê≠§ÔºåÈóÆÈ¢òÂèØËÉΩÂú®‰∫éÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫null„ÄÇÊàë‰ª¨ÈúÄË¶ÅÁ°Æ‰øùÂ∑•ÂÖ∑Ë∞ÉÁî®ÊúâÂîØ‰∏ÄÁöÑIDÔºåÂπ∂‰∏îÂ∑•ÂÖ∑ÂìçÂ∫î‰∏é‰πãÂåπÈÖç„ÄÇ\\n\\nÂè¶Â§ñÔºåPythonËÑöÊú¨ÂèØËÉΩÊ≤°ÊúâÊ≠£Á°ÆÂ§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®ÁöÑID„ÄÇÂú®ËÑöÊú¨‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®IDÊòØ‰ªéAPIÂìçÂ∫î‰∏≠Ëé∑ÂèñÁöÑÔºå‰ΩÜÂèØËÉΩÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ã‰∏∫null„ÄÇËøôÈúÄË¶ÅÊ£ÄÊü•ËÑöÊú¨‰∏≠Â§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÈÉ®ÂàÜ„ÄÇ\\n\\nÁÑ∂ËÄåÔºåÁî®Êà∑Ê≤°ÊúâÊèê‰æõÂÆåÊï¥ÁöÑÈîôËØØ‰ø°ÊÅØÔºåÊâÄ‰ª•ÊàëÂè™ËÉΩÊé®Êµã„ÄÇ\\n\\n‰∏∫‰∫ÜË∞ÉËØïÔºåÊàëÂèØ‰ª•Âª∫ËÆÆÁî®Êà∑Ê£ÄÊü•APIÂìçÂ∫î‰∏≠Â∑•ÂÖ∑Ë∞ÉÁî®ÁöÑIDÔºåÂπ∂Á°Æ‰øùÂú®Â∑•ÂÖ∑ÂìçÂ∫î‰∏≠Ê≠£Á°ÆËÆæÁΩÆ„ÄÇÊ≠§Â§ñÔºåÂèØ‰ª•Â∞ùËØï‰øÆÊîπËÑöÊú¨ÔºåÁ°Æ‰øùÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏ç‰∏∫null„ÄÇ\\n\\n‰ΩÜÁî®Êà∑ÂèØËÉΩÂ∏åÊúõÊàëÁõ¥Êé•‰øÆÂ§çËÑöÊú¨„ÄÇÊàëÂèØ‰ª•ÂàÜÊûêËÑöÊú¨‰∏≠Â§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÈÉ®ÂàÜÔºåÊâæÂá∫ÊΩúÂú®ÈóÆÈ¢ò„ÄÇ\\n\\nËÆ©ÊàëÁúãÁúãËÑöÊú¨‰∏≠‰∏éÂ∑•ÂÖ∑Ë∞ÉÁî®Áõ∏ÂÖ≥ÁöÑÈÉ®ÂàÜÔºö\\n\\nÂú®APIClientÁ±ªÁöÑ`_process_stream_data`ÊñπÊ≥ï‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®Ë¢´Êî∂ÈõÜÂà∞`tool_calls_buffer`‰∏≠„ÄÇIDÊòØ‰ªé`tool_call[\\\"id\\\"]`Ëé∑ÂèñÁöÑ„ÄÇÂ¶ÇÊûúAPIËøîÂõûÁöÑID‰∏∫nullÔºåÈÇ£‰πàËÑöÊú¨‰∏≠Â≠òÂÇ®ÁöÑIDÂ∞±ÊòØnull„ÄÇ\\n\\nÂú®`_handle_tool_calls`ÊñπÊ≥ï‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®Ë¢´ÊâßË°åÔºåÁªìÊûúË¢´Ê∑ªÂä†Âà∞`metadata.tool_results`‰∏≠„ÄÇËøôÈáåÔºå`result`ÂåÖÂê´`\\\"tool_call_id\\\": tool_call[\\\"id\\\"]`ÔºåÂ¶ÇÊûúID‰∏∫nullÔºåÈÇ£‰πàÁªìÊûú‰∏≠ÁöÑID‰πüÊòØnull„ÄÇ\\n\\nÂΩì‰øùÂ≠ò‰∏∫JSONÊó∂ÔºåÂ∑•ÂÖ∑Ê∂àÊÅØÁöÑ`\\\"tool_call_id\\\"`‰∏∫null„ÄÇ\\n\\nÂΩìËøô‰∏™JSON‰Ωú‰∏∫‰∏ä‰∏ãÊñáÂä†ËΩΩÊó∂ÔºåAPIÂèØËÉΩÂõ†‰∏∫ID‰∏∫nullËÄåÊãíÁªù„ÄÇ\\n\\nËß£ÂÜ≥ÊñπÊ°àÔºöÁ°Æ‰øùÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏ç‰∏∫null„ÄÇÂ¶ÇÊûúAPIËøîÂõûnullÔºåÊàë‰ª¨ÂèØ‰ª•ÁîüÊàê‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑID„ÄÇÊàñËÄÖÔºåÂú®‰øùÂ≠ò‰∏ä‰∏ãÊñáÊó∂ÔºåÊàë‰ª¨ÂèØ‰ª•ÂøΩÁï•ID‰∏∫nullÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®Ôºü‰ΩÜËøôÊ†∑ÂèØËÉΩÁ†¥ÂùèÂØπËØùÁöÑËøûÁª≠ÊÄß„ÄÇ\\n\\nÊõ¥Â•ΩÁöÑÊñπÊ≥ïÊòØÔºöÂú®Â∑•ÂÖ∑Ë∞ÉÁî®ÊâßË°åÂêéÔºåÁ°Æ‰øùÁªìÊûú‰∏≠ÁöÑ`tool_call_id`‰∏éÂéüÂßãË∞ÉÁî®ÂåπÈÖç„ÄÇÂ¶ÇÊûúÂéüÂßãID‰∏∫nullÔºåÊàë‰ª¨ÂèØ‰ª•ÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫ID„ÄÇ\\n\\n‰ΩÜÊàë‰ª¨ÈúÄË¶Å‰øÆÊîπËÑöÊú¨„ÄÇÁî®Êà∑ÂèØËÉΩÂ∏åÊúõÊàëÊèê‰æõ‰∏Ä‰∏™‰øÆÂ§çÊñπÊ°à„ÄÇ\\n\\nËÆ©ÊàëÊ£ÄÊü•ËÑöÊú¨‰∏≠ÊòØÂê¶ÊúâÁîüÊàêIDÁöÑÈÄªËæë„ÄÇÂú®`ToolManager.execute_tool`‰∏≠ÔºåËøîÂõûÁöÑÂ≠óÂÖ∏ÂåÖÂê´`\\\"tool_call_id\\\": tool_call[\\\"id\\\"]`„ÄÇÂ¶ÇÊûú`tool_call[\\\"id\\\"]`‰∏∫nullÔºåÈÇ£‰πàÂ∞±ÊòØnull„ÄÇ\\n\\nÊàë‰ª¨ÂèØ‰ª•‰øÆÊîπËøôÈáåÔºöÂ¶ÇÊûúID‰∏∫nullÔºåÂàôÁîüÊàê‰∏Ä‰∏™UUID„ÄÇ\\n\\n‰ΩÜÈ¶ñÂÖàÔºåÊàë‰ª¨ÈúÄË¶Å‰∫ÜËß£API‰∏∫‰ΩïËøîÂõûnullÁöÑID„ÄÇÂèØËÉΩÊòØAPIÁöÑbugÔºåÊàñËÄÖÂìçÂ∫îÊ†ºÂºè‰∏çÊ≠£Á°Æ„ÄÇ\\n\\nÂè¶Â§ñÔºåÂú®JSON‰∏≠ÔºåÂ∑•ÂÖ∑Ë∞ÉÁî®ÊòØ`\\\"id\\\": null`ÔºåËøôÊòéÊòæÊòØnull„ÄÇ\\n\\nÂõ†Ê≠§ÔºåÂú®ËÑöÊú¨‰∏≠ÔºåÊàë‰ª¨Â∫îËØ•Âú®Â∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÊó∂ÁîüÊàê‰∏Ä‰∏™ID„ÄÇ\\n\\nËÆ©ÊàëÂú®`ToolManager.execute_tool`‰∏≠‰øÆÊîπÔºöÊ£ÄÊü•`tool_call[\\\"id\\\"]`ÔºåÂ¶ÇÊûú‰∏∫nullÊàñÁ©∫ÔºåÂàôÁîüÊàê‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑID„ÄÇ\\n\\n‰ΩÜÂ∑•ÂÖ∑Ë∞ÉÁî®IDÊòØAPIÊèê‰æõÁöÑÔºåÊàë‰ª¨Â∫îËØ•‰øùÊåÅ‰∏ÄËá¥ÊÄß„ÄÇ‰∏çËøáÔºå‰∏∫‰∫ÜÂÖºÂÆπÊÄßÔºåÊàë‰ª¨ÂèØ‰ª•ÁîüÊàê‰∏Ä‰∏™ID„ÄÇ\\n\\nÊ≠§Â§ñÔºåÂú®‰øùÂ≠ò‰∏ä‰∏ãÊñáÊó∂ÔºåÊàë‰ª¨ÂèØËÉΩÈúÄË¶ÅÁ°Æ‰øùID‰∏ç‰∏∫null„ÄÇ\\n\\nÁé∞Âú®ÔºåÁî®Êà∑ÂèØËÉΩÂ∏åÊúõÊàëÁõ¥Êé•‰øÆÊîπËÑöÊú¨Âπ∂Êèê‰æõ‰øÆÂ§çÂêéÁöÑÁâàÊú¨„ÄÇ\\n\\nËÆ©ÊàëÁºñÂÜô‰∏Ä‰∏™‰øÆÂ§çÊñπÊ°àÔºö\\n\\nÂú®`ToolManager.execute_tool`ÊñπÊ≥ï‰∏≠Ôºö\\n\\n```python\\ndef execute_tool(self, tool_call: Dict) -> Dict:\\n    try:\\n        function_name = tool_call[\\\"function\\\"][\\\"name\\\"]\\n        function_args = json.loads(tool_call[\\\"function\\\"][\\\"arguments\\\"])\\n        \\n        if function_name not in self.available_functions:\\n            raise ToolError(f\\\"Unknown function: {function_name}\\\")\\n        \\n        print(f\\\"\\\\n{Colors.CYAN}üîß Executing tool: {function_name}{Colors.END}\\\", file=sys.stderr)\\n        print(f\\\"   Arguments: {function_args}\\\", file=sys.stderr)\\n        \\n        # Execute the function\\n        result = self.available_functions[function_name](**function_args)\\n        \\n        print(f\\\"   Result: {result}\\\", file=sys.stderr)\\n        \\n        # Get tool call ID, generate if null or empty\\n        tool_call_id = tool_call.get(\\\"id\\\")\\n        if not tool_call_id:\\n            import uuid\\n            tool_call_id = str(uuid.uuid4())\\n            print(f\\\"{Colors.warn('‚ö†')} Generated tool call ID: {tool_call_id}\\\", file=sys.stderr)\\n        \\n        return {\\n            \\\"tool_call_id\\\": tool_call_id,\\n            \\\"role\\\": \\\"tool\\\",\\n            \\\"name\\\": function_name,\\n            \\\"content\\\": json.dumps(result, ensure_ascii=False)\\n        }\\n        \\n    except Exception as e:\\n        error_msg = f\\\"Tool execution failed: {str(e)}\\\"\\n        print(f\\\"{Colors.error('‚ùå')} {error_msg}\\\", file=sys.stderr)\\n        # Generate ID for error response as well\\n        tool_call_id = tool_call.get(\\\"id\\\")\\n        if not tool_call_id:\\n            import uuid\\n            tool_call_id = str(uuid.uuid4())\\n        return {\\n            \\\"tool_call_id\\\": tool_call_id,\\n            \\\"role\\\": \\\"tool\\\",\\n            \\\"name\\\": tool_call[\\\"function\\\"][\\\"name\\\"],\\n            \\\"content\\\": json.dumps({\\\"error\\\": error_msg}, ensure_ascii=False)\\n        }\\n```\\n\\nÊ≠§Â§ñÔºåÂú®`APIClient._handle_tool_calls`‰∏≠ÔºåÊàë‰ª¨ÂèØËÉΩËøòÈúÄË¶ÅÁ°Æ‰øùÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏ç‰∏∫nullÔºå‰ΩÜËøôÈáåÊàë‰ª¨Âè™ÊòØ‰º†ÈÄíÂéüÂßãÂ∑•ÂÖ∑Ë∞ÉÁî®„ÄÇ\\n\\nÂè¶Â§ñÔºåÂú®‰øùÂ≠òJSONÊó∂ÔºåÊàë‰ª¨ÂèØ‰ª•Ê£ÄÊü•Âπ∂‰øÆÂ§çID„ÄÇ\\n\\n‰ΩÜÊúÄÂ•ΩÂú®Ê∫êÂ§¥Ëß£ÂÜ≥ÔºöÂΩìAPIËøîÂõûnull IDÊó∂ÔºåÊàë‰ª¨ÁîüÊàê‰∏Ä‰∏™ID„ÄÇ\\n\\nÁÑ∂ËÄåÔºåÊàë‰ª¨Êó†Ê≥ïÊéßÂà∂APIÂìçÂ∫î„ÄÇ‰ΩÜÊàë‰ª¨ÂèØ‰ª•‰øÆÊîπËÑöÊú¨ÔºåÂú®Êé•Êî∂Âà∞Â∑•ÂÖ∑Ë∞ÉÁî®Êó∂Ê£ÄÊü•IDÔºåÂ¶ÇÊûú‰∏∫nullÔºåÂàôÁîüÊàê‰∏Ä‰∏™„ÄÇ\\n\\nÂú®`_process_stream_data`ÊñπÊ≥ï‰∏≠ÔºåÂΩìÂ§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®Êó∂ÔºåÊàë‰ª¨ÂèØ‰ª•Ê£ÄÊü•IDÂπ∂ÁîüÊàê‰∏Ä‰∏™„ÄÇ\\n\\n‰ΩÜËøôÊ†∑ÂèØËÉΩ‰ºö‰Ωø‰ª£Á†ÅÂ§çÊùÇÂåñ„ÄÇ‰∏Ä‰∏™Êõ¥ÁÆÄÂçïÁöÑÊñπÊ≥ïÊòØÂú®Â∑•ÂÖ∑ÊâßË°åÊó∂ÁîüÊàêIDÔºåÂ¶Ç‰∏äÊâÄËø∞„ÄÇ\\n\\n‰ΩÜÂ∑•ÂÖ∑Ë∞ÉÁî®IDÈúÄË¶Å‰∏éÂéüÂßãË∞ÉÁî®ÂåπÈÖçÔºåÂê¶ÂàôAPIÂèØËÉΩÊó†Ê≥ïÂÖ≥ËÅîÂ∑•ÂÖ∑ÂìçÂ∫î„ÄÇ‰∏çËøáÔºåÂ¶ÇÊûúÂéüÂßãID‰∏∫nullÔºåÈÇ£‰πàÁîüÊàê‰∏Ä‰∏™IDÂ∫îËØ•Ê≤°ÈóÆÈ¢òÔºåÂõ†‰∏∫APIÂèØËÉΩ‰∏çÂÖ≥ÂøÉID„ÄÇ\\n\\n‰ΩÜ‰∏∫‰∫ÜÂÆâÂÖ®Ëµ∑ËßÅÔºåÊàë‰ª¨Â∫îËØ•Âú®Â∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÊó∂ÁîüÊàê‰∏Ä‰∏™ÔºåÂπ∂Âú®Â∑•ÂÖ∑ÂìçÂ∫î‰∏≠‰ΩøÁî®Áõ∏ÂêåÁöÑID„ÄÇ\\n\\nÊ≠§Â§ñÔºåÂú®‰øùÂ≠ò‰∏ä‰∏ãÊñáÊó∂ÔºåÊàë‰ª¨Â∫îËØ•Á°Æ‰øùÂ∑•ÂÖ∑Ë∞ÉÁî®ÂíåÂ∑•ÂÖ∑ÂìçÂ∫î‰∏≠ÁöÑID‰∏ÄËá¥„ÄÇ\\n\\nÁé∞Âú®ÔºåÁî®Êà∑ËøòÊèêÂà∞‚ÄúËøîÂõûtool_calls stop reason‚ÄùÔºåËøôÊÑèÂë≥ÁùÄAPIÂú®Â∑•ÂÖ∑Ë∞ÉÁî®ÂêéÂÅúÊ≠¢‰∫ÜÔºå‰ΩÜÂΩìÊàë‰ª¨Â∞ÜÂÖ∂‰Ωú‰∏∫‰∏ä‰∏ãÊñáËæìÂÖ•Êó∂ÔºåÂá∫Áé∞‰∫Ü400ÈîôËØØ„ÄÇ\\n\\nËøôÂèØËÉΩÊòØÂõ†‰∏∫Ê∂àÊÅØÈ°∫Â∫èÊàñÊ†ºÂºèÈóÆÈ¢ò„ÄÇ\\n\\nËÆ©Êàë‰ª¨ÁúãÁúãJSON‰∏≠ÁöÑÊ∂àÊÅØÈ°∫Â∫èÔºö\\n\\n1. Áî®Êà∑Ê∂àÊÅØ\\n2. Âä©ÊâãÊ∂àÊÅØÔºàÂ∏¶ÊÄùËÄÉÂÜÖÂÆπÔºâ\\n3. Áî®Êà∑Ê∂àÊÅØ\\n4. Âä©ÊâãÊ∂àÊÅØÔºàÂ∏¶ÊÄùËÄÉÂÜÖÂÆπÔºâ\\n5. Áî®Êà∑Ê∂àÊÅØ\\n6. Âä©ÊâãÊ∂àÊÅØÔºàÂ∏¶ÊÄùËÄÉÂÜÖÂÆπÂíåÂ∑•ÂÖ∑Ë∞ÉÁî®Ôºâ\\n7. Â∑•ÂÖ∑Ê∂àÊÅØÔºàtool_call_id‰∏∫nullÔºâ\\n\\nÂΩìËøô‰∏™‰∏ä‰∏ãÊñáË¢´Âä†ËΩΩÊó∂ÔºåËÑöÊú¨‰ºöÂ∞ÜÂÖ∂‰Ωú‰∏∫Ê∂àÊÅØÂàóË°®ÂèëÈÄÅ„ÄÇAPIÂèØËÉΩÊúüÊúõÂ∑•ÂÖ∑Ê∂àÊÅØÂú®Âä©ÊâãÊ∂àÊÅØ‰πãÂêéÔºåÂπ∂‰∏îtool_call_id‰∏çËÉΩ‰∏∫null„ÄÇ\\n\\nÂõ†Ê≠§Ôºå‰øÆÂ§çÂ∑•ÂÖ∑Ë∞ÉÁî®IDÂèØËÉΩËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ\\n\\nÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÈúÄË¶ÅÁ°Æ‰øùÂ∑•ÂÖ∑Ê∂àÊÅØÁöÑÊ†ºÂºèÊ≠£Á°Æ„ÄÇÂú®OpenAI API‰∏≠ÔºåÂ∑•ÂÖ∑Ê∂àÊÅØÁöÑroleÊòØ‚Äútool‚ÄùÔºåÂπ∂‰∏îÂøÖÈ°ªÂåÖÂê´tool_call_id„ÄÇ\\n\\nÂõ†Ê≠§ÔºåÊàëÂª∫ËÆÆ‰øÆÊîπËÑöÊú¨ÔºåÂú®Â∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÊó∂ÁîüÊàê‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑID„ÄÇ\\n\\nÁé∞Âú®ÔºåÊàëÈúÄË¶ÅÂ∞Ü‰øÆÂ§çÊñπÊ°àÊèê‰æõÁªôÁî®Êà∑„ÄÇÊàëÂèØ‰ª•ËæìÂá∫‰øÆÊîπÂêéÁöÑËÑöÊú¨ÈÉ®ÂàÜÔºåÊàñËÄÖÊèê‰æõ‰∏Ä‰∏™Ë°•‰∏Å„ÄÇ\\n\\n‰ΩÜÁî®Êà∑ÂèØËÉΩÂ∏åÊúõÊàëÁõ¥Êé•‰øÆÊîπËÑöÊú¨Âπ∂ËøîÂõûÊï¥‰∏™Êñá‰ª∂„ÄÇÁÑ∂ËÄåÔºåËÑöÊú¨ÂæàÈïøÔºåÊàëÂèØ‰ª•Âè™Êèê‰æõ‰øÆÊîπÂêéÁöÑÈÉ®ÂàÜ„ÄÇ\\n\\nËÆ©ÊàëÂÖàËæìÂá∫‰øÆÊîπÂêéÁöÑToolManager.execute_toolÊñπÊ≥ï„ÄÇ\\n\\nÂè¶Â§ñÔºåÊàë‰ª¨ËøòÈúÄË¶ÅÂú®APIClient._process_stream_data‰∏≠Â§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®ID‰∏∫nullÁöÑÊÉÖÂÜµÔºå‰ΩÜ‰πüËÆ∏‰∏çÈúÄË¶Å„ÄÇ\\n\\nËÆ©Êàë‰ª¨ÂÖàÂÆûÁé∞‰øÆÂ§ç„ÄÇ\\n\\nÊàëÂ∞ÜÁºñÂÜô‰∏Ä‰∏™Ë°•‰∏ÅÔºåÂåÖÂê´ÂØπToolManager.execute_toolÁöÑ‰øÆÊîπ„ÄÇ\\n\\nÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÈúÄË¶ÅÂú®‰øùÂ≠ò‰∏ä‰∏ãÊñáÊó∂Á°Æ‰øùID‰∏ç‰∏∫nullÔºå‰ΩÜ‰øÆÂ§çÂ∑•ÂÖ∑ÊâßË°åÂêéÔºåIDÂ∫îËØ•Â∑≤ÁªèÂ≠òÂú®„ÄÇ\\n\\nÁé∞Âú®ÔºåÊàëÂ∞ÜÁîüÊàê‰∏Ä‰∏™ÂõûÁ≠îÔºåËß£ÈáäÈóÆÈ¢òÂπ∂Êèê‰æõ‰øÆÂ§çÊñπÊ°à„ÄÇ\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"```py\\n#!/usr/bin/env python3\\n\\\"\\\"\\\"\\nAdvanced Typewriter Effect HTTP Client for OpenAI-compatible APIs (Enhanced Version).\\nZero-default policy with graceful fallback - avoids crashes when encountering unsupported parameters.\\n\\\"\\\"\\\"\\n\\nimport argparse\\nimport base64\\nimport importlib.util\\nimport inspect\\nimport io\\nimport json\\nimport logging\\nimport mimetypes\\nimport os\\nimport re\\nimport signal\\nimport sys\\nimport time\\nfrom dataclasses import dataclass, field\\nfrom datetime import datetime\\nfrom pathlib import Path\\nfrom typing import Optional, Dict, List, Any, Tuple, Union, Callable\\n\\nimport requests\\nfrom dotenv import load_dotenv\\n\\n# ============ Setup & Constants ============\\n\\n# Configure logging\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format=\\\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\\\",\\n    stream=sys.stderr,\\n)\\nlogger = logging.getLogger(__name__)\\n\\nCODE_BLOCK = \\\"`\\\" * 3\\n\\n\\nclass Constants:\\n    \\\"\\\"\\\"Application constants\\\"\\\"\\\"\\n\\n    CHUNK_SIZE = 1024\\n    STREAM_DELAY_REASONING = 0.01\\n    STREAM_DELAY_CONTENT = 0.03\\n    TIMEOUT = 60 * 60 * 6  # ‰øùÊåÅ 1 Â∞èÊó∂Ë∂ÖÊó∂\\n\\n\\nclass Colors:\\n    \\\"\\\"\\\"ANSI Color codes for terminal output.\\\"\\\"\\\"\\n\\n    GREEN = \\\"\\\\033[92m\\\"\\n    YELLOW = \\\"\\\\033[93m\\\"\\n    RED = \\\"\\\\033[91m\\\"\\n    BLUE = \\\"\\\\033[94m\\\"\\n    CYAN = \\\"\\\\033[96m\\\"\\n    BOLD = \\\"\\\\033[1m\\\"\\n    END = \\\"\\\\033[0m\\\"\\n\\n    @classmethod\\n    def info(cls, text: str) -> str:\\n        return f\\\"{cls.GREEN}{text}{cls.END}\\\"\\n\\n    @classmethod\\n    def warn(cls, text: str) -> str:\\n        return f\\\"{cls.YELLOW}{text}{cls.END}\\\"\\n\\n    @classmethod\\n    def error(cls, text: str) -> str:\\n        return f\\\"{cls.RED}{text}{cls.END}\\\"\\n\\n    @classmethod\\n    def bold(cls, text: str) -> str:\\n        return f\\\"{cls.BOLD}{text}{cls.END}\\\"\\n\\n\\n# ============ Exceptions ============\\n\\n\\nclass ChatError(Exception):\\n    \\\"\\\"\\\"Base exception for chat errors\\\"\\\"\\\"\\n\\n    pass\\n\\n\\nclass ConfigurationError(ChatError):\\n    \\\"\\\"\\\"Missing required configuration\\\"\\\"\\\"\\n\\n    pass\\n\\n\\nclass APIError(ChatError):\\n    \\\"\\\"\\\"API communication error\\\"\\\"\\\"\\n\\n    pass\\n\\n\\nclass FileProcessingError(ChatError):\\n    \\\"\\\"\\\"File I/O error\\\"\\\"\\\"\\n\\n    pass\\n\\n\\nclass ToolError(ChatError):\\n    \\\"\\\"\\\"Tool execution error\\\"\\\"\\\"\\n\\n    pass\\n\\n\\n# ============ UTF-8 Setup ============\\n\\n\\ndef setup_utf8():\\n    \\\"\\\"\\\"Force UTF-8 encoding for Windows and Unix\\\"\\\"\\\"\\n    if sys.platform == \\\"win32\\\":\\n        import ctypes\\n\\n        try:\\n            kernel32 = ctypes.windll.kernel32\\n            kernel32.SetConsoleCP(65001)\\n            kernel32.SetConsoleOutputCP(65001)\\n        except Exception:\\n            pass\\n\\n    if hasattr(sys.stdout, \\\"buffer\\\"):\\n        sys.stdout = io.TextIOWrapper(\\n            sys.stdout.buffer, encoding=\\\"utf-8\\\", errors=\\\"replace\\\", line_buffering=True\\n        )\\n\\n\\n# ============ Data Models ============\\n\\n\\n@dataclass\\nclass APIConfig:\\n    \\\"\\\"\\\"API connection configuration - only endpoint and api_key\\\"\\\"\\\"\\n\\n    endpoint: str = \\\"\\\"\\n    api_key: str = \\\"\\\"\\n    timeout: int = Constants.TIMEOUT  # HTTP timeout, not API parameter\\n\\n    def validate(self) -> None:\\n        if not self.endpoint:\\n            raise ConfigurationError(\\\"API endpoint is required\\\")\\n        if not self.api_key:\\n            raise ConfigurationError(\\\"API key is required\\\")\\n\\n\\n@dataclass\\nclass RequestMetadata:\\n    \\\"\\\"\\\"Metadata for request tracking and reporting\\\"\\\"\\\"\\n\\n    start_time: datetime = field(default_factory=datetime.now)\\n    end_time: Optional[datetime] = None\\n    prompt_tokens: int = 0\\n    completion_tokens: int = 0\\n    total_tokens: int = 0\\n    finish_reason: Optional[str] = None\\n    model: Optional[str] = None\\n    duration: float = 0.0\\n    tool_calls: List[Dict] = field(default_factory=list)\\n    tool_results: List[Dict] = field(default_factory=list)\\n\\n    def finalize(self) -> None:\\n        self.end_time = datetime.now()\\n        self.duration = (self.end_time - self.start_time).total_seconds()\\n\\n    def get_token_rate(self) -> Optional[float]:\\n        \\\"\\\"\\\"Calculate tokens per second\\\"\\\"\\\"\\n        if self.duration > 0 and self.completion_tokens > 0:\\n            return self.completion_tokens / self.duration\\n        return None\\n\\n\\n# ============ Tool System ============\\n\\n\\nclass ToolManager:\\n    \\\"\\\"\\\"Manages tool functions and execution\\\"\\\"\\\"\\n\\n    def __init__(self, tools_module_path: str = \\\"tools.py\\\"):\\n        self.tools_module_path = tools_module_path\\n        self.available_functions: Dict[str, Callable] = {}\\n        self.tool_definitions: List[Dict] = []\\n        self._load_tools()\\n\\n    def _load_tools(self):\\n        \\\"\\\"\\\"Load tool functions from tools.py module\\\"\\\"\\\"\\n        try:\\n            spec = importlib.util.spec_from_file_location(\\n                \\\"tools\\\", self.tools_module_path\\n            )\\n            if spec and spec.loader:\\n                tools_module = importlib.util.module_from_spec(spec)\\n                spec.loader.exec_module(tools_module)\\n\\n                # Find all functions in the module\\n                for name, obj in inspect.getmembers(tools_module):\\n                    if inspect.isfunction(obj) and not name.startswith(\\\"_\\\"):\\n                        # Check if function has proper annotations for OpenAI tools\\n                        sig = inspect.signature(obj)\\n                        params = {}\\n                        for param_name, param in sig.parameters.items():\\n                            if param_name != \\\"self\\\":\\n                                param_type = (\\n                                    str(param.annotation)\\n                                    if param.annotation != inspect.Parameter.empty\\n                                    else \\\"string\\\"\\n                                )\\n                                params[param_name] = {\\n                                    \\\"type\\\": param_type.replace(\\\"<class '\\\", \\\"\\\")\\n                                    .replace(\\\"'>\\\", \\\"\\\")\\n                                    .lower(),\\n                                    \\\"description\\\": f\\\"Parameter {param_name}\\\",\\n                                }\\n\\n                        # Create tool definition\\n                        tool_def = {\\n                            \\\"type\\\": \\\"function\\\",\\n                            \\\"function\\\": {\\n                                \\\"name\\\": name,\\n                                \\\"description\\\": obj.__doc__ or f\\\"Function {name}\\\",\\n                                \\\"parameters\\\": {\\n                                    \\\"type\\\": \\\"object\\\",\\n                                    \\\"properties\\\": params,\\n                                    \\\"required\\\": list(params.keys()),\\n                                },\\n                            },\\n                        }\\n\\n                        self.tool_definitions.append(tool_def)\\n                        self.available_functions[name] = obj\\n\\n                        print(\\n                            f\\\"{Colors.info('‚úì')} Loaded tool: {name}\\\", file=sys.stderr\\n                        )\\n\\n                if not self.tool_definitions:\\n                    print(\\n                        f\\\"{Colors.warn('‚ö†')} No tools found in {self.tools_module_path}\\\",\\n                        file=sys.stderr,\\n                    )\\n            else:\\n                print(\\n                    f\\\"{Colors.warn('‚ö†')} Tools module not found: {self.tools_module_path}\\\",\\n                    file=sys.stderr,\\n                )\\n        except Exception as e:\\n            print(f\\\"{Colors.warn('‚ö†')} Failed to load tools: {e}\\\", file=sys.stderr)\\n\\n    def execute_tool(self, tool_call: Dict) -> Dict:\\n        \\\"\\\"\\\"Execute a tool call and return result\\\"\\\"\\\"\\n        try:\\n            function_name = tool_call[\\\"function\\\"][\\\"name\\\"]\\n            function_args = json.loads(tool_call[\\\"function\\\"][\\\"arguments\\\"])\\n\\n            if function_name not in self.available_functions:\\n                raise ToolError(f\\\"Unknown function: {function_name}\\\")\\n\\n            print(\\n                f\\\"\\\\n{Colors.CYAN}üîß Executing tool: {function_name}{Colors.END}\\\",\\n                file=sys.stderr,\\n            )\\n            print(f\\\"   Arguments: {function_args}\\\", file=sys.stderr)\\n\\n            # Execute the function\\n            result = self.available_functions[function_name](**function_args)\\n\\n            print(f\\\"   Result: {result}\\\", file=sys.stderr)\\n\\n            # ‰øÆÂ§çÔºöÂ§ÑÁêÜnullÊàñÁ©∫ÁöÑtool_call_id\\n            tool_call_id = tool_call.get(\\\"id\\\")\\n            if not tool_call_id:\\n                import uuid\\n\\n                tool_call_id = str(uuid.uuid4())\\n                print(\\n                    f\\\"{Colors.warn('‚ö†')} Generated tool call ID: {tool_call_id}\\\",\\n                    file=sys.stderr,\\n                )\\n\\n            return {\\n                \\\"tool_call_id\\\": tool_call_id,\\n                \\\"role\\\": \\\"tool\\\",\\n                \\\"name\\\": function_name,\\n                \\\"content\\\": json.dumps(result, ensure_ascii=False),\\n            }\\n\\n        except Exception as e:\\n            error_msg = f\\\"Tool execution failed: {str(e)}\\\"\\n            print(f\\\"{Colors.error('‚ùå')} {error_msg}\\\", file=sys.stderr)\\n\\n            # ‰øÆÂ§çÔºöÈîôËØØÂìçÂ∫î‰πüÁîüÊàêID\\n            tool_call_id = tool_call.get(\\\"id\\\")\\n            if not tool_call_id:\\n                import uuid\\n\\n                tool_call_id = str(uuid.uuid4())\\n\\n            return {\\n                \\\"tool_call_id\\\": tool_call_id,\\n                \\\"role\\\": \\\"tool\\\",\\n                \\\"name\\\": tool_call[\\\"function\\\"][\\\"name\\\"],\\n                \\\"content\\\": json.dumps({\\\"error\\\": error_msg}, ensure_ascii=False),\\n            }\\n\\n    def get_tool_definitions(self) -> List[Dict]:\\n        \\\"\\\"\\\"Get OpenAI-compatible tool definitions\\\"\\\"\\\"\\n        return self.tool_definitions\\n\\n\\n# ============ Configuration Management ============\\n\\n\\nclass ConfigManager:\\n    \\\"\\\"\\\"Manages configuration loading with graceful fallback\\\"\\\"\\\"\\n\\n    ENV_PATHS = [\\\".env\\\", \\\"../.env\\\", Path.home() / \\\".ai_chat.env\\\"]\\n    PROFILE_PATHS = [\\n        Path(\\\"profiles.json\\\"),\\n        Path(\\\"../profiles.json\\\"),\\n        Path.home() / \\\".ai_chat_profiles.json\\\",\\n    ]\\n\\n    @staticmethod\\n    def load_connection_config(args) -> APIConfig:\\n        \\\"\\\"\\\"Load endpoint and api_key with priority: CLI > Profile > Env > Default\\\"\\\"\\\"\\n        config = APIConfig()\\n\\n        # 1. Load .env files first\\n        for env_path in ConfigManager.ENV_PATHS:\\n            if Path(env_path).exists():\\n                load_dotenv(env_path)\\n                logger.info(f\\\"Loaded environment: {env_path}\\\")\\n                break\\n\\n        # 2. From environment variables\\n        config.endpoint = os.getenv(\\\"ENDPOINT\\\", \\\"\\\")\\n        config.api_key = os.getenv(\\\"API_KEY\\\", \\\"\\\")\\n\\n        # 3. Try to load from profile\\n        profile_config = ConfigManager._load_profile(args.profile)\\n        if profile_config:\\n            if \\\"endpoint\\\" in profile_config:\\n                config.endpoint = profile_config[\\\"endpoint\\\"]\\n            if \\\"api_key\\\" in profile_config:\\n                config.api_key = profile_config[\\\"api_key\\\"]\\n            if \\\"apiKey\\\" in profile_config:\\n                config.api_key = profile_config[\\\"apiKey\\\"]\\n\\n        # 4. CLI arguments override everything\\n        if hasattr(args, \\\"endpoint\\\") and args.endpoint:\\n            config.endpoint = args.endpoint\\n        if hasattr(args, \\\"api_key\\\") and args.api_key:\\n            config.api_key = args.api_key\\n\\n        return config\\n\\n    @staticmethod\\n    def _load_profile(profile_name: str) -> Optional[Dict[str, Any]]:\\n        \\\"\\\"\\\"Load profile from various locations\\\"\\\"\\\"\\n        for path in ConfigManager.PROFILE_PATHS:\\n            if path.exists():\\n                try:\\n                    with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n                        profiles = json.load(f)\\n\\n                    if profile_name in profiles:\\n                        print(\\n                            f\\\"{Colors.info('‚úì')} Loaded profile: {profile_name} from {path}\\\",\\n                            file=sys.stderr,\\n                        )\\n                        return profiles[profile_name]\\n                except Exception as e:\\n                    logger.error(f\\\"Failed to load profile {path}: {e}\\\")\\n\\n        if profile_name != \\\"default\\\":\\n            print(\\n                f\\\"{Colors.warn('‚ö†')} Profile '{profile_name}' not found, using default\\\",\\n                file=sys.stderr,\\n            )\\n        return None\\n\\n    @staticmethod\\n    def load_request_body(config_path: str, args) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Load request body from config.json with safe parameter handling\\\"\\\"\\\"\\n        path = Path(config_path).expanduser().resolve()\\n\\n        if not path.exists():\\n            if (\\n                config_path != \\\"config.json\\\"\\n            ):  # User explicitly specified non-existent file\\n                raise FileNotFoundError(f\\\"Config file not found: {config_path}\\\")\\n            # Use empty config if default config.json doesn't exist\\n            return {}\\n\\n        try:\\n            with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n                data = json.load(f)\\n\\n            print(f\\\"{Colors.info('‚úì')} Loaded request config: {path}\\\", file=sys.stderr)\\n\\n            # Apply CLI overrides for safety-critical parameters\\n            ConfigManager._apply_cli_overrides(data, args)\\n\\n            return data\\n        except Exception as e:\\n            raise ValueError(f\\\"Failed to load config file ({config_path}): {e}\\\")\\n\\n    @staticmethod\\n    def _apply_cli_overrides(data: Dict[str, Any], args) -> None:\\n        \\\"\\\"\\\"Apply CLI arguments to request body, removing unsupported parameters\\\"\\\"\\\"\\n        # Safety: Remove problematic parameters if CLI provides alternatives\\n        if hasattr(args, \\\"model\\\") and args.model:\\n            data[\\\"model\\\"] = args.model\\n            # Remove any model-specific parameters that might conflict\\n            data.pop(\\\"temperature\\\", None)  # Some models don't support temperature\\n            data.pop(\\\"top_p\\\", None)\\n            data.pop(\\\"frequency_penalty\\\", None)\\n            data.pop(\\\"presence_penalty\\\", None)\\n\\n        # OpenAI standard parameters override\\n        if hasattr(args, \\\"temperature\\\") and args.temperature is not None:\\n            data[\\\"temperature\\\"] = args.temperature\\n        if hasattr(args, \\\"max_tokens\\\") and args.max_tokens is not None:\\n            data[\\\"max_tokens\\\"] = args.max_tokens\\n        if hasattr(args, \\\"top_p\\\") and args.top_p is not None:\\n            data[\\\"top_p\\\"] = args.top_p\\n        if hasattr(args, \\\"presence_penalty\\\") and args.presence_penalty is not None:\\n            data[\\\"presence_penalty\\\"] = args.presence_penalty\\n        if hasattr(args, \\\"frequency_penalty\\\") and args.frequency_penalty is not None:\\n            data[\\\"frequency_penalty\\\"] = args.frequency_penalty\\n        if hasattr(args, \\\"seed\\\") and args.seed is not None:\\n            data[\\\"seed\\\"] = args.seed\\n\\n        # Stream handling\\n        if hasattr(args, \\\"no_stream\\\") and args.no_stream:\\n            data[\\\"stream\\\"] = False\\n        elif \\\"stream\\\" not in data:\\n            data[\\\"stream\\\"] = True  # Default to stream for better UX\\n\\n        # Thinking enable/disable\\n        if hasattr(args, \\\"enable_thinking\\\"):\\n            if args.enable_thinking is not None:\\n                data[\\\"enable_thinking\\\"] = args.enable_thinking\\n                # Some APIs don't support reasoning_content, remove if disabled\\n                if not args.enable_thinking:\\n                    data.pop(\\\"reasoning_content\\\", None)\\n\\n    @staticmethod\\n    def save_context_to_config(config_path: str, messages: List[Dict]) -> None:\\n        \\\"\\\"\\\"Save conversation context back to config.json\\\"\\\"\\\"\\n        try:\\n            path = Path(config_path).expanduser().resolve()\\n\\n            # Load existing config\\n            if path.exists():\\n                with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n                    config_data = json.load(f)\\n            else:\\n                config_data = {}\\n\\n            # Update messages\\n            config_data[\\\"messages\\\"] = messages\\n\\n            # Save back\\n            with open(path, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n                json.dump(config_data, f, ensure_ascii=False, indent=2)\\n\\n            print(f\\\"{Colors.info('‚úì')} Saved context to: {path}\\\", file=sys.stderr)\\n\\n        except Exception as e:\\n            print(f\\\"{Colors.warn('‚ö†')} Failed to save context: {e}\\\", file=sys.stderr)\\n\\n\\n# ============ File & Message Building ============\\n\\n\\nclass MessageBuilder:\\n    \\\"\\\"\\\"Builds messages with file context support\\\"\\\"\\\"\\n\\n    IMAGE_TYPES = {\\\".png\\\", \\\".jpg\\\", \\\".jpeg\\\", \\\".gif\\\", \\\".webp\\\", \\\".bmp\\\"}\\n\\n    @staticmethod\\n    def load_file(filepath: str) -> Union[str, Dict[str, Any]]:\\n        \\\"\\\"\\\"Load file content with MIME type detection\\\"\\\"\\\"\\n        if filepath.startswith(\\\"@\\\"):\\n            filepath = filepath[1:]\\n\\n        path = Path(filepath).expanduser().resolve()\\n        if not path.exists():\\n            raise FileNotFoundError(f\\\"File not found: {filepath}\\\")\\n\\n        mime_type, _ = mimetypes.guess_type(path)\\n        if not mime_type:\\n            mime_type = \\\"application/octet-stream\\\"\\n\\n        # Image handling\\n        if (\\n            mime_type.startswith(\\\"image/\\\")\\n            or path.suffix.lower() in MessageBuilder.IMAGE_TYPES\\n        ):\\n            with open(path, \\\"rb\\\") as f:\\n                encoded = base64.b64encode(f.read()).decode(\\\"utf-8\\\")\\n            return {\\n                \\\"type\\\": \\\"image_url\\\",\\n                \\\"image_url\\\": {\\\"url\\\": f\\\"data:{mime_type};base64,{encoded}\\\"},\\n            }\\n\\n        # Text file handling with encoding detection\\n        encodings = [\\\"utf-8\\\", \\\"utf-8-sig\\\", \\\"gbk\\\", \\\"latin-1\\\"]\\n        content = None\\n\\n        for enc in encodings:\\n            try:\\n                with open(path, \\\"r\\\", encoding=enc, errors=\\\"strict\\\") as f:\\n                    content = f.read()\\n                break\\n            except UnicodeDecodeError:\\n                continue\\n\\n        if content is None:\\n            with open(path, \\\"r\\\", encoding=\\\"utf-8\\\", errors=\\\"replace\\\") as f:\\n                content = f.read()\\n\\n        # Format based on file type\\n        suffix = path.suffix[1:] if path.suffix else \\\"text\\\"\\n        if path.suffix in [\\\".md\\\", \\\".txt\\\", \\\".rst\\\"]:\\n            return f\\\"{content}\\\\n\\\\n[File: {path.name}]\\\"\\n        else:\\n            return f\\\"{CODE_BLOCK}{suffix}\\\\n{content}\\\\n{CODE_BLOCK}\\\\n[File: {path.name}]\\\"\\n\\n    @staticmethod\\n    def build_messages(args, config_data: Dict | None = None) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Build final message list with priority: context > config > prompt\\\"\\\"\\\"\\n        messages = []\\n\\n        # Extract system prompt from config if exists\\n        system_prompt = None\\n        config_messages = []\\n        if config_data:\\n            system_prompt = config_data.pop(\\\"system\\\", None)\\n            config_messages = config_data.pop(\\\"messages\\\", [])\\n\\n        # 1. Add system prompt if exists (always first)\\n        if system_prompt:\\n            messages.append({\\\"role\\\": \\\"system\\\", \\\"content\\\": system_prompt})\\n            print(f\\\"{Colors.info('‚úì')} Added system prompt\\\", file=sys.stderr)\\n\\n        # 2. Load context if provided\\n        if hasattr(args, \\\"context\\\") and args.context:\\n            try:\\n                context_messages = MessageBuilder._load_context(args.context)\\n                messages.extend(context_messages)\\n                print(\\n                    f\\\"{Colors.info('‚úì')} Loaded conversation context: {args.context}\\\",\\n                    file=sys.stderr,\\n                )\\n            except Exception as e:\\n                print(\\n                    f\\\"{Colors.warn('‚ö†')} Failed to load context: {e}\\\", file=sys.stderr\\n                )\\n\\n        # 3. Add messages from config\\n        if config_messages:\\n            messages.extend(config_messages)\\n\\n        # 4. Add current user input\\n        if hasattr(args, \\\"prompt\\\") and args.prompt:\\n            user_messages = MessageBuilder._build_user_messages(args.prompt)\\n            messages.extend(user_messages)\\n\\n        if not messages and not system_prompt:\\n            raise ConfigurationError(\\n                \\\"No messages to send. Please provide prompt or messages in config.\\\"\\n            )\\n\\n        return messages\\n\\n    @staticmethod\\n    def _load_context(context_path: str) -> List[Dict]:\\n        \\\"\\\"\\\"Load conversation history from JSON file\\\"\\\"\\\"\\n        path = Path(context_path).expanduser().resolve()\\n        if not path.exists():\\n            raise FileNotFoundError(f\\\"Context file not found: {context_path}\\\")\\n\\n        with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n            data = json.load(f)\\n\\n        if isinstance(data, list):\\n            return data\\n        elif isinstance(data, dict) and \\\"messages\\\" in data:\\n            return data[\\\"messages\\\"]\\n        else:\\n            raise ValueError(\\n                \\\"Invalid context format: expected list or {messages: [...]}\\\"\\n            )\\n\\n    @staticmethod\\n    def _build_user_messages(prompt_parts: List[str]) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Build user messages from prompt parts\\\"\\\"\\\"\\n        if not prompt_parts:\\n            return []\\n\\n        content_parts = []\\n        has_image = False\\n\\n        for part in prompt_parts:\\n            if part.startswith(\\\"@\\\"):\\n                try:\\n                    file_content = MessageBuilder.load_file(part)\\n                    if isinstance(file_content, dict):\\n                        has_image = True\\n                        print(\\n                            f\\\"{Colors.info('‚úì')} Attached image: {part[1:]}\\\",\\n                            file=sys.stderr,\\n                        )\\n                    else:\\n                        print(\\n                            f\\\"{Colors.info('‚úì')} Attached file: {part[1:]}\\\",\\n                            file=sys.stderr,\\n                        )\\n                    content_parts.append(file_content)\\n                except Exception as e:\\n                    print(\\n                        f\\\"{Colors.warn('‚ö†')} Failed to load file {part}: {e}\\\",\\n                        file=sys.stderr,\\n                    )\\n                    content_parts.append(f\\\"[Failed to load file: {part}]\\\")\\n            else:\\n                content_parts.append(part)\\n\\n        # Build message content\\n        if has_image:\\n            multimodal_content = []\\n            current_text = []\\n\\n            for part in content_parts:\\n                if isinstance(part, dict):\\n                    if current_text:\\n                        multimodal_content.append(\\n                            {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n\\\".join(current_text)}\\n                        )\\n                        current_text = []\\n                    multimodal_content.append(part)\\n                else:\\n                    current_text.append(part)\\n\\n            if current_text:\\n                multimodal_content.append(\\n                    {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n\\\\n\\\".join(current_text)}\\n                )\\n\\n            return [{\\\"role\\\": \\\"user\\\", \\\"content\\\": multimodal_content}]\\n        else:\\n            user_content = \\\"\\\\n\\\\n\\\".join(content_parts)\\n            return [{\\\"role\\\": \\\"user\\\", \\\"content\\\": user_content}]\\n\\n\\n# ============ Typewriter Output ============\\n\\n\\nclass TypewriterPrinter:\\n    \\\"\\\"\\\"Typewriter effect printer with file output\\\"\\\"\\\"\\n\\n    def __init__(self, output_file: Optional[io.TextIOWrapper] = None):\\n        self.output_file = output_file\\n        self.reasoning_buffer = \\\"\\\"\\n        self.content_buffer = \\\"\\\"\\n        self.reasoning_printed = 0\\n        self.content_printed = 0\\n        self.in_reasoning = True\\n        self.start_time = time.time()\\n        self.first_token_time = None\\n        self.token_count = 0\\n\\n        # Performance tracking\\n        self.reasoning_start = None\\n        self.content_start = None\\n\\n    def write(self, text: str, is_reasoning: bool = False):\\n        \\\"\\\"\\\"Write text to file\\\"\\\"\\\"\\n        if self.output_file:\\n            try:\\n                self.output_file.write(text)\\n                if not is_reasoning:\\n                    self.output_file.flush()\\n            except Exception as e:\\n                print(f\\\"\\\\n{Colors.error('[File write error]')} {e}\\\", file=sys.stderr)\\n\\n    def update_reasoning(self, delta: str):\\n        \\\"\\\"\\\"Update and print reasoning content\\\"\\\"\\\"\\n        if not delta:\\n            return\\n\\n        if self.first_token_time is None:\\n            self.first_token_time = time.time()\\n\\n        if self.reasoning_start is None:\\n            self.reasoning_start = time.time()\\n            print(f\\\"\\\\n{Colors.BLUE}üí≠ Thinking:{Colors.END}\\\")\\n            if self.output_file:\\n                self.write(\\\"\\\\nüí≠ Thinking:\\\\n\\\", is_reasoning=True)\\n\\n        self.reasoning_buffer += delta\\n        new_text = self.reasoning_buffer[self.reasoning_printed :]\\n\\n        # Print to console with typewriter effect\\n        for char in new_text:\\n            print(char, end=\\\"\\\", flush=True)\\n            time.sleep(Constants.STREAM_DELAY_REASONING)\\n\\n        # Write to file\\n        self.write(new_text, is_reasoning=True)\\n\\n        self.reasoning_printed = len(self.reasoning_buffer)\\n        self.token_count += len(delta.split())  # Rough estimate\\n\\n    def switch_to_content(self):\\n        \\\"\\\"\\\"Switch from reasoning to content output\\\"\\\"\\\"\\n        if self.in_reasoning:\\n            self.in_reasoning = False\\n            self.content_start = time.time()\\n\\n            # Console output\\n            print(f\\\"\\\\n{Colors.bold('='*50)}\\\")\\n            print(f\\\"{Colors.CYAN}‚ú® Response:{Colors.END}\\\")\\n            print(f\\\"{Colors.bold('='*50)}\\\")\\n\\n            # File output\\n            if self.output_file:\\n                self.write(f\\\"\\\\n\\\\n{'='*50}\\\\n‚ú® Response:\\\\n{'='*50}\\\\n\\\\n\\\")\\n\\n    def update_content(self, delta: str):\\n        \\\"\\\"\\\"Update and print response content\\\"\\\"\\\"\\n        if not delta:\\n            return\\n\\n        if self.in_reasoning:\\n            self.switch_to_content()\\n\\n        if self.first_token_time is None:\\n            self.first_token_time = time.time()\\n\\n        self.content_buffer += delta\\n        new_text = self.content_buffer[self.content_printed :]\\n\\n        # Typewriter effect\\n        for char in new_text:\\n            print(char, end=\\\"\\\", flush=True)\\n            time.sleep(Constants.STREAM_DELAY_CONTENT)\\n\\n        self.write(new_text)\\n\\n        self.content_printed = len(self.content_buffer)\\n        self.token_count += len(delta.split())  # Rough estimate\\n\\n    def finalize(self):\\n        \\\"\\\"\\\"Finalize output\\\"\\\"\\\"\\n        if self.output_file:\\n            try:\\n                self.output_file.flush()\\n            except Exception as e:\\n                print(f\\\"{Colors.warn('‚ö†')} Failed to flush file: {e}\\\", file=sys.stderr)\\n\\n\\n# ============ HTTP Client ============\\n\\n\\nclass APIClient:\\n    \\\"\\\"\\\"HTTP client with retry and error handling using Session\\\"\\\"\\\"\\n\\n    def __init__(self, config: APIConfig, max_retries: int = 3):\\n        self.config = config\\n        self.max_retries = max_retries\\n        self.session = requests.Session()\\n        self.session.headers.update(\\n            {\\n                \\\"Authorization\\\": f\\\"Bearer {config.api_key}\\\",\\n                \\\"Content-Type\\\": \\\"application/json\\\",\\n                \\\"Accept\\\": \\\"text/event-stream\\\",\\n            }\\n        )\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.session.close()\\n\\n    def _estimate_tokens(self, messages: List[Dict]) -> Tuple[int, int]:\\n        \\\"\\\"\\\"‰º∞ÁÆó Prompt ÁöÑ Token Êï∞Ôºà‰øùÂÆà‰º∞ËÆ°ÔºöÊØè 3 ‰∏™Â≠óÁ¨¶ 1 ‰∏™ TokenÔºâ\\\"\\\"\\\"\\n        try:\\n            # Â∞Ü messages ËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤ËÆ°ÁÆó\\n            text = json.dumps(messages, ensure_ascii=False)\\n            char_count = len(text)\\n            # ‰øùÂÆà‰º∞ËÆ°ÔºöËã±ÊñáÁ∫¶ 4 Â≠óÁ¨¶/tokenÔºå‰∏≠ÊñáÁ∫¶ 1-2 Â≠óÁ¨¶/tokenÔºåÊ∑∑ÂêàÊåâ 3 ËÆ°ÁÆó\\n            estimated_tokens = char_count // 3\\n            return char_count, estimated_tokens\\n        except:\\n            return 0, 0\\n\\n    def request(\\n        self,\\n        payload: Dict[str, Any],\\n        printer: TypewriterPrinter,\\n        tool_manager: Optional[ToolManager] = None,\\n    ) -> RequestMetadata:\\n        \\\"\\\"\\\"Execute API request with retry logic\\\"\\\"\\\"\\n        metadata = RequestMetadata(model=payload.get(\\\"model\\\"))\\n        is_stream = payload.get(\\\"stream\\\", True)\\n\\n        # ËÆ°ÁÆó Prompt ‰ø°ÊÅØ\\n        messages = payload.get(\\\"messages\\\", [])\\n        char_count, estimated_tokens = self._estimate_tokens(messages)\\n        max_tokens = payload.get(\\\"max_tokens\\\", \\\"Not set\\\")\\n\\n        # ÊòæÁ§∫ËØ∑Ê±Ç‰ø°ÊÅØÔºåÂåÖÊã¨ Token È¢Ñ‰º∞\\n        print(\\n            f\\\"\\\\n{Colors.CYAN}üöÄ Requesting {metadata.model or 'unknown model'}...{Colors.END}\\\"\\n        )\\n        print(f\\\"   Endpoint: {self.config.endpoint}\\\")\\n        print(f\\\"   Stream: {'Yes' if is_stream else 'No'}\\\")\\n        print(f\\\"   Prompt Size: {char_count} chars (Est. ~{estimated_tokens} tokens)\\\")\\n\\n        if max_tokens != \\\"Not set\\\":\\n            print(f\\\"   Max Tokens: {max_tokens}\\\")\\n        else:\\n            print(f\\\"   Max Tokens: {Colors.warn('Not set')} (model default)\\\")\\n\\n        # Safety: Remove any empty or None parameters that might cause API errors\\n        clean_payload = self._clean_payload(payload)\\n\\n        for attempt in range(self.max_retries + 1):\\n            try:\\n                response = self.session.post(\\n                    self.config.endpoint,\\n                    json=clean_payload,\\n                    stream=is_stream,\\n                    timeout=self.config.timeout,\\n                )\\n                response.raise_for_status()\\n\\n                if is_stream:\\n                    return self._handle_stream(\\n                        response, printer, metadata, tool_manager\\n                    )\\n                else:\\n                    return self._handle_non_stream(\\n                        response, printer, metadata, tool_manager\\n                    )\\n\\n            except requests.exceptions.RequestException as e:\\n                if attempt < self.max_retries:\\n                    wait_time = 2**attempt\\n                    print(\\n                        f\\\"{Colors.warn(f'‚ö†Ô∏è  Request failed ({e}), retrying in {wait_time}s...')}\\\",\\n                        file=sys.stderr,\\n                    )\\n                    time.sleep(wait_time)\\n                else:\\n                    raise APIError(\\n                        f\\\"Request failed after {self.max_retries} retries: {e}\\\"\\n                    )\\n\\n        raise APIError(\\\"Max retries exceeded\\\")\\n\\n    def _clean_payload(self, payload: Dict[str, Any]) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Remove potentially problematic parameters\\\"\\\"\\\"\\n        clean = payload.copy()\\n\\n        # Remove None values\\n        for key in list(clean.keys()):\\n            if clean[key] is None:\\n                del clean[key]\\n\\n        # Remove empty strings\\n        for key in list(clean.keys()):\\n            if isinstance(clean[key], str) and not clean[key].strip():\\n                del clean[key]\\n\\n        return clean\\n\\n    def _handle_stream(\\n        self,\\n        response,\\n        printer: TypewriterPrinter,\\n        metadata: RequestMetadata,\\n        tool_manager: Optional[ToolManager],\\n    ) -> RequestMetadata:\\n        \\\"\\\"\\\"Handle streaming response\\\"\\\"\\\"\\n        buffer = \\\"\\\"\\n        tool_calls_buffer = {}\\n\\n        for chunk in response.iter_content(chunk_size=Constants.CHUNK_SIZE):\\n            if not chunk:\\n                continue\\n\\n            buffer += chunk.decode(\\\"utf-8\\\", errors=\\\"replace\\\")\\n\\n            while \\\"\\\\n\\\" in buffer:\\n                line, buffer = buffer.split(\\\"\\\\n\\\", 1)\\n                line = line.strip()\\n\\n                if not line.startswith(\\\"data: \\\"):\\n                    continue\\n\\n                data_str = line[6:]\\n                if data_str == \\\"[DONE]\\\":\\n                    break\\n\\n                try:\\n                    data = json.loads(data_str)\\n                    self._process_stream_data(\\n                        data, printer, metadata, tool_calls_buffer\\n                    )\\n                except json.JSONDecodeError:\\n                    continue\\n\\n        # Handle tool calls after stream ends\\n        if tool_calls_buffer and tool_manager:\\n            self._handle_tool_calls(tool_calls_buffer, metadata, tool_manager)\\n\\n        metadata.finalize()\\n        return metadata\\n\\n    def _process_stream_data(\\n        self,\\n        data: Dict,\\n        printer: TypewriterPrinter,\\n        metadata: RequestMetadata,\\n        tool_calls_buffer: Dict,\\n    ):\\n        \\\"\\\"\\\"Process individual stream data chunk\\\"\\\"\\\"\\n        # Update usage\\n        if \\\"usage\\\" in data and data[\\\"usage\\\"]:\\n            metadata.prompt_tokens = data[\\\"usage\\\"].get(\\\"prompt_tokens\\\", 0)\\n            metadata.completion_tokens = data[\\\"usage\\\"].get(\\\"completion_tokens\\\", 0)\\n            metadata.total_tokens = data[\\\"usage\\\"].get(\\\"total_tokens\\\", 0)\\n\\n        choices = data.get(\\\"choices\\\", [])\\n        if not choices:\\n            return\\n\\n        choice = choices[0]\\n        delta = choice.get(\\\"delta\\\", {})\\n\\n        # Update finish reason\\n        finish_reason = choice.get(\\\"finish_reason\\\")\\n        if finish_reason:\\n            metadata.finish_reason = finish_reason\\n\\n        # Process tool calls\\n        tool_calls = delta.get(\\\"tool_calls\\\", [])\\n        for tool_call in tool_calls:\\n            idx = tool_call.get(\\\"index\\\", 0)\\n            if idx not in tool_calls_buffer:\\n                tool_calls_buffer[idx] = {\\n                    \\\"id\\\": \\\"\\\",\\n                    \\\"type\\\": \\\"function\\\",\\n                    \\\"function\\\": {\\\"name\\\": \\\"\\\", \\\"arguments\\\": \\\"\\\"},\\n                }\\n\\n            if \\\"id\\\" in tool_call:\\n                tool_calls_buffer[idx][\\\"id\\\"] = tool_call[\\\"id\\\"]\\n            if \\\"function\\\" in tool_call:\\n                if \\\"name\\\" in tool_call[\\\"function\\\"]:\\n                    tool_calls_buffer[idx][\\\"function\\\"][\\\"name\\\"] = tool_call[\\\"function\\\"][\\n                        \\\"name\\\"\\n                    ]\\n                if \\\"arguments\\\" in tool_call[\\\"function\\\"]:\\n                    tool_calls_buffer[idx][\\\"function\\\"][\\\"arguments\\\"] += tool_call[\\n                        \\\"function\\\"\\n                    ][\\\"arguments\\\"]\\n\\n        # Process content\\n        reasoning_delta = delta.get(\\\"reasoning_content\\\") or delta.get(\\\"reasoning\\\") or \\\"\\\"\\n        content_delta = delta.get(\\\"content\\\", \\\"\\\")\\n\\n        if reasoning_delta:\\n            printer.update_reasoning(reasoning_delta)\\n\\n        if content_delta:\\n            printer.update_content(content_delta)\\n\\n    def _handle_non_stream(\\n        self,\\n        response,\\n        printer: TypewriterPrinter,\\n        metadata: RequestMetadata,\\n        tool_manager: Optional[ToolManager],\\n    ) -> RequestMetadata:\\n        \\\"\\\"\\\"Handle non-streaming response\\\"\\\"\\\"\\n        data = response.json()\\n        choice = data.get(\\\"choices\\\", [{}])[0]\\n        message = choice.get(\\\"message\\\", {})\\n\\n        reasoning = message.get(\\\"reasoning_content\\\", \\\"\\\")\\n        content = message.get(\\\"content\\\", \\\"\\\")\\n\\n        if reasoning:\\n            printer.update_reasoning(reasoning)\\n\\n        printer.switch_to_content()\\n        printer.update_content(content)\\n\\n        # Handle tool calls\\n        tool_calls = message.get(\\\"tool_calls\\\", [])\\n        if tool_calls and tool_manager:\\n            self._handle_tool_calls(tool_calls, metadata, tool_manager)\\n\\n        # Update metadata\\n        if \\\"usage\\\" in data:\\n            metadata.prompt_tokens = data[\\\"usage\\\"].get(\\\"prompt_tokens\\\", 0)\\n            metadata.completion_tokens = data[\\\"usage\\\"].get(\\\"completion_tokens\\\", 0)\\n            metadata.total_tokens = data[\\\"usage\\\"].get(\\\"total_tokens\\\", 0)\\n\\n        metadata.finish_reason = choice.get(\\\"finish_reason\\\")\\n        metadata.finalize()\\n\\n        return metadata\\n\\n    def _handle_tool_calls(\\n        self,\\n        tool_calls: Union[List, Dict],\\n        metadata: RequestMetadata,\\n        tool_manager: ToolManager,\\n    ):\\n        \\\"\\\"\\\"Handle tool calls and execute them\\\"\\\"\\\"\\n        if isinstance(tool_calls, dict):\\n            # Convert dict to list\\n            tool_calls_list = []\\n            for idx in sorted(tool_calls.keys()):\\n                tool_calls_list.append(tool_calls[idx])\\n            tool_calls = tool_calls_list\\n\\n        metadata.tool_calls = tool_calls\\n\\n        print(f\\\"\\\\n{Colors.CYAN}üõ†Ô∏è  Tool Calls Detected:{Colors.END}\\\")\\n        for i, tool_call in enumerate(tool_calls, 1):\\n            print(\\n                f\\\"   {i}. {tool_call['function']['name']}({tool_call['function']['arguments']})\\\"\\n            )\\n\\n        # Execute tool calls\\n        for tool_call in tool_calls:\\n            result = tool_manager.execute_tool(tool_call)\\n            metadata.tool_results.append(result)\\n\\n\\n# ============ Result Saving ============\\n\\n\\nclass ResultSaver:\\n    \\\"\\\"\\\"Saves conversation results to files\\\"\\\"\\\"\\n\\n    @staticmethod\\n    def save(\\n        base_path: Path,\\n        payload: Dict[str, Any],\\n        metadata: RequestMetadata,\\n        printer: TypewriterPrinter,\\n    ) -> Tuple[Path, Path]:\\n        \\\"\\\"\\\"Save results as Markdown and JSON\\\"\\\"\\\"\\n        # Create directory if needed\\n        base_path.parent.mkdir(parents=True, exist_ok=True)\\n\\n        # 1. Save Markdown\\n        md_path = ResultSaver._save_markdown(base_path, payload, metadata, printer)\\n\\n        # 2. Save JSON for continuation\\n        json_path = ResultSaver._save_json(base_path, payload, metadata, printer)\\n\\n        return md_path, json_path\\n\\n    @staticmethod\\n    def _save_markdown(\\n        base_path: Path,\\n        payload: Dict[str, Any],\\n        metadata: RequestMetadata,\\n        printer: TypewriterPrinter,\\n    ) -> Path:\\n        \\\"\\\"\\\"Save conversation as Markdown with full history\\\"\\\"\\\"\\n        md_path = base_path.with_suffix(\\\".md\\\")\\n\\n        # Prepare payload for display (hide messages to save space)\\n        display_payload = payload.copy()\\n        messages = display_payload.pop(\\\"messages\\\", [])\\n        if messages:\\n            display_payload[\\\"messages\\\"] = f\\\"[{len(messages)} messages hidden]\\\"\\n\\n        # Build Markdown content\\n        md_content = f\\\"\\\"\\\"# AI Conversation Log\\n**Time**: {metadata.start_time.strftime('%Y-%m-%d %H:%M:%S')}  \\n**Model**: {metadata.model or 'Unknown'}  \\n**Duration**: {metadata.duration:.2f}s  \\n\\n## Request Configuration\\n{CODE_BLOCK}json\\n{json.dumps(display_payload, indent=2, ensure_ascii=False)}\\n{CODE_BLOCK}\\n\\n## Conversation History\\n\\n\\\"\\\"\\\"\\n\\n        # Add all messages\\n        for msg in messages:\\n            role = msg.get(\\\"role\\\", \\\"unknown\\\").upper()\\n            content = msg.get(\\\"content\\\", \\\"\\\")\\n\\n            # Handle multimodal content\\n            if isinstance(content, list):\\n                texts = []\\n                for item in content:\\n                    if isinstance(item, dict):\\n                        if item.get(\\\"type\\\") == \\\"text\\\":\\n                            texts.append(item.get(\\\"text\\\", \\\"\\\"))\\n                        elif item.get(\\\"type\\\") == \\\"image_url\\\":\\n                            texts.append(\\\"[Image]\\\")\\n                content = \\\"\\\\n\\\".join(texts)\\n\\n            md_content += f\\\"### {role}\\\\n\\\\n{content}\\\\n\\\\n\\\"\\n\\n        # Add reasoning if present\\n        if printer.reasoning_buffer:\\n            md_content += f\\\"\\\"\\\"\\n---\\n\\n## üí≠ Thinking Process\\n<details>\\n<summary>Click to expand ({len(printer.reasoning_buffer)} chars)</summary>\\n\\n{printer.reasoning_buffer}\\n</details>\\n\\n\\\"\\\"\\\"\\n\\n        # Add response\\n        md_content += f\\\"\\\"\\\"\\n---\\n\\n## ‚ú® Response\\n{printer.content_buffer}\\n\\n\\\"\\\"\\\"\\n\\n        # Add tool calls if present\\n        if metadata.tool_calls:\\n            md_content += f\\\"\\\"\\\"\\n---\\n\\n## üõ†Ô∏è Tool Calls\\n\\\"\\\"\\\"\\n            for i, tool_call in enumerate(metadata.tool_calls, 1):\\n                md_content += f\\\"\\\"\\\"\\n### Tool Call {i}\\n- **Function**: `{tool_call['function']['name']}`\\n- **Arguments**: `{tool_call['function']['arguments']}`\\n\\\"\\\"\\\"\\n\\n            if metadata.tool_results:\\n                md_content += f\\\"\\\"\\\"\\n### Tool Results\\n\\\"\\\"\\\"\\n                for result in metadata.tool_results:\\n                    md_content += f\\\"\\\"\\\"\\n- **{result['name']}**: {result['content']}\\n\\\"\\\"\\\"\\n\\n        md_content += f\\\"\\\"\\\"\\n---\\n## üìä Statistics\\n\\n- **Finish Reason**: {metadata.finish_reason or 'N/A'}\\n- **Tokens**: {metadata.total_tokens or 'N/A'} (Prompt: {metadata.prompt_tokens or 'N/A'}, Completion: {metadata.completion_tokens or 'N/A'})\\n- **Token Rate**: {f'{metadata.get_token_rate():.1f} tokens/s' if metadata.get_token_rate() else 'N/A'}\\n- **First Token Latency**: {f'{(printer.first_token_time - printer.start_time):.2f}s' if printer.first_token_time else 'N/A'}\\n- **Estimated Total Tokens**: {printer.token_count}\\n\\\"\\\"\\\"\\n\\n        with open(md_path, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n            f.write(md_content)\\n\\n        return md_path\\n\\n    @staticmethod\\n    def _save_json(\\n        base_path: Path,\\n        payload: Dict[str, Any],\\n        metadata: RequestMetadata,\\n        printer: TypewriterPrinter,\\n    ) -> Path:\\n        \\\"\\\"\\\"Save conversation as JSON for continuation\\\"\\\"\\\"\\n        json_path = base_path.with_suffix(\\\".json\\\")\\n\\n        # Build conversation for continuation\\n        continuation = {\\n            \\\"metadata\\\": {\\n                \\\"export_time\\\": datetime.now().isoformat(),\\n                \\\"model\\\": metadata.model,\\n                \\\"total_tokens\\\": metadata.total_tokens,\\n                \\\"duration\\\": metadata.duration,\\n            },\\n            \\\"messages\\\": payload.get(\\\"messages\\\", []).copy(),\\n        }\\n\\n        # Add assistant response\\n        assistant_msg = {\\n            \\\"role\\\": \\\"assistant\\\",\\n            \\\"content\\\": printer.content_buffer,\\n        }\\n        if printer.reasoning_buffer:\\n            assistant_msg[\\\"reasoning_content\\\"] = printer.reasoning_buffer\\n        if metadata.tool_calls:\\n            # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_calls‰∏≠ÁöÑID‰∏ç‰∏∫null\\n            for tool_call in metadata.tool_calls:\\n                if not tool_call.get(\\\"id\\\"):\\n                    import uuid\\n\\n                    tool_call[\\\"id\\\"] = str(uuid.uuid4())\\n            assistant_msg[\\\"tool_calls\\\"] = metadata.tool_calls\\n\\n        continuation[\\\"messages\\\"].append(assistant_msg)\\n\\n        # Add tool results as tool messages\\n        for result in metadata.tool_results:\\n            # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_call_id‰∏ç‰∏∫null\\n            if not result.get(\\\"tool_call_id\\\"):\\n                import uuid\\n\\n                result[\\\"tool_call_id\\\"] = str(uuid.uuid4())\\n            continuation[\\\"messages\\\"].append(result)\\n\\n        with open(json_path, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n            json.dump(continuation, f, ensure_ascii=False, indent=2)\\n\\n        return json_path\\n\\n\\n# ============ Argument Parser ============\\n\\n\\ndef create_argument_parser() -> argparse.ArgumentParser:\\n    \\\"\\\"\\\"Create enhanced argument parser\\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"AI Chat CLI - Enhanced with graceful parameter handling\\\",\\n        formatter_class=argparse.RawTextHelpFormatter,\\n        epilog=\\\"\\\"\\\"\\nConfiguration Strategy:\\n  1. Unified settings via config.json to avoid model-specific parameter issues\\n  2. CLI arguments safely override config.json (removes conflicting params)\\n  3. profiles.json or .env provides endpoint and api_key\\n\\nImage Support:\\n  Use @filename.png to attach images (png, jpg, jpeg, gif, webp, bmp supported)\\n  Example: python chat.py @photo.jpg \\\"Describe this image\\\"\\n\\nToken Estimation:\\n  Before sending, the CLI will display estimated prompt tokens and max_tokens\\n  to help you avoid context overflow.\\n\\nTool Support:\\n  Define tools in tools.py with proper function signatures and docstrings\\n  The CLI will automatically load and make them available to the model\\n\\nExamples:\\n  # Basic usage with default config\\n  python chat.py \\\"Hello, how are you?\\\"\\n  \\n  # With file attachment (code or image)\\n  python chat.py @code.py \\\"Explain this code\\\"\\n  python chat.py @image.png \\\"What's in this image?\\\"\\n  \\n  # Custom profile and config\\n  python chat.py -p deepseek -f deepseek_config.json \\\"Explain quantum computing\\\"\\n  \\n  # Continue conversation\\n  python chat.py --context conversation.json \\\"Continue from here\\\"\\n  \\n  # Start new conversation (don't save to config)\\n  python chat.py --new \\\"Start fresh conversation\\\"\\n  \\n  # Disable streaming\\n  python chat.py --no-stream \\\"Generate a long response\\\"\\n  \\n  # Override parameters\\n  python chat.py --temperature 0.5 --max-tokens 2000 \\\"Creative writing\\\"\\n        \\\"\\\"\\\",\\n    )\\n\\n    # Connection\\n    parser.add_argument(\\n        \\\"--profile\\\", \\\"-p\\\", default=\\\"default\\\", help=\\\"Profile name in profiles.json\\\"\\n    )\\n    parser.add_argument(\\\"--endpoint\\\", \\\"-e\\\", help=\\\"Override API endpoint\\\")\\n    parser.add_argument(\\\"--api-key\\\", \\\"-k\\\", help=\\\"Override API key\\\")\\n\\n    # Configuration\\n    parser.add_argument(\\n        \\\"--config\\\", \\\"-c\\\", default=\\\"config.json\\\", help=\\\"Request body configuration file\\\"\\n    )\\n    parser.add_argument(\\n        \\\"--new\\\",\\n        action=\\\"store_true\\\",\\n        help=\\\"Start new conversation (don't save context to config)\\\",\\n    )\\n\\n    # Input\\n    parser.add_argument(\\n        \\\"prompt\\\", nargs=\\\"*\\\", help=\\\"Prompt text (supports @filename for files/images)\\\"\\n    )\\n    parser.add_argument(\\n        \\\"--context\\\", help=\\\"Conversation context JSON file for continuation\\\"\\n    )\\n\\n    # OpenAI Parameters (override config.json)\\n    parser.add_argument(\\\"--model\\\", \\\"-m\\\", help=\\\"Model name (overrides config)\\\")\\n    parser.add_argument(\\n        \\\"--temperature\\\", \\\"-t\\\", type=float, help=\\\"Sampling temperature (0-2)\\\"\\n    )\\n    parser.add_argument(\\\"--max-tokens\\\", type=int, help=\\\"Maximum tokens to generate\\\")\\n    parser.add_argument(\\\"--top-p\\\", type=float, help=\\\"Nucleus sampling parameter\\\")\\n    parser.add_argument(\\\"--presence-penalty\\\", type=float, help=\\\"Presence penalty\\\")\\n    parser.add_argument(\\\"--frequency-penalty\\\", type=float, help=\\\"Frequency penalty\\\")\\n    parser.add_argument(\\\"--seed\\\", type=int, help=\\\"Random seed for reproducibility\\\")\\n\\n    # Behavior (safely override config)\\n    parser.add_argument(\\n        \\\"--no-stream\\\", action=\\\"store_true\\\", help=\\\"Disable streaming output\\\"\\n    )\\n    parser.add_argument(\\n        \\\"--enable-thinking\\\",\\n        action=\\\"store_true\\\",\\n        default=None,\\n        help=\\\"Enable thinking process\\\",\\n    )\\n    parser.add_argument(\\n        \\\"--no-thinking\\\",\\n        dest=\\\"enable_thinking\\\",\\n        action=\\\"store_false\\\",\\n        help=\\\"Disable thinking process\\\",\\n    )\\n\\n    # Tools\\n    parser.add_argument(\\n        \\\"--tools\\\", default=\\\"tools.py\\\", help=\\\"Python module containing tool functions\\\"\\n    )\\n    parser.add_argument(\\n        \\\"--no-tools\\\", action=\\\"store_true\\\", help=\\\"Disable tool functions\\\"\\n    )\\n\\n    # Output\\n    parser.add_argument(\\\"--output\\\", \\\"-o\\\", help=\\\"Output file base name\\\")\\n\\n    return parser\\n\\n\\n# ============ Main Function ============\\n\\n\\ndef handle_interrupt(signum, frame):\\n    \\\"\\\"\\\"Handle Ctrl+C gracefully\\\"\\\"\\\"\\n    print(f\\\"\\\\n{Colors.warn('‚ö†Ô∏è  Interrupted by user')}\\\")\\n    sys.exit(130)\\n\\n\\ndef main():\\n    \\\"\\\"\\\"Main entry point\\\"\\\"\\\"\\n    # Setup\\n    setup_utf8()\\n    signal.signal(signal.SIGINT, handle_interrupt)\\n\\n    # Parse arguments\\n    parser = create_argument_parser()\\n    args = parser.parse_args()\\n\\n    try:\\n        # 1. Load connection configuration\\n        api_config = ConfigManager.load_connection_config(args)\\n        api_config.validate()\\n\\n        # 2. Load request body from config.json\\n        request_body = ConfigManager.load_request_body(args.config, args)\\n\\n        # 3. Initialize tool manager\\n        tool_manager = None\\n        if not args.no_tools and Path(args.tools).exists():\\n            tool_manager = ToolManager(args.tools)\\n            tool_definitions = tool_manager.get_tool_definitions()\\n            if tool_definitions:\\n                request_body[\\\"tools\\\"] = tool_definitions\\n                print(\\n                    f\\\"{Colors.info('‚úì')} Added {len(tool_definitions)} tools to request\\\",\\n                    file=sys.stderr,\\n                )\\n\\n        # 4. Build messages\\n        messages = MessageBuilder.build_messages(args, request_body)\\n        request_body[\\\"messages\\\"] = messages\\n\\n        # 5. Determine output path\\n        if args.output:\\n            output_base = Path(args.output)\\n        else:\\n            timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n            prompt_hint = (\\n                args.prompt[0][:20]\\n                if args.prompt\\n                else request_body.get(\\\"model\\\", \\\"chat\\\")\\n            )\\n            safe_name = re.sub(r\\\"[^\\\\w\\\\s-]\\\", \\\"\\\", prompt_hint).strip().replace(\\\" \\\", \\\"_\\\")\\n            output_base = Path(f\\\"chat_{safe_name}_{timestamp}\\\")\\n\\n        # 6. Execute request\\n        with open(output_base.with_suffix(\\\".md\\\"), \\\"w\\\", encoding=\\\"utf-8\\\") as out_file:\\n            printer = TypewriterPrinter(out_file)\\n\\n            with APIClient(api_config) as client:\\n                metadata = client.request(request_body, printer, tool_manager)\\n\\n            printer.finalize()\\n\\n            # 7. Add separator before complete message\\n            print(f\\\"\\\\n{Colors.bold('‚îÄ' * 60)}\\\")\\n\\n            # 8. Print statistics\\n            print(f\\\"\\\\n{Colors.info('‚úÖ Complete')}\\\")\\n            print(f\\\"   Duration: {metadata.duration:.2f}s\\\")\\n\\n            if metadata.total_tokens:\\n                print(\\n                    f\\\"   Tokens: {metadata.total_tokens} (Prompt: {metadata.prompt_tokens}, Completion: {metadata.completion_tokens})\\\"\\n                )\\n                if token_rate := metadata.get_token_rate():\\n                    print(f\\\"   Token Rate: {token_rate:.1f} tokens/s\\\")\\n\\n            if metadata.finish_reason:\\n                print(f\\\"   Finish Reason: {metadata.finish_reason}\\\")\\n\\n            if printer.first_token_time:\\n                print(\\n                    f\\\"   First Token: {(printer.first_token_time - printer.start_time):.2f}s\\\"\\n                )\\n\\n            if metadata.tool_calls:\\n                print(f\\\"   Tool Calls: {len(metadata.tool_calls)} executed\\\")\\n\\n            # 9. Save results\\n            md_path, json_path = ResultSaver.save(\\n                output_base, request_body, metadata, printer\\n            )\\n\\n            print(f\\\"\\\\n{Colors.info('üíæ Saved:')}\\\")\\n            print(f\\\"   {md_path}\\\")\\n            print(f\\\"   {json_path}\\\")\\n            print(\\n                f\\\"\\\\n{Colors.info('üí° Tip:')} Continue with: --context {json_path.name}\\\"\\n            )\\n\\n            # 10. Save context to config.json if not --new\\n            if not args.new:\\n                # Update messages with assistant response\\n                updated_messages = messages.copy()\\n\\n                # Add assistant message\\n                assistant_msg = {\\n                    \\\"role\\\": \\\"assistant\\\",\\n                    \\\"content\\\": printer.content_buffer,\\n                }\\n                if printer.reasoning_buffer:\\n                    assistant_msg[\\\"reasoning_content\\\"] = printer.reasoning_buffer\\n                if metadata.tool_calls:\\n                    assistant_msg[\\\"tool_calls\\\"] = metadata.tool_calls\\n\\n                updated_messages.append(assistant_msg)\\n\\n                # Add tool results\\n                for result in metadata.tool_results:\\n                    updated_messages.append(result)\\n\\n                # Save to config.json\\n                ConfigManager.save_context_to_config(args.config, updated_messages)\\n\\n    except KeyboardInterrupt:\\n        print(f\\\"\\\\n{Colors.warn('‚ö†Ô∏è  Interrupted')}\\\", file=sys.stderr)\\n        sys.exit(130)\\n    except (ConfigurationError, FileNotFoundError) as e:\\n        print(f\\\"{Colors.error('‚ùå Configuration Error:')} {e}\\\", file=sys.stderr)\\n        sys.exit(1)\\n    except APIError as e:\\n        print(f\\\"{Colors.error('‚ùå API Error:')} {e}\\\", file=sys.stderr)\\n        sys.exit(1)\\n    except Exception as e:\\n        logger.exception(\\\"Unexpected error\\\")\\n        print(f\\\"{Colors.error('‚ùå Unexpected error:')} {e}\\\", file=sys.stderr)\\n        sys.exit(1)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n\\n```\\n[File: deepseek.py]\\n\\n```json\\n{\\n  \\\"metadata\\\": {\\n    \\\"export_time\\\": \\\"2026-02-06T03:52:29.393673\\\",\\n    \\\"model\\\": \\\"Qwen/Qwen3-8B\\\",\\n    \\\"total_tokens\\\": 771,\\n    \\\"duration\\\": 13.744392\\n  },\\n  \\\"messages\\\": [\\n    {\\n      \\\"role\\\": \\\"user\\\",\\n      \\\"content\\\": \\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\"\\n    },\\n    {\\n      \\\"role\\\": \\\"assistant\\\",\\n      \\\"content\\\": \\\"\\\\n\\\\nÊàëÊó†Ê≥ïÁêÜËß£‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùËøô‰∏™ËØçÁöÑÂÖ∑‰ΩìÂê´‰πâÔºåÊÇ®ÊòØÂê¶Âú®Ë°®ËææÊüêÁßçÊÉÖÁª™ÊàñÈúÄË¶ÅÂ∏ÆÂä©ÔºüÂ¶ÇÊûúÈúÄË¶ÅËÆ°ÁÆó„ÄÅÊü•ËØ¢Êó∂Èó¥ÊàñÂ§©Ê∞î‰ø°ÊÅØÔºåËØ∑ÂëäËØâÊàëÂÖ∑‰ΩìÂÜÖÂÆπÔºåÊàë‰ºöÂ∞ΩÂäõÂçèÂä©ÊÇ®„ÄÇ\\\",\\n      \\\"reasoning_content\\\": \\\"\\\\nOkay, the user just said \\\\\\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\\\\\". Let me think about how to respond. First, I need to understand what they mean. \\\\\\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\\\\\" is a term that's often used in Chinese online culture, especially in gaming communities. It's a way to express frustration or exasperation, similar to \\\\\\\"arara\\\\\\\" in Japanese or \\\\\\\"arrgghh\\\\\\\" in English. People might use it when they're upset about something in a game, like a loss or a difficult situation.\\\\n\\\\nNow, looking at the tools provided, there are three functions: calculate, get_current_time, and search_weather. The user's message doesn't seem to involve any of these. They didn't ask for a math calculation, didn't mention a timezone for the current time, and didn't request weather info for a city. So, none of the tools are applicable here. \\\\n\\\\nI should check if there's any hidden request or if the user is testing me. But \\\\\\\"ÈòøÂ∑¥ÈòøÂ∑¥\\\\\\\" is just an expression. Maybe they want me to acknowledge their frustration or explain the term? However, since the tools don't cover that, I need to respond without using any function calls. \\\\n\\\\nThe correct approach is to inform the user that I can't assist with their current message and ask how I can help. That way, I stay within the provided tools and offer further support if needed.\\\\n\\\"\\n    },\\n    {\\n      \\\"role\\\": \\\"user\\\",\\n      \\\"content\\\": \\\"‰ªäÊó•Â§©Ê∞î\\\"\\n    },\\n    {\\n      \\\"role\\\": \\\"assistant\\\",\\n      \\\"content\\\": \\\"\\\\n\\\\nËØ∑ÂëäËØâÊàëÊÇ®ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞îÔºåÊàëÂ∞Ü‰∏∫ÊÇ®ÊêúÁ¥¢Áõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ\\\",\\n      \\\"reasoning_content\\\": \\\"\\\\nÂ•ΩÁöÑÔºåÁî®Êà∑Áé∞Âú®ÈóÆÁöÑÊòØ‚Äú‰ªäÊó•Â§©Ê∞î‚ÄùÔºåÁúãËµ∑Êù•‰ªñ‰ª¨ÊÉ≥‰∫ÜËß£‰ªäÂ§©ÁöÑÂ§©Ê∞îÊÉÖÂÜµ„ÄÇ‰∏çËøá‰πãÂâçÁöÑÂØπËØù‰∏≠ÔºåÁî®Êà∑ËØ¥‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùÔºåÊàëÊ≤°ËÉΩÂáÜÁ°ÆÁêÜËß£‰ªñ‰ª¨ÁöÑÊÑèÊÄù„ÄÇÂèØËÉΩÁî®Êà∑ÊòØÂú®ÊµãËØïÊàñËÄÖÁî®ÊüêÁßçÊöóÂè∑Ôºå‰ΩÜÂêéÊù•‰ªñ‰ª¨Áõ¥Êé•ÈóÆÂ§©Ê∞î‰∫Ü„ÄÇ\\\\n\\\\nÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁ°ÆÂÆöÁî®Êà∑ÊòØÂê¶ÈúÄË¶ÅÊü•ËØ¢Â§©Ê∞î„ÄÇÊ†πÊçÆÂΩìÂâçÁöÑÂ∑•ÂÖ∑ÔºåÊúâ‰∏Ä‰∏™search_weatherÂáΩÊï∞ÔºåÈúÄË¶ÅÂèÇÊï∞ÊòØÂüéÂ∏Ç„ÄÇÁî®Êà∑Ê≤°ÊúâÊèêÂà∞ÂÖ∑‰ΩìÂüéÂ∏ÇÔºåÊâÄ‰ª•ÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•ËØ¢ÈóÆ„ÄÇ‰ΩÜÊúâÊó∂ÂÄôÁî®Êà∑ÂèØËÉΩÈªòËÆ§Êü•ËØ¢ÊâÄÂú®ÂüéÂ∏ÇÔºå‰∏çËøáËøôÈáåÊ≤°Êúâ‰∏ä‰∏ãÊñáÔºåÊâÄ‰ª•‰∏çËÉΩÁ°ÆÂÆö„ÄÇ\\\\n\\\\nÂè¶Â§ñÔºåÁî®Êà∑ÂèØËÉΩÊòØÂú®ÁªßÁª≠‰πãÂâçÁöÑÂØπËØùÔºå‰ΩÜ‰πãÂâçÁöÑ‚ÄúÈòøÂ∑¥ÈòøÂ∑¥‚ÄùÊ≤°ÊúâÊòéÁ°Æ‰ø°ÊÅØ„ÄÇÊâÄ‰ª•ÊúÄÂ•ΩËøòÊòØÂÖàÁ°ÆËÆ§Áî®Êà∑ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞î„ÄÇ‰∏çËøá‰πüÊúâÂèØËÉΩÁî®Êà∑Âè™ÊòØÁ™ÅÁÑ∂ÊÉ≥‰∫ÜËß£Â§©Ê∞îÔºåÊ≤°ÊúâÊèêÂà∞ÂüéÂ∏ÇÔºåËøôÊó∂ÂÄôÂèØËÉΩÈúÄË¶ÅÊèêÁ§∫‰ªñ‰ª¨Êèê‰æõÂüéÂ∏ÇÂêçÁß∞„ÄÇ\\\\n\\\\nÊ£ÄÊü•Â∑•ÂÖ∑‰∏≠ÁöÑÂáΩÊï∞Ôºåsearch_weatherÁöÑÂèÇÊï∞ÊòØcityÔºåÂøÖÂ°´„ÄÇÊâÄ‰ª•ÂøÖÈ°ªËé∑ÂèñÂüéÂ∏Ç‰ø°ÊÅØÊâçËÉΩË∞ÉÁî®„ÄÇÂõ†Ê≠§ÔºåÊàëÂ∫îËØ•ÂõûÂ§çÁî®Êà∑ÔºåËØ¢ÈóÆ‰ªñ‰ª¨ÊÉ≥Êü•ËØ¢Âì™‰∏™ÂüéÂ∏ÇÁöÑÂ§©Ê∞î„ÄÇËøôÊ†∑Êó¢Á¨¶ÂêàÂ∑•ÂÖ∑ÁöÑË¶ÅÊ±ÇÔºå‰πüËÉΩÁ°Æ‰øùÊèê‰æõÂáÜÁ°ÆÁöÑ‰ø°ÊÅØ„ÄÇ\\\\n\\\\nÂè¶Â§ñÔºåÊòØÂê¶ÊúâÂÖ∂‰ªñÂèØËÉΩÔºüÊØîÂ¶ÇÁî®Êà∑ÂèØËÉΩËØØËæìÂÖ•ÔºåÊàñËÄÖÊúâÂÖ∂‰ªñÊÑèÂõæÔºü‰ΩÜÊ†πÊçÆÁé∞ÊúâÂ∑•ÂÖ∑ÔºåÂè™ÊúâÂ§©Ê∞î„ÄÅËÆ°ÁÆóÊó∂Èó¥ÂíåËé∑ÂèñÊó∂Èó¥ÁöÑÂäüËÉΩ„ÄÇÊâÄ‰ª•ÊúÄÂêàÁêÜÁöÑÂõûÂ∫îÊòØËØ∑Ê±ÇÂüéÂ∏ÇÂêçÁß∞„ÄÇ\\\\n\\\"\\n    },\\n    {\\n      \\\"role\\\": \\\"user\\\",\\n      \\\"content\\\": \\\"ÂçóÊûÅ\\\"\\n    },\\n    {\\n      \\\"role\\\": \\\"assistant\\\",\\n      \\\"content\\\": \\\"\\\\n\\\\n\\\",\\n      \\\"reasoning_content\\\": \\\"\\\\nÂ•ΩÁöÑÔºåÁî®Êà∑Áé∞Âú®ÊèêÂà∞‚ÄúÂçóÊûÅ‚ÄùÔºåÈúÄË¶ÅÊü•ËØ¢ÈÇ£ÈáåÁöÑÂ§©Ê∞î„ÄÇÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁ°ÆËÆ§Áî®Êà∑ÊòØÂê¶ÁúüÁöÑÊÉ≥Êü•ËØ¢ÂçóÊûÅÁöÑÂ§©Ê∞î„ÄÇÂçóÊûÅÊòØ‰∏Ä‰∏™Â§ßÊ¥≤ÔºåÂ§©Ê∞îÊÉÖÂÜµÂèØËÉΩÊØîËæÉÁâπÊÆäÔºåËÄå‰∏îÁî®Êà∑ÂèØËÉΩÊ≤°ÊúâÊÑèËØÜÂà∞ÈúÄË¶ÅÊåáÂÆöÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ‰∏çËøáÔºåÊ†πÊçÆ‰πãÂâçÁöÑÂØπËØùÔºåÁî®Êà∑ÂèØËÉΩÊòØÂú®ÊµãËØïÊàñËÄÖÊúâÂÖ∂‰ªñÊÑèÂõæÔºå‰ΩÜÊ†πÊçÆÂ∑•ÂÖ∑‰∏≠ÁöÑsearch_weatherÂáΩÊï∞ÔºåÂèÇÊï∞ÊòØÂüéÂ∏ÇÔºåÊâÄ‰ª•ÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•Á°ÆËÆ§„ÄÇ\\\\n\\\\n‰∏çËøáÔºåÁî®Êà∑‰πãÂâçÂ∑≤ÁªèÈóÆËøá‚Äú‰ªäÊó•Â§©Ê∞î‚ÄùÔºåÁÑ∂ÂêéÊàëËÆ©‰ªñÊåáÂÆöÂüéÂ∏ÇÔºåÁé∞Âú®‰ªñÁõ¥Êé•ÂõûÁ≠î‚ÄúÂçóÊûÅ‚ÄùÔºåÂèØËÉΩÊòØÊåáÊï¥‰∏™ÂçóÊûÅÊ¥≤ÁöÑÂ§©Ê∞îÊÉÖÂÜµ„ÄÇ‰ΩÜÂçóÊûÅÊ¥≤Ê≤°ÊúâÁªü‰∏ÄÁöÑÂüéÂ∏ÇÔºåÂèØËÉΩÊúâÁßëÁ†îÁ´ôÔºåÊØîÂ¶ÇÈ∫¶ÂÖãÈªòÂ§öÁ´ô„ÄÅÈïøÂüéÁ´ôÁ≠â„ÄÇËøôÊó∂ÂÄôÊàëÈúÄË¶ÅÂà§Êñ≠ÊòØÂê¶Â∫îËØ•Áõ¥Êé•‰ΩøÁî®‚ÄúÂçóÊûÅ‚Äù‰Ωú‰∏∫ÂüéÂ∏ÇÂèÇÊï∞ÔºåÊàñËÄÖÊòØÂê¶ÈúÄË¶ÅÊèêÁ§∫Áî®Êà∑ËæìÂÖ•ÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ\\\\n\\\\nÊ†πÊçÆÂ∑•ÂÖ∑ÁöÑÊèèËø∞Ôºåsearch_weatherÂáΩÊï∞ÁöÑÂèÇÊï∞ÊòØcityÔºåÁ±ªÂûã‰∏∫Â≠óÁ¨¶‰∏≤Ôºå‰ΩÜÂèØËÉΩËØ•ÂáΩÊï∞ÂÜÖÈÉ®Â§ÑÁêÜ‰∫ÜÁ±ª‰ºº‚ÄúÂçóÊûÅ‚ÄùÁöÑËæìÂÖ•ÔºåÊàñËÄÖÁî®Êà∑ÂèØËÉΩÊúüÊúõÂæóÂà∞ÂçóÊûÅÊ¥≤ÁöÑÂ§©Ê∞îÊ¶ÇÂÜµ„ÄÇ‰πüÊúâÂèØËÉΩÁî®Êà∑ËæìÂÖ•ÊúâËØØÔºåÊàñËÄÖÊÉ≥‰∫ÜËß£Êüê‰∏™ÁâπÂÆöÂüéÂ∏ÇÁöÑÂ§©Ê∞îÔºå‰ΩÜËØØÂÜô‰∏∫ÂçóÊûÅ„ÄÇ‰æãÂ¶ÇÔºåÁî®Êà∑ÂèØËÉΩÊÉ≥Êü•Âåó‰∫¨„ÄÅ‰∏äÊµ∑Á≠âÂüéÂ∏ÇÔºå‰ΩÜÈîôËØØËæìÂÖ•‰∫Ü‚ÄúÂçóÊûÅ‚Äù„ÄÇ\\\\n\\\\nËøôÊó∂ÂÄôÔºåÊàëÈúÄË¶ÅÂÖàË∞ÉÁî®search_weatherÂáΩÊï∞ÔºåÂ∞ùËØï‰ΩøÁî®‚ÄúÂçóÊûÅ‚Äù‰Ωú‰∏∫cityÂèÇÊï∞ÔºåÁúãÁúãÊòØÂê¶ËÉΩËøîÂõûÊúâÊïàÁªìÊûú„ÄÇÂ¶ÇÊûúËøîÂõûÈîôËØØÔºåÂèØËÉΩÈúÄË¶ÅÊèêÁ§∫Áî®Êà∑Á°ÆËÆ§ÂüéÂ∏ÇÂêçÁß∞„ÄÇ‰ΩÜÊ†πÊçÆÂΩìÂâçÂ∑•ÂÖ∑ÁöÑËÆæÂÆöÔºåÂèØËÉΩÈúÄË¶ÅÁõ¥Êé•Ë∞ÉÁî®ÂáΩÊï∞ÔºåÁÑ∂ÂêéÊ†πÊçÆÁªìÊûúÂ§ÑÁêÜ„ÄÇÂ¶ÇÊûúÂáΩÊï∞ÊîØÊåÅÂú∞Âå∫Êü•ËØ¢ÔºåÂèØËÉΩÊ≤°ÈóÆÈ¢òÔºõÂ¶ÇÊûú‰∏çÊîØÊåÅÔºåÂèØËÉΩÈúÄË¶ÅËøõ‰∏ÄÊ≠•ÊæÑÊ∏Ö„ÄÇ\\\\n\\\\nÂõ†Ê≠§ÔºåÊ≠£Á°ÆÁöÑÂÅöÊ≥ïÊòØÂÖàË∞ÉÁî®search_weatherÂáΩÊï∞ÔºåÂèÇÊï∞‰∏∫city: \\\\\\\"ÂçóÊûÅ\\\\\\\"ÔºåÁÑ∂ÂêéÊ†πÊçÆËøîÂõûÁöÑÊï∞ÊçÆÂÜ≥ÂÆöÊòØÂê¶ÈúÄË¶ÅËøõ‰∏ÄÊ≠•ËØ¢ÈóÆÁî®Êà∑ÂÖ∑‰ΩìÂüéÂ∏Ç„ÄÇ‰ΩÜÊ†πÊçÆÁî®Êà∑‰πãÂâçÁöÑÂØπËØùÔºå‰ªñ‰ª¨ÂèØËÉΩÂè™ÊòØÊÉ≥‰∫ÜËß£ÂçóÊûÅÁöÑÂ§©Ê∞îÊÉÖÂÜµÔºåÊâÄ‰ª•Áõ¥Êé•Ë∞ÉÁî®ÂáΩÊï∞ÊòØÂêàÈÄÇÁöÑ„ÄÇ\\\\n\\\",\\n      \\\"tool_calls\\\": [\\n        {\\n          \\\"id\\\": null,\\n          \\\"type\\\": \\\"function\\\",\\n          \\\"function\\\": {\\n            \\\"name\\\": \\\"search_weather\\\",\\n            \\\"arguments\\\": \\\" {\\\\\\\"city\\\\\\\": \\\\\\\"ÂçóÊûÅ\\\\\\\"}\\\"\\n          }\\n        }\\n      ]\\n    },\\n    {\\n      \\\"tool_call_id\\\": null,\\n      \\\"role\\\": \\\"tool\\\",\\n      \\\"name\\\": \\\"search_weather\\\",\\n      \\\"content\\\": \\\"\\\\\\\"Weather in ÂçóÊûÅ: Sunny, 25¬∞C\\\\\\\"\\\"\\n    }\\n  ]\\n}\\n```\\n[File: chat_ÂçóÊûÅ_20260206_035215.json]\\n\\nËøôÊ†∑ÂØπÂêóÔºå‰ΩÜÊòØÊâæÂú∞ÊñπÂ•ΩÈ∫ªÁÉ¶Ôºå‰∏ãÊ¨°‰Ω†ËøòÊòØÁõ¥Êé•ÁªôÊàëÂÆåÊï¥code\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\n\",\n      \"reasoning_content\": \"Áî®Êà∑Â∑≤ÁªèÂ∫îÁî®‰∫ÜÊàëÂª∫ËÆÆÁöÑ‰øÆÂ§çÔºå‰øÆÊîπ‰∫ÜToolManager.execute_toolÊñπÊ≥ïÔºåÊ∑ªÂä†‰∫ÜuuidÁîüÊàê„ÄÇ‰ΩÜÁî®Êà∑ËØ¥\\\"ÊâæÂú∞ÊñπÂ•ΩÈ∫ªÁÉ¶Ôºå‰∏ãÊ¨°‰Ω†ËøòÊòØÁõ¥Êé•ÁªôÊàëÂÆåÊï¥code\\\"„ÄÇÊÑèÊÄùÊòØ‰∏ãÊ¨°ÊàëÂ∫îËØ•Êèê‰æõÂÆåÊï¥ÁöÑ‰ª£Á†ÅÔºåËÄå‰∏çÊòØÂè™Êèê‰æõ‰øÆÊîπÈÉ®ÂàÜ„ÄÇ\\n\\n‰∏çËøáÔºåÁé∞Âú®Áî®Êà∑ÂèØËÉΩËøòÊúâÈóÆÈ¢òÔºö‰øÆÂ§çÂ∑≤ÁªèÂ∫îÁî®Ôºå‰ΩÜÂèØËÉΩËøòÈúÄË¶Å‰øÆÂ§çResultSaver._save_jsonÊñπÊ≥ïÔºåÁ°Æ‰øù‰øùÂ≠òÁöÑJSON‰∏≠tool_call_id‰∏ç‰∏∫null„ÄÇÁî®Êà∑Â∑≤ÁªèÂ∫îÁî®‰∫ÜÈÇ£‰∏™‰øÆÂ§çÂêóÔºü‰ªéÊèê‰æõÁöÑ‰ª£Á†ÅÁúãÔºåResultSaver._save_jsonÊñπÊ≥ï‰πüÂ∑≤ÁªèË¢´‰øÆÂ§ç‰∫ÜÔºåÊ∑ªÔøΩÔøΩÔøΩ‰∫ÜuuidÁîüÊàê„ÄÇ\\n\\n‰ΩÜÁî®Êà∑ÂèØËÉΩ‰ªçÁÑ∂ÈÅáÂà∞400ÈîôËØØÔºüÂõ†‰∏∫Áé∞ÊúâÁöÑJSONÊñá‰ª∂Ôºàchat_ÂçóÊûÅ_20260206_035215.jsonÔºâ‰∏≠tool_call_id‰ªçÁÑ∂ÊòØnull„ÄÇ‰øÆÂ§çÂè™ÂØπÊñ∞ÁîüÊàêÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®ÊúâÊïàÔºåÂØπÂ∑≤ÊúâÁöÑJSONÊñá‰ª∂Êó†Êïà„ÄÇÁî®Êà∑ÈúÄË¶ÅÊâãÂä®ÁºñËæëJSONÊñá‰ª∂ÔºåÊàñËÄÖÈáçÊñ∞ËøêË°åÂØπËØùÁîüÊàêÊñ∞ÁöÑJSON„ÄÇ\\n\\nÁî®Êà∑ÂèØËÉΩÂ∏åÊúõÊàëÊèê‰æõ‰∏Ä‰∏™ËÑöÊú¨ÊàñÊñπÊ≥ïÊù•‰øÆÂ§çÁé∞ÊúâÁöÑJSONÊñá‰ª∂„ÄÇÊàñËÄÖÔºåÊàëÂèØ‰ª•‰øÆÊîπMessageBuilder._load_contextÊñπÊ≥ïÔºåÂú®Âä†ËΩΩ‰∏ä‰∏ãÊñáÊó∂Ëá™Âä®‰øÆÂ§çnullÁöÑtool_call_id„ÄÇ\\n\\nËÆ©ÊàëÊ£ÄÊü•‰∏Ä‰∏ãMessageBuilder._load_contextÊñπÊ≥ïÔºöÂÆÉÂè™ÊòØÂä†ËΩΩJSONÊñá‰ª∂Âπ∂ËøîÂõûÊ∂àÊÅØÂàóË°®„ÄÇÊàëÂèØ‰ª•‰øÆÊîπËøô‰∏™ÊñπÊ≥ïÔºåÂú®Âä†ËΩΩÊó∂‰øÆÂ§çnullÁöÑtool_call_id„ÄÇ\\n\\nÂè¶Â§ñÔºåÁî®Êà∑ÂèØËÉΩËøòÂ∏åÊúõ‰øÆÂ§çtool_callsÊï∞ÁªÑ‰∏≠ÁöÑidÂ≠óÊÆµÔºàÂú®assistantÊ∂àÊÅØ‰∏≠ÔºâÔºåÁõÆÂâç‰πüÊòØnull„ÄÇ\\n\\nÊâÄ‰ª•ÔºåÊàëÈúÄË¶Å‰øÆÊîπMessageBuilder._load_contextÊñπÊ≥ïÔºåÈÅçÂéÜÊâÄÊúâÊ∂àÊÅØÔºå‰øÆÂ§çtool_call_idÂíåtool_calls.id„ÄÇ\\n\\nËÆ©ÊàëÊèê‰æõ‰∏Ä‰∏™‰øÆÂ§çÁâàÊú¨ÁöÑMessageBuilder._load_contextÊñπÊ≥ï„ÄÇ‰ΩÜÁî®Êà∑ÊÉ≥Ë¶ÅÂÆåÊï¥ÁöÑ‰ª£Á†ÅÔºåÊàëÂèØ‰ª•Êèê‰æõÊï¥‰∏™Á±ªÁöÑ‰øÆÊîπÔºåÊàñËÄÖÂè™Êèê‰æõ‰øÆÊîπÁöÑÊñπÊ≥ï„ÄÇËÄÉËôëÂà∞Áî®Êà∑ËØ¥\\\"ÊâæÂú∞ÊñπÂ•ΩÈ∫ªÁÉ¶\\\"ÔºåÊàëÂ∫îËØ•Êèê‰æõÂÆåÊï¥ÁöÑMessageBuilderÁ±ªÔºåÊàñËÄÖËá≥Â∞ëÊèê‰æõ‰øÆÊîπÁöÑÊñπÊ≥ï„ÄÇ\\n\\nÈ¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁúãÁúãÂΩìÂâçÁöÑMessageBuilderÁ±ªÂú®‰ª£Á†Å‰∏≠ÁöÑ‰ΩçÁΩÆ„ÄÇ‰ªéÊèê‰æõÁöÑ‰ª£Á†ÅÁúãÔºåMessageBuilderÁ±ªÂú®Á¨¨352Ë°åÂºÄÂßã„ÄÇÊàëÈúÄË¶Å‰øÆÊîπ_load_contextÊñπÊ≥ïÔºåÊàñËÄÖÊ∑ªÂä†‰∏Ä‰∏™ËæÖÂä©ÊñπÊ≥ïÊù•‰øÆÂ§çÊ∂àÊÅØ„ÄÇ\\n\\nÊõ¥Â•ΩÁöÑÊñπÊ≥ïÊòØÂú®Âä†ËΩΩ‰∏ä‰∏ãÊñáÊó∂Ëá™Âä®‰øÆÂ§ç„ÄÇÊàëÂ∞Ü‰øÆÊîπ_load_contextÊñπÊ≥ïÔºåÊ∑ªÂä†‰∏Ä‰∏™ÈÄíÂΩíÂáΩÊï∞Êù•‰øÆÂ§çÊ∂àÊÅØ‰∏≠ÁöÑtool_call_idÂíåtool_calls.id„ÄÇ\\n\\nËÆ©ÊàëÁºñÂÜô‰øÆÂ§ç‰ª£Á†ÅÔºö\",\n      \"tool_calls\": [\n        {\n          \"id\": \"50d14990-0216-4296-b5d8-fa6cf3d61a76\",\n          \"type\": \"function\",\n          \"function\": {\n            \"name\": \"get_current_time\",\n            \"arguments\": \"{\\\"timezone\\\": \\\"UTC\\\"}\"\n          }\n        }\n      ]\n    },\n    {\n      \"tool_call_id\": \"7163d582-dcb7-41ca-86fe-15985cf0e2af\",\n      \"role\": \"tool\",\n      \"name\": \"get_current_time\",\n      \"content\": \"\\\"Current time in UTC: 2026-02-06 04:07:38\\\"\"\n    }\n  ],\n  \"context\": \"chat_deepseekpy_20260206_034856.json\"\n}\n```\n[File: deepseek.json]"
    },
    {
      "role": "assistant",
      "content": "ÊàëÁêÜËß£‰Ω†ÁöÑÈúÄÊ±Ç‰∫ÜÔºÅËÆ©ÊàëÈáçÊñ∞Êï¥ÁêÜ‰∏Ä‰∏ãÈÄªËæëÔºåÂπ∂Êèê‰æõÂÆåÊï¥ÁöÑ‰øÆÂ§ç‰ª£Á†Å„ÄÇ‰∏ªË¶ÅÈóÆÈ¢òÊúâ‰∏â‰∏™Ôºö\n\n1. **tool_call_id‰∏∫null**ÔºöÈúÄË¶Å‰øÆÂ§çÂ∑•ÂÖ∑Ë∞ÉÁî®ID\n2. **--newÂèÇÊï∞Êó†Êïà**ÔºöÈúÄË¶ÅÊ≠£Á°ÆÂÆûÁé∞Êñ∞ÂØπËØùÈÄªËæë\n3. **config.json‰øùÂ≠ò‰∫Ümessages**Ôºö‰∏çÂ∫îËØ•‰øùÂ≠òÂØπËØùÂéÜÂè≤Âà∞config.json\n\n‰∏ãÈù¢ÊòØÂÆåÊï¥ÁöÑ‰øÆÂ§ç‰ª£Á†ÅÔºö\n\n```python\n#!/usr/bin/env python3\n\"\"\"\nAdvanced Typewriter Effect HTTP Client for OpenAI-compatible APIs (Enhanced Version).\nZero-default policy with graceful fallback - avoids crashes when encountering unsupported parameters.\n\"\"\"\n\nimport argparse\nimport base64\nimport importlib.util\nimport inspect\nimport io\nimport json\nimport logging\nimport mimetypes\nimport os\nimport re\nimport signal\nimport sys\nimport time\nimport uuid\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional, Dict, List, Any, Tuple, Union, Callable\n\nimport requests\nfrom dotenv import load_dotenv\n\n# ============ Setup & Constants ============\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    stream=sys.stderr,\n)\nlogger = logging.getLogger(__name__)\n\nCODE_BLOCK = \"`\" * 3\n\n\nclass Constants:\n    \"\"\"Application constants\"\"\"\n\n    CHUNK_SIZE = 1024\n    STREAM_DELAY_REASONING = 0.01\n    STREAM_DELAY_CONTENT = 0.03\n    TIMEOUT = 60 * 60 * 6  # ‰øùÊåÅ 1 Â∞èÊó∂Ë∂ÖÊó∂\n\n\nclass Colors:\n    \"\"\"ANSI Color codes for terminal output.\"\"\"\n\n    GREEN = \"\\033[92m\"\n    YELLOW = \"\\033[93m\"\n    RED = \"\\033[91m\"\n    BLUE = \"\\033[94m\"\n    CYAN = \"\\033[96m\"\n    BOLD = \"\\033[1m\"\n    END = \"\\033[0m\"\n\n    @classmethod\n    def info(cls, text: str) -> str:\n        return f\"{cls.GREEN}{text}{cls.END}\"\n\n    @classmethod\n    def warn(cls, text: str) -> str:\n        return f\"{cls.YELLOW}{text}{cls.END}\"\n\n    @classmethod\n    def error(cls, text: str) -> str:\n        return f\"{cls.RED}{text}{cls.END}\"\n\n    @classmethod\n    def bold(cls, text: str) -> str:\n        return f\"{cls.BOLD}{text}{cls.END}\"\n\n\n# ============ Exceptions ============\n\n\nclass ChatError(Exception):\n    \"\"\"Base exception for chat errors\"\"\"\n\n    pass\n\n\nclass ConfigurationError(ChatError):\n    \"\"\"Missing required configuration\"\"\"\n\n    pass\n\n\nclass APIError(ChatError):\n    \"\"\"API communication error\"\"\"\n\n    pass\n\n\nclass FileProcessingError(ChatError):\n    \"\"\"File I/O error\"\"\"\n\n    pass\n\n\nclass ToolError(ChatError):\n    \"\"\"Tool execution error\"\"\"\n\n    pass\n\n\n# ============ UTF-8 Setup ============\n\n\ndef setup_utf8():\n    \"\"\"Force UTF-8 encoding for Windows and Unix\"\"\"\n    if sys.platform == \"win32\":\n        import ctypes\n\n        try:\n            kernel32 = ctypes.windll.kernel32\n            kernel32.SetConsoleCP(65001)\n            kernel32.SetConsoleOutputCP(65001)\n        except Exception:\n            pass\n\n    if hasattr(sys.stdout, \"buffer\"):\n        sys.stdout = io.TextIOWrapper(\n            sys.stdout.buffer, encoding=\"utf-8\", errors=\"replace\", line_buffering=True\n        )\n\n\n# ============ Data Models ============\n\n\n@dataclass\nclass APIConfig:\n    \"\"\"API connection configuration - only endpoint and api_key\"\"\"\n\n    endpoint: str = \"\"\n    api_key: str = \"\"\n    timeout: int = Constants.TIMEOUT  # HTTP timeout, not API parameter\n\n    def validate(self) -> None:\n        if not self.endpoint:\n            raise ConfigurationError(\"API endpoint is required\")\n        if not self.api_key:\n            raise ConfigurationError(\"API key is required\")\n\n\n@dataclass\nclass RequestMetadata:\n    \"\"\"Metadata for request tracking and reporting\"\"\"\n\n    start_time: datetime = field(default_factory=datetime.now)\n    end_time: Optional[datetime] = None\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n    total_tokens: int = 0\n    finish_reason: Optional[str] = None\n    model: Optional[str] = None\n    duration: float = 0.0\n    tool_calls: List[Dict] = field(default_factory=list)\n    tool_results: List[Dict] = field(default_factory=list)\n\n    def finalize(self) -> None:\n        self.end_time = datetime.now()\n        self.duration = (self.end_time - self.start_time).total_seconds()\n\n    def get_token_rate(self) -> Optional[float]:\n        \"\"\"Calculate tokens per second\"\"\"\n        if self.duration > 0 and self.completion_tokens > 0:\n            return self.completion_tokens / self.duration\n        return None\n\n\n# ============ Tool System ============\n\n\nclass ToolManager:\n    \"\"\"Manages tool functions and execution\"\"\"\n\n    def __init__(self, tools_module_path: str = \"tools.py\"):\n        self.tools_module_path = tools_module_path\n        self.available_functions: Dict[str, Callable] = {}\n        self.tool_definitions: List[Dict] = []\n        self._load_tools()\n\n    def _load_tools(self):\n        \"\"\"Load tool functions from tools.py module\"\"\"\n        try:\n            spec = importlib.util.spec_from_file_location(\n                \"tools\", self.tools_module_path\n            )\n            if spec and spec.loader:\n                tools_module = importlib.util.module_from_spec(spec)\n                spec.loader.exec_module(tools_module)\n\n                # Find all functions in the module\n                for name, obj in inspect.getmembers(tools_module):\n                    if inspect.isfunction(obj) and not name.startswith(\"_\"):\n                        # Check if function has proper annotations for OpenAI tools\n                        sig = inspect.signature(obj)\n                        params = {}\n                        for param_name, param in sig.parameters.items():\n                            if param_name != \"self\":\n                                param_type = (\n                                    str(param.annotation)\n                                    if param.annotation != inspect.Parameter.empty\n                                    else \"string\"\n                                )\n                                params[param_name] = {\n                                    \"type\": param_type.replace(\"<class '\", \"\")\n                                    .replace(\"'>\", \"\")\n                                    .lower(),\n                                    \"description\": f\"Parameter {param_name}\",\n                                }\n\n                        # Create tool definition\n                        tool_def = {\n                            \"type\": \"function\",\n                            \"function\": {\n                                \"name\": name,\n                                \"description\": obj.__doc__ or f\"Function {name}\",\n                                \"parameters\": {\n                                    \"type\": \"object\",\n                                    \"properties\": params,\n                                    \"required\": list(params.keys()),\n                                },\n                            },\n                        }\n\n                        self.tool_definitions.append(tool_def)\n                        self.available_functions[name] = obj\n\n                        print(\n                            f\"{Colors.info('‚úì')} Loaded tool: {name}\", file=sys.stderr\n                        )\n\n                if not self.tool_definitions:\n                    print(\n                        f\"{Colors.warn('‚ö†')} No tools found in {self.tools_module_path}\",\n                        file=sys.stderr,\n                    )\n            else:\n                print(\n                    f\"{Colors.warn('‚ö†')} Tools module not found: {self.tools_module_path}\",\n                    file=sys.stderr,\n                )\n        except Exception as e:\n            print(f\"{Colors.warn('‚ö†')} Failed to load tools: {e}\", file=sys.stderr)\n\n    def execute_tool(self, tool_call: Dict) -> Dict:\n        \"\"\"Execute a tool call and return result\"\"\"\n        try:\n            function_name = tool_call[\"function\"][\"name\"]\n            function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n\n            if function_name not in self.available_functions:\n                raise ToolError(f\"Unknown function: {function_name}\")\n\n            print(\n                f\"\\n{Colors.CYAN}üîß Executing tool: {function_name}{Colors.END}\",\n                file=sys.stderr,\n            )\n            print(f\"   Arguments: {function_args}\", file=sys.stderr)\n\n            # Execute the function\n            result = self.available_functions[function_name](**function_args)\n\n            print(f\"   Result: {result}\", file=sys.stderr)\n\n            # ‰øÆÂ§çÔºöÂ§ÑÁêÜnullÊàñÁ©∫ÁöÑtool_call_id\n            tool_call_id = tool_call.get(\"id\")\n            if not tool_call_id:\n                tool_call_id = str(uuid.uuid4())\n                print(\n                    f\"{Colors.warn('‚ö†')} Generated tool call ID: {tool_call_id}\",\n                    file=sys.stderr,\n                )\n\n            return {\n                \"tool_call_id\": tool_call_id,\n                \"role\": \"tool\",\n                \"name\": function_name,\n                \"content\": json.dumps(result, ensure_ascii=False),\n            }\n\n        except Exception as e:\n            error_msg = f\"Tool execution failed: {str(e)}\"\n            print(f\"{Colors.error('‚ùå')} {error_msg}\", file=sys.stderr)\n\n            # ‰øÆÂ§çÔºöÈîôËØØÂìçÂ∫î‰πüÁîüÊàêID\n            tool_call_id = tool_call.get(\"id\")\n            if not tool_call_id:\n                tool_call_id = str(uuid.uuid4())\n\n            return {\n                \"tool_call_id\": tool_call_id,\n                \"role\": \"tool\",\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": json.dumps({\"error\": error_msg}, ensure_ascii=False),\n            }\n\n    def get_tool_definitions(self) -> List[Dict]:\n        \"\"\"Get OpenAI-compatible tool definitions\"\"\"\n        return self.tool_definitions\n\n\n# ============ Configuration Management ============\n\n\nclass ConfigManager:\n    \"\"\"Manages configuration loading with graceful fallback\"\"\"\n\n    ENV_PATHS = [\".env\", \"../.env\", Path.home() / \".ai_chat.env\"]\n    PROFILE_PATHS = [\n        Path(\"profiles.json\"),\n        Path(\"../profiles.json\"),\n        Path.home() / \".ai_chat_profiles.json\",\n    ]\n\n    @staticmethod\n    def load_connection_config(args) -> APIConfig:\n        \"\"\"Load endpoint and api_key with priority: CLI > Profile > Env > Default\"\"\"\n        config = APIConfig()\n\n        # 1. Load .env files first\n        for env_path in ConfigManager.ENV_PATHS:\n            if Path(env_path).exists():\n                load_dotenv(env_path)\n                logger.info(f\"Loaded environment: {env_path}\")\n                break\n\n        # 2. From environment variables\n        config.endpoint = os.getenv(\"ENDPOINT\", \"\")\n        config.api_key = os.getenv(\"API_KEY\", \"\")\n\n        # 3. Try to load from profile\n        profile_config = ConfigManager._load_profile(args.profile)\n        if profile_config:\n            if \"endpoint\" in profile_config:\n                config.endpoint = profile_config[\"endpoint\"]\n            if \"api_key\" in profile_config:\n                config.api_key = profile_config[\"api_key\"]\n            if \"apiKey\" in profile_config:\n                config.api_key = profile_config[\"apiKey\"]\n\n        # 4. CLI arguments override everything\n        if hasattr(args, \"endpoint\") and args.endpoint:\n            config.endpoint = args.endpoint\n        if hasattr(args, \"api_key\") and args.api_key:\n            config.api_key = args.api_key\n\n        return config\n\n    @staticmethod\n    def _load_profile(profile_name: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Load profile from various locations\"\"\"\n        for path in ConfigManager.PROFILE_PATHS:\n            if path.exists():\n                try:\n                    with open(path, \"r\", encoding=\"utf-8\") as f:\n                        profiles = json.load(f)\n\n                    if profile_name in profiles:\n                        print(\n                            f\"{Colors.info('‚úì')} Loaded profile: {profile_name} from {path}\",\n                            file=sys.stderr,\n                        )\n                        return profiles[profile_name]\n                except Exception as e:\n                    logger.error(f\"Failed to load profile {path}: {e}\")\n\n        if profile_name != \"default\":\n            print(\n                f\"{Colors.warn('‚ö†')} Profile '{profile_name}' not found, using default\",\n                file=sys.stderr,\n            )\n        return None\n\n    @staticmethod\n    def load_request_body(config_path: str, args) -> Dict[str, Any]:\n        \"\"\"Load request body from config.json with safe parameter handling\"\"\"\n        path = Path(config_path).expanduser().resolve()\n\n        if not path.exists():\n            if (\n                config_path != \"config.json\"\n            ):  # User explicitly specified non-existent file\n                raise FileNotFoundError(f\"Config file not found: {config_path}\")\n            # Use empty config if default config.json doesn't exist\n            return {}\n\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            print(f\"{Colors.info('‚úì')} Loaded request config: {path}\", file=sys.stderr)\n\n            # Apply CLI overrides for safety-critical parameters\n            ConfigManager._apply_cli_overrides(data, args)\n\n            return data\n        except Exception as e:\n            raise ValueError(f\"Failed to load config file ({config_path}): {e}\")\n\n    @staticmethod\n    def _apply_cli_overrides(data: Dict[str, Any], args) -> None:\n        \"\"\"Apply CLI arguments to request body, removing unsupported parameters\"\"\"\n        # Safety: Remove problematic parameters if CLI provides alternatives\n        if hasattr(args, \"model\") and args.model:\n            data[\"model\"] = args.model\n            # Remove any model-specific parameters that might conflict\n            data.pop(\"temperature\", None)  # Some models don't support temperature\n            data.pop(\"top_p\", None)\n            data.pop(\"frequency_penalty\", None)\n            data.pop(\"presence_penalty\", None)\n\n        # OpenAI standard parameters override\n        if hasattr(args, \"temperature\") and args.temperature is not None:\n            data[\"temperature\"] = args.temperature\n        if hasattr(args, \"max_tokens\") and args.max_tokens is not None:\n            data[\"max_tokens\"] = args.max_tokens\n        if hasattr(args, \"top_p\") and args.top_p is not None:\n            data[\"top_p\"] = args.top_p\n        if hasattr(args, \"presence_penalty\") and args.presence_penalty is not None:\n            data[\"presence_penalty\"] = args.presence_penalty\n        if hasattr(args, \"frequency_penalty\") and args.frequency_penalty is not None:\n            data[\"frequency_penalty\"] = args.frequency_penalty\n        if hasattr(args, \"seed\") and args.seed is not None:\n            data[\"seed\"] = args.seed\n\n        # Stream handling\n        if hasattr(args, \"no_stream\") and args.no_stream:\n            data[\"stream\"] = False\n        elif \"stream\" not in data:\n            data[\"stream\"] = True  # Default to stream for better UX\n\n        # Thinking enable/disable\n        if hasattr(args, \"enable_thinking\"):\n            if args.enable_thinking is not None:\n                data[\"enable_thinking\"] = args.enable_thinking\n                # Some APIs don't support reasoning_content, remove if disabled\n                if not args.enable_thinking:\n                    data.pop(\"reasoning_content\", None)\n\n    @staticmethod\n    def save_context_to_config(config_path: str, messages: List[Dict]) -> None:\n        \"\"\"Save conversation context back to config.json - Âè™‰øùÂ≠òsystem prompt\"\"\"\n        try:\n            path = Path(config_path).expanduser().resolve()\n\n            # Load existing config\n            if path.exists():\n                with open(path, \"r\", encoding=\"utf-8\") as f:\n                    config_data = json.load(f)\n            else:\n                config_data = {}\n\n            # Âè™‰øùÂ≠òsystem promptÔºå‰∏ç‰øùÂ≠òmessages\n            # ‰ªémessages‰∏≠ÊèêÂèñsystem promptÔºàÁ¨¨‰∏ÄÊù°Ê∂àÊÅØÂ¶ÇÊûúÊòØsystemËßíËâ≤Ôºâ\n            system_prompt = None\n            if messages and messages[0].get(\"role\") == \"system\":\n                system_prompt = messages[0].get(\"content\")\n                config_data[\"system\"] = system_prompt\n            \n            # Á°Æ‰øù‰∏ç‰øùÂ≠òmessagesÂà∞config.json\n            if \"messages\" in config_data:\n                del config_data[\"messages\"]\n\n            # Save back\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(config_data, f, ensure_ascii=False, indent=2)\n\n            print(f\"{Colors.info('‚úì')} Saved system prompt to: {path}\", file=sys.stderr)\n\n        except Exception as e:\n            print(f\"{Colors.warn('‚ö†')} Failed to save context: {e}\", file=sys.stderr)\n\n\n# ============ File & Message Building ============\n\n\nclass MessageBuilder:\n    \"\"\"Builds messages with file context support\"\"\"\n\n    IMAGE_TYPES = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\", \".bmp\"}\n\n    @staticmethod\n    def load_file(filepath: str) -> Union[str, Dict[str, Any]]:\n        \"\"\"Load file content with MIME type detection\"\"\"\n        if filepath.startswith(\"@\"):\n            filepath = filepath[1:]\n\n        path = Path(filepath).expanduser().resolve()\n        if not path.exists():\n            raise FileNotFoundError(f\"File not found: {filepath}\")\n\n        mime_type, _ = mimetypes.guess_type(path)\n        if not mime_type:\n            mime_type = \"application/octet-stream\"\n\n        # Image handling\n        if (\n            mime_type.startswith(\"image/\")\n            or path.suffix.lower() in MessageBuilder.IMAGE_TYPES\n        ):\n            with open(path, \"rb\") as f:\n                encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n            return {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:{mime_type};base64,{encoded}\"},\n            }\n\n        # Text file handling with encoding detection\n        encodings = [\"utf-8\", \"utf-8-sig\", \"gbk\", \"latin-1\"]\n        content = None\n\n        for enc in encodings:\n            try:\n                with open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n                    content = f.read()\n                break\n            except UnicodeDecodeError:\n                continue\n\n        if content is None:\n            with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n                content = f.read()\n\n        # Format based on file type\n        suffix = path.suffix[1:] if path.suffix else \"text\"\n        if path.suffix in [\".md\", \".txt\", \".rst\"]:\n            return f\"{content}\\n\\n[File: {path.name}]\"\n        else:\n            return f\"{CODE_BLOCK}{suffix}\\n{content}\\n{CODE_BLOCK}\\n[File: {path.name}]\"\n\n    @staticmethod\n    def build_messages(args, config_data: Dict | None = None) -> List[Dict[str, Any]]:\n        \"\"\"Build final message list with priority: context > config > prompt\"\"\"\n        messages = []\n\n        # Extract system prompt from config if exists\n        system_prompt = None\n        config_messages = []\n        if config_data:\n            system_prompt = config_data.pop(\"system\", None)\n            config_messages = config_data.pop(\"messages\", [])\n\n        # 1. Add system prompt if exists (always first)\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n            print(f\"{Colors.info('‚úì')} Added system prompt\", file=sys.stderr)\n\n        # 2. Load context if provided AND not --new\n        if hasattr(args, \"context\") and args.context and not args.new:\n            try:\n                context_messages = MessageBuilder._load_context(args.context)\n                messages.extend(context_messages)\n                print(\n                    f\"{Colors.info('‚úì')} Loaded conversation context: {args.context}\",\n                    file=sys.stderr,\n                )\n            except Exception as e:\n                print(\n                    f\"{Colors.warn('‚ö†')} Failed to load context: {e}\", file=sys.stderr\n                )\n\n        # 3. Add messages from config (only if not --new)\n        if config_messages and not args.new:\n            messages.extend(config_messages)\n\n        # 4. Add current user input\n        if hasattr(args, \"prompt\") and args.prompt:\n            user_messages = MessageBuilder._build_user_messages(args.prompt)\n            messages.extend(user_messages)\n\n        if not messages and not system_prompt:\n            raise ConfigurationError(\n                \"No messages to send. Please provide prompt or messages in config.\"\n            )\n\n        return messages\n\n    @staticmethod\n    def _load_context(context_path: str) -> List[Dict]:\n        \"\"\"Load conversation history from JSON file and fix null IDs\"\"\"\n        path = Path(context_path).expanduser().resolve()\n        if not path.exists():\n            raise FileNotFoundError(f\"Context file not found: {context_path}\")\n\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n\n        if isinstance(data, list):\n            messages = data\n        elif isinstance(data, dict) and \"messages\" in data:\n            messages = data[\"messages\"]\n        else:\n            raise ValueError(\n                \"Invalid context format: expected list or {messages: [...]}\"\n            )\n\n        # ‰øÆÂ§çÔºöÁ°Æ‰øùÊâÄÊúâtool_call_idÂíåtool_calls‰∏≠ÁöÑid‰∏ç‰∏∫null\n        for msg in messages:\n            # ‰øÆÂ§çtool_calls‰∏≠ÁöÑid\n            if \"tool_calls\" in msg:\n                for tool_call in msg[\"tool_calls\"]:\n                    if not tool_call.get(\"id\"):\n                        tool_call[\"id\"] = str(uuid.uuid4())\n            \n            # ‰øÆÂ§çtoolÊ∂àÊÅØ‰∏≠ÁöÑtool_call_id\n            if msg.get(\"role\") == \"tool\" and not msg.get(\"tool_call_id\"):\n                msg[\"tool_call_id\"] = str(uuid.uuid4())\n\n        return messages\n\n    @staticmethod\n    def _build_user_messages(prompt_parts: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Build user messages from prompt parts\"\"\"\n        if not prompt_parts:\n            return []\n\n        content_parts = []\n        has_image = False\n\n        for part in prompt_parts:\n            if part.startswith(\"@\"):\n                try:\n                    file_content = MessageBuilder.load_file(part)\n                    if isinstance(file_content, dict):\n                        has_image = True\n                        print(\n                            f\"{Colors.info('‚úì')} Attached image: {part[1:]}\",\n                            file=sys.stderr,\n                        )\n                    else:\n                        print(\n                            f\"{Colors.info('‚úì')} Attached file: {part[1:]}\",\n                            file=sys.stderr,\n                        )\n                    content_parts.append(file_content)\n                except Exception as e:\n                    print(\n                        f\"{Colors.warn('‚ö†')} Failed to load file {part}: {e}\",\n                        file=sys.stderr,\n                    )\n                    content_parts.append(f\"[Failed to load file: {part}]\")\n            else:\n                content_parts.append(part)\n\n        # Build message content\n        if has_image:\n            multimodal_content = []\n            current_text = []\n\n            for part in content_parts:\n                if isinstance(part, dict):\n                    if current_text:\n                        multimodal_content.append(\n                            {\"type\": \"text\", \"text\": \"\\n\".join(current_text)}\n                        )\n                        current_text = []\n                    multimodal_content.append(part)\n                else:\n                    current_text.append(part)\n\n            if current_text:\n                multimodal_content.append(\n                    {\"type\": \"text\", \"text\": \"\\n\\n\".join(current_text)}\n                )\n\n            return [{\"role\": \"user\", \"content\": multimodal_content}]\n        else:\n            user_content = \"\\n\\n\".join(content_parts)\n            return [{\"role\": \"user\", \"content\": user_content}]\n\n\n# ============ Typewriter Output ============\n\n\nclass TypewriterPrinter:\n    \"\"\"Typewriter effect printer with file output\"\"\"\n\n    def __init__(self, output_file: Optional[io.TextIOWrapper] = None):\n        self.output_file = output_file\n        self.reasoning_buffer = \"\"\n        self.content_buffer = \"\"\n        self.reasoning_printed = 0\n        self.content_printed = 0\n        self.in_reasoning = True\n        self.start_time = time.time()\n        self.first_token_time = None\n        self.token_count = 0\n\n        # Performance tracking\n        self.reasoning_start = None\n        self.content_start = None\n\n    def write(self, text: str, is_reasoning: bool = False):\n        \"\"\"Write text to file\"\"\"\n        if self.output_file:\n            try:\n                self.output_file.write(text)\n                if not is_reasoning:\n                    self.output_file.flush()\n            except Exception as e:\n                print(f\"\\n{Colors.error('[File write error]')} {e}\", file=sys.stderr)\n\n    def update_reasoning(self, delta: str):\n        \"\"\"Update and print reasoning content\"\"\"\n        if not delta:\n            return\n\n        if self.first_token_time is None:\n            self.first_token_time = time.time()\n\n        if self.reasoning_start is None:\n            self.reasoning_start = time.time()\n            print(f\"\\n{Colors.BLUE}üí≠ Thinking:{Colors.END}\")\n            if self.output_file:\n                self.write(\"\\nüí≠ Thinking:\\n\", is_reasoning=True)\n\n        self.reasoning_buffer += delta\n        new_text = self.reasoning_buffer[self.reasoning_printed :]\n\n        # Print to console with typewriter effect\n        for char in new_text:\n            print(char, end=\"\", flush=True)\n            time.sleep(Constants.STREAM_DELAY_REASONING)\n\n        # Write to file\n        self.write(new_text, is_reasoning=True)\n\n        self.reasoning_printed = len(self.reasoning_buffer)\n        self.token_count += len(delta.split())  # Rough estimate\n\n    def switch_to_content(self):\n        \"\"\"Switch from reasoning to content output\"\"\"\n        if self.in_reasoning:\n            self.in_reasoning = False\n            self.content_start = time.time()\n\n            # Console output\n            print(f\"\\n{Colors.bold('='*50)}\")\n            print(f\"{Colors.CYAN}‚ú® Response:{Colors.END}\")\n            print(f\"{Colors.bold('='*50)}\")\n\n            # File output\n            if self.output_file:\n                self.write(f\"\\n\\n{'='*50}\\n‚ú® Response:\\n{'='*50}\\n\\n\")\n\n    def update_content(self, delta: str):\n        \"\"\"Update and print response content\"\"\"\n        if not delta:\n            return\n\n        if self.in_reasoning:\n            self.switch_to_content()\n\n        if self.first_token_time is None:\n            self.first_token_time = time.time()\n\n        self.content_buffer += delta\n        new_text = self.content_buffer[self.content_printed :]\n\n        # Typewriter effect\n        for char in new_text:\n            print(char, end=\"\", flush=True)\n            time.sleep(Constants.STREAM_DELAY_CONTENT)\n\n        self.write(new_text)\n\n        self.content_printed = len(self.content_buffer)\n        self.token_count += len(delta.split())  # Rough estimate\n\n    def finalize(self):\n        \"\"\"Finalize output\"\"\"\n        if self.output_file:\n            try:\n                self.output_file.flush()\n            except Exception as e:\n                print(f\"{Colors.warn('‚ö†')} Failed to flush file: {e}\", file=sys.stderr)\n\n\n# ============ HTTP Client ============\n\n\nclass APIClient:\n    \"\"\"HTTP client with retry and error handling using Session\"\"\"\n\n    def __init__(self, config: APIConfig, max_retries: int = 3):\n        self.config = config\n        self.max_retries = max_retries\n        self.session = requests.Session()\n        self.session.headers.update(\n            {\n                \"Authorization\": f\"Bearer {config.api_key}\",\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"text/event-stream\",\n            }\n        )\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.session.close()\n\n    def _estimate_tokens(self, messages: List[Dict]) -> Tuple[int, int]:\n        \"\"\"‰º∞ÁÆó Prompt ÁöÑ Token Êï∞Ôºà‰øùÂÆà‰º∞ËÆ°ÔºöÊØè 3 ‰∏™Â≠óÁ¨¶ 1 ‰∏™ TokenÔºâ\"\"\"\n        try:\n            # Â∞Ü messages ËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤ËÆ°ÁÆó\n            text = json.dumps(messages, ensure_ascii=False)\n            char_count = len(text)\n            # ‰øùÂÆà‰º∞ËÆ°ÔºöËã±ÊñáÁ∫¶ 4 Â≠óÁ¨¶/tokenÔºå‰∏≠ÊñáÁ∫¶ 1-2 Â≠óÁ¨¶/tokenÔºåÊ∑∑ÂêàÊåâ 3 ËÆ°ÁÆó\n            estimated_tokens = char_count // 3\n            return char_count, estimated_tokens\n        except:\n            return 0, 0\n\n    def request(\n        self,\n        payload: Dict[str, Any],\n        printer: TypewriterPrinter,\n        tool_manager: Optional[ToolManager] = None,\n    ) -> RequestMetadata:\n        \"\"\"Execute API request with retry logic\"\"\"\n        metadata = RequestMetadata(model=payload.get(\"model\"))\n        is_stream = payload.get(\"stream\", True)\n\n        # ËÆ°ÁÆó Prompt ‰ø°ÊÅØ\n        messages = payload.get(\"messages\", [])\n        char_count, estimated_tokens = self._estimate_tokens(messages)\n        max_tokens = payload.get(\"max_tokens\", \"Not set\")\n\n        # ÊòæÁ§∫ËØ∑Ê±Ç‰ø°ÊÅØÔºåÂåÖÊã¨ Token È¢Ñ‰º∞\n        print(\n            f\"\\n{Colors.CYAN}üöÄ Requesting {metadata.model or 'unknown model'}...{Colors.END}\"\n        )\n        print(f\"   Endpoint: {self.config.endpoint}\")\n        print(f\"   Stream: {'Yes' if is_stream else 'No'}\")\n        print(f\"   Prompt Size: {char_count} chars (Est. ~{estimated_tokens} tokens)\")\n\n        if max_tokens != \"Not set\":\n            print(f\"   Max Tokens: {max_tokens}\")\n        else:\n            print(f\"   Max Tokens: {Colors.warn('Not set')} (model default)\")\n\n        # Safety: Remove any empty or None parameters that might cause API errors\n        clean_payload = self._clean_payload(payload)\n\n        for attempt in range(self.max_retries + 1):\n            try:\n                response = self.session.post(\n                    self.config.endpoint,\n                    json=clean_payload,\n                    stream=is_stream,\n                    timeout=self.config.timeout,\n                )\n                response.raise_for_status()\n\n                if is_stream:\n                    return self._handle_stream(\n                        response, printer, metadata, tool_manager\n                    )\n                else:\n                    return self._handle_non_stream(\n                        response, printer, metadata, tool_manager\n                    )\n\n            except requests.exceptions.RequestException as e:\n                if attempt < self.max_retries:\n                    wait_time = 2**attempt\n                    print(\n                        f\"{Colors.warn(f'‚ö†Ô∏è  Request failed ({e}), retrying in {wait_time}s...')}\",\n                        file=sys.stderr,\n                    )\n                    time.sleep(wait_time)\n                else:\n                    raise APIError(\n                        f\"Request failed after {self.max_retries} retries: {e}\"\n                    )\n\n        raise APIError(\"Max retries exceeded\")\n\n    def _clean_payload(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Remove potentially problematic parameters\"\"\"\n        clean = payload.copy()\n\n        # Remove None values\n        for key in list(clean.keys()):\n            if clean[key] is None:\n                del clean[key]\n\n        # Remove empty strings\n        for key in list(clean.keys()):\n            if isinstance(clean[key], str) and not clean[key].strip():\n                del clean[key]\n\n        return clean\n\n    def _handle_stream(\n        self,\n        response,\n        printer: TypewriterPrinter,\n        metadata: RequestMetadata,\n        tool_manager: Optional[ToolManager],\n    ) -> RequestMetadata:\n        \"\"\"Handle streaming response\"\"\"\n        buffer = \"\"\n        tool_calls_buffer = {}\n\n        for chunk in response.iter_content(chunk_size=Constants.CHUNK_SIZE):\n            if not chunk:\n                continue\n\n            buffer += chunk.decode(\"utf-8\", errors=\"replace\")\n\n            while \"\\n\" in buffer:\n                line, buffer = buffer.split(\"\\n\", 1)\n                line = line.strip()\n\n                if not line.startswith(\"data: \"):\n                    continue\n\n                data_str = line[6:]\n                if data_str == \"[DONE]\":\n                    break\n\n                try:\n                    data = json.loads(data_str)\n                    self._process_stream_data(\n                        data, printer, metadata, tool_calls_buffer\n                    )\n                except json.JSONDecodeError:\n                    continue\n\n        # Handle tool calls after stream ends\n        if tool_calls_buffer and tool_manager:\n            self._handle_tool_calls(tool_calls_buffer, metadata, tool_manager)\n\n        metadata.finalize()\n        return metadata\n\n    def _process_stream_data(\n        self,\n        data: Dict,\n        printer: TypewriterPrinter,\n        metadata: RequestMetadata,\n        tool_calls_buffer: Dict,\n    ):\n        \"\"\"Process individual stream data chunk\"\"\"\n        # Update usage\n        if \"usage\" in data and data[\"usage\"]:\n            metadata.prompt_tokens = data[\"usage\"].get(\"prompt_tokens\", 0)\n            metadata.completion_tokens = data[\"usage\"].get(\"completion_tokens\", 0)\n            metadata.total_tokens = data[\"usage\"].get(\"total_tokens\", 0)\n\n        choices = data.get(\"choices\", [])\n        if not choices:\n            return\n\n        choice = choices[0]\n        delta = choice.get(\"delta\", {})\n\n        # Update finish reason\n        finish_reason = choice.get(\"finish_reason\")\n        if finish_reason:\n            metadata.finish_reason = finish_reason\n\n        # Process tool calls\n        tool_calls = delta.get(\"tool_calls\", [])\n        for tool_call in tool_calls:\n            idx = tool_call.get(\"index\", 0)\n            if idx not in tool_calls_buffer:\n                tool_calls_buffer[idx] = {\n                    \"id\": \"\",\n                    \"type\": \"function\",\n                    \"function\": {\"name\": \"\", \"arguments\": \"\"},\n                }\n\n            if \"id\" in tool_call:\n                tool_calls_buffer[idx][\"id\"] = tool_call[\"id\"]\n            if \"function\" in tool_call:\n                if \"name\" in tool_call[\"function\"]:\n                    tool_calls_buffer[idx][\"function\"][\"name\"] = tool_call[\"function\"][\n                        \"name\"\n                    ]\n                if \"arguments\" in tool_call[\"function\"]:\n                    tool_calls_buffer[idx][\"function\"][\"arguments\"] += tool_call[\n                        \"function\"\n                    ][\"arguments\"]\n\n        # Process content\n        reasoning_delta = delta.get(\"reasoning_content\") or delta.get(\"reasoning\") or \"\"\n        content_delta = delta.get(\"content\", \"\")\n\n        if reasoning_delta:\n            printer.update_reasoning(reasoning_delta)\n\n        if content_delta:\n            printer.update_content(content_delta)\n\n    def _handle_non_stream(\n        self,\n        response,\n        printer: TypewriterPrinter,\n        metadata: RequestMetadata,\n        tool_manager: Optional[ToolManager],\n    ) -> RequestMetadata:\n        \"\"\"Handle non-streaming response\"\"\"\n        data = response.json()\n        choice = data.get(\"choices\", [{}])[0]\n        message = choice.get(\"message\", {})\n\n        reasoning = message.get(\"reasoning_content\", \"\")\n        content = message.get(\"content\", \"\")\n\n        if reasoning:\n            printer.update_reasoning(reasoning)\n\n        printer.switch_to_content()\n        printer.update_content(content)\n\n        # Handle tool calls\n        tool_calls = message.get(\"tool_calls\", [])\n        if tool_calls and tool_manager:\n            self._handle_tool_calls(tool_calls, metadata, tool_manager)\n\n        # Update metadata\n        if \"usage\" in data:\n            metadata.prompt_tokens = data[\"usage\"].get(\"prompt_tokens\", 0)\n            metadata.completion_tokens = data[\"usage\"].get(\"completion_tokens\", 0)\n            metadata.total_tokens = data[\"usage\"].get(\"total_tokens\", 0)\n\n        metadata.finish_reason = choice.get(\"finish_reason\")\n        metadata.finalize()\n\n        return metadata\n\n    def _handle_tool_calls(\n        self,\n        tool_calls: Union[List, Dict],\n        metadata: RequestMetadata,\n        tool_manager: ToolManager,\n    ):\n        \"\"\"Handle tool calls and execute them\"\"\"\n        if isinstance(tool_calls, dict):\n            # Convert dict to list\n            tool_calls_list = []\n            for idx in sorted(tool_calls.keys()):\n                tool_calls_list.append(tool_calls[idx])\n            tool_calls = tool_calls_list\n\n        metadata.tool_calls = tool_calls\n\n        print(f\"\\n{Colors.CYAN}üõ†Ô∏è  Tool Calls Detected:{Colors.END}\")\n        for i, tool_call in enumerate(tool_calls, 1):\n            print(\n                f\"   {i}. {tool_call['function']['name']}({tool_call['function']['arguments']})\"\n            )\n\n        # Execute tool calls\n        for tool_call in tool_calls:\n            result = tool_manager.execute_tool(tool_call)\n            metadata.tool_results.append(result)\n\n\n# ============ Result Saving ============\n\n\nclass ResultSaver:\n    \"\"\"Saves conversation results to files\"\"\"\n\n    @staticmethod\n    def save(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Tuple[Path, Path]:\n        \"\"\"Save results as Markdown and JSON\"\"\"\n        # Create directory if needed\n        base_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # 1. Save Markdown\n        md_path = ResultSaver._save_markdown(base_path, payload, metadata, printer)\n\n        # 2. Save JSON for continuation\n        json_path = ResultSaver._save_json(base_path, payload, metadata, printer)\n\n        return md_path, json_path\n\n    @staticmethod\n    def _save_markdown(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Path:\n        \"\"\"Save conversation as Markdown with full history\"\"\"\n        md_path = base_path.with_suffix(\".md\")\n\n        # Prepare payload for display (hide messages to save space)\n        display_payload = payload.copy()\n        messages = display_payload.pop(\"messages\", [])\n        if messages:\n            display_payload[\"messages\"] = f\"[{len(messages)} messages hidden]\"\n\n        # Build Markdown content\n        md_content = f\"\"\"# AI Conversation Log\n**Time**: {metadata.start_time.strftime('%Y-%m-%d %H:%M:%S')}  \n**Model**: {metadata.model or 'Unknown'}  \n**Duration**: {metadata.duration:.2f}s  \n\n## Request Configuration\n{CODE_BLOCK}json\n{json.dumps(display_payload, indent=2, ensure_ascii=False)}\n{CODE_BLOCK}\n\n## Conversation History\n\n\"\"\"\n\n        # Add all messages\n        for msg in messages:\n            role = msg.get(\"role\", \"unknown\").upper()\n            content = msg.get(\"content\", \"\")\n\n            # Handle multimodal content\n            if isinstance(content, list):\n                texts = []\n                for item in content:\n                    if isinstance(item, dict):\n                        if item.get(\"type\") == \"text\":\n                            texts.append(item.get(\"text\", \"\"))\n                        elif item.get(\"type\") == \"image_url\":\n                            texts.append(\"[Image]\")\n                content = \"\\n\".join(texts)\n\n            md_content += f\"### {role}\\n\\n{content}\\n\\n\"\n\n        # Add reasoning if present\n        if printer.reasoning_buffer:\n            md_content += f\"\"\"\n---\n\n## üí≠ Thinking Process\n<details>\n<summary>Click to expand ({len(printer.reasoning_buffer)} chars)</summary>\n\n{printer.reasoning_buffer}\n</details>\n\n\"\"\"\n\n        # Add response\n        md_content += f\"\"\"\n---\n\n## ‚ú® Response\n{printer.content_buffer}\n\n\"\"\"\n\n        # Add tool calls if present\n        if metadata.tool_calls:\n            md_content += f\"\"\"\n---\n\n## üõ†Ô∏è Tool Calls\n\"\"\"\n            for i, tool_call in enumerate(metadata.tool_calls, 1):\n                md_content += f\"\"\"\n### Tool Call {i}\n- **Function**: `{tool_call['function']['name']}`\n- **Arguments**: `{tool_call['function']['arguments']}`\n\"\"\"\n\n            if metadata.tool_results:\n                md_content += f\"\"\"\n### Tool Results\n\"\"\"\n                for result in metadata.tool_results:\n                    md_content += f\"\"\"\n- **{result['name']}**: {result['content']}\n\"\"\"\n\n        md_content += f\"\"\"\n---\n## üìä Statistics\n\n- **Finish Reason**: {metadata.finish_reason or 'N/A'}\n- **Tokens**: {metadata.total_tokens or 'N/A'} (Prompt: {metadata.prompt_tokens or 'N/A'}, Completion: {metadata.completion_tokens or 'N/A'})\n- **Token Rate**: {f'{metadata.get_token_rate():.1f} tokens/s' if metadata.get_token_rate() else 'N/A'}\n- **First Token Latency**: {f'{(printer.first_token_time - printer.start_time):.2f}s' if printer.first_token_time else 'N/A'}\n- **Estimated Total Tokens**: {printer.token_count}\n\"\"\"\n\n        with open(md_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(md_content)\n\n        return md_path\n\n    @staticmethod\n    def _save_json(\n        base_path: Path,\n        payload: Dict[str, Any],\n        metadata: RequestMetadata,\n        printer: TypewriterPrinter,\n    ) -> Path:\n        \"\"\"Save conversation as JSON for continuation\"\"\"\n        json_path = base_path.with_suffix(\".json\")\n\n        # Build conversation for continuation\n        continuation = {\n            \"metadata\": {\n                \"export_time\": datetime.now().isoformat(),\n                \"model\": metadata.model,\n                \"total_tokens\": metadata.total_tokens,\n                \"duration\": metadata.duration,\n            },\n            \"messages\": payload.get(\"messages\", []).copy(),\n        }\n\n        # Add assistant response\n        assistant_msg = {\n            \"role\": \"assistant\",\n            \"content\": printer.content_buffer,\n        }\n        if printer.reasoning_buffer:\n            assistant_msg[\"reasoning_content\"] = printer.reasoning_buffer\n        if metadata.tool_calls:\n            # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_calls‰∏≠ÁöÑID‰∏ç‰∏∫null\n            for tool_call in metadata.tool_calls:\n                if not tool_call.get(\"id\"):\n                    tool_call[\"id\"] = str(uuid.uuid4())\n            assistant_msg[\"tool_calls\"] = metadata.tool_calls\n\n        continuation[\"messages\"].append(assistant_msg)\n\n        # Add tool results as tool messages\n        for result in metadata.tool_results:\n            # ‰øÆÂ§çÔºöÁ°Æ‰øùtool_call_id‰∏ç‰∏∫null\n            if not result.get(\"tool_call_id\"):\n                result[\"tool_call_id\"] = str(uuid.uuid4())\n            continuation[\"messages\"].append(result)\n\n        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(continuation, f, ensure_ascii=False, indent=2)\n\n        return json_path\n\n\n# ============ Argument Parser ============\n\n\ndef create_argument_parser() -> argparse.ArgumentParser:\n    \"\"\"Create enhanced argument parser\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"AI Chat CLI - Enhanced with graceful parameter handling\",\n        formatter_class=argparse.RawTextHelpFormatter,\n        epilog=\"\"\"\nConfiguration Strategy:\n  1. Unified settings via config.json to avoid model-specific parameter issues\n  2. CLI arguments safely override config.json (removes conflicting params)\n  3. profiles.json or .env provides endpoint and api_key\n\nImage Support:\n  Use @filename.png to attach images (png, jpg, jpeg, gif, webp, bmp supported)\n  Example: python chat.py @photo.jpg \"Describe this image\"\n\nToken Estimation:\n  Before sending, the CLI will display estimated prompt tokens and max_tokens\n  to help you avoid context overflow.\n\nTool Support:\n  Define tools in tools.py with proper function signatures and docstrings\n  The CLI will automatically load and make them available to the model\n\nExamples:\n  # Basic usage with default config\n  python chat.py \"Hello, how are you?\"\n  \n  # With file attachment (code or image)\n  python chat.py @code.py \"Explain this code\"\n  python chat.py @image.png \"What's in this image?\"\n  \n  # Custom profile and config\n  python chat.py -p deepseek -f deepseek_config.json \"Explain quantum computing\"\n  \n  # Continue conversation\n  python chat.py --context conversation.json \"Continue from here\"\n  \n  # Start new conversation (don't save context to config)\n  python chat.py --new \"Start fresh conversation\"\n  \n  # Disable streaming\n  python chat.py --no-stream \"Generate a long response\"\n  \n  # Override parameters\n  python chat.py --temperature 0.5 --max-tokens 2000 \"Creative writing\"\n        \"\"\",\n    )\n\n    # Connection\n    parser.add_argument(\n        \"--profile\", \"-p\", default=\"default\", help=\"Profile name in profiles.json\"\n    )\n    parser.add_argument(\"--endpoint\", \"-e\", help=\"Override API endpoint\")\n    parser.add_argument(\"--api-key\", \"-k\", help=\"Override API key\")\n\n    # Configuration\n    parser.add_argument(\n        \"--config\", \"-c\", default=\"config.json\", help=\"Request body configuration file\"\n    )\n    parser.add_argument(\n        \"--new\",\n        action=\"store_true\",\n        help=\"Start new conversation (ignore context and config messages)\",\n    )\n\n    # Input\n    parser.add_argument(\n        \"prompt\", nargs=\"*\", help=\"Prompt text (supports @filename for files/images)\"\n    )\n    parser.add_argument(\n        \"--context\", help=\"Conversation context JSON file for continuation\"\n    )\n\n    # OpenAI Parameters (override config.json)\n    parser.add_argument(\"--model\", \"-m\", help=\"Model name (overrides config)\")\n    parser.add_argument(\n        \"--temperature\", \"-t\", type=float, help=\"Sampling temperature (0-2)\"\n    )\n    parser.add_argument(\"--max-tokens\", type=int, help=\"Maximum tokens to generate\")\n    parser.add_argument(\"--top-p\", type=float, help=\"Nucleus sampling parameter\")\n    parser.add_argument(\"--presence-penalty\", type=float, help=\"Presence penalty\")\n    parser.add_argument(\"--frequency-penalty\", type=float, help=\"Frequency penalty\")\n    parser.add_argument(\"--seed\", type=int, help=\"Random seed for reproducibility\")\n\n    # Behavior (safely override config)\n    parser.add_argument(\n        \"--no-stream\", action=\"store_true\", help=\"Disable streaming output\"\n    )\n    parser.add_argument(\n        \"--enable-thinking\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable thinking process\",\n    )\n    parser.add_argument(\n        \"--no-thinking\",\n        dest=\"enable_thinking\",\n        action=\"store_false\",\n        help=\"Disable thinking process\",\n    )\n\n    # Tools\n    parser.add_argument(\n        \"--tools\", default=\"tools.py\", help=\"Python module containing tool functions\"\n    )\n    parser.add_argument(\n        \"--no-tools\", action=\"store_true\", help=\"Disable tool functions\"\n    )\n\n    # Output\n    parser.add_argument(\"--output\", \"-o\", help=\"Output file base name\")\n\n    return parser\n\n\n# ============ Main Function ============\n\n\ndef handle_interrupt(signum, frame):\n    \"\"\"Handle Ctrl+C gracefully\"\"\"\n    print(f\"\\n{Colors.warn('‚ö†Ô∏è  Interrupted by user')}\")\n    sys.exit(130)\n\n\ndef main():\n    \"\"\"Main entry point\"\"\"\n    # Setup\n    setup_utf8()\n    signal.signal(signal.SIGINT, handle_interrupt)\n\n    # Parse arguments\n    parser = create_argument_parser()\n    args = parser.parse_args()\n\n    try:\n        # 1. Load connection configuration\n        api_config = ConfigManager.load_connection_config(args)\n        api_config.validate()\n\n        # 2. Load request body from config.json\n        request_body = ConfigManager.load_request_body(args.config, args)\n\n        # 3. Initialize tool manager\n        tool_manager = None\n        if not args.no_tools and Path(args.tools).exists():\n            tool_manager = ToolManager(args.tools)\n            tool_definitions = tool_manager.get_tool_definitions()\n            if tool_definitions:\n                request_body[\"tools\"] = tool_definitions\n                print(\n                    f\"{Colors.info('‚úì')} Added {len(tool_definitions)} tools to request\",\n                    file=sys.stderr,\n                )\n\n        # 4. Build messages\n        messages = MessageBuilder.build_messages(args, request_body)\n        request_body[\"messages\"] = messages\n\n        # 5. Determine output path\n        if args.output:\n            output_base = Path(args.output)\n        else:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            prompt_hint = (\n                args.prompt[0][:20]\n                if args.prompt\n                else request_body.get(\"model\", \"chat\")\n            )\n            safe_name = re.sub(r\"[^\\w\\s-]\", \"\", prompt_hint).strip().replace(\" \", \"_\")\n            output_base = Path(f\"chat_{safe_name}_{timestamp}\")\n\n        # 6. Execute request\n        with open(output_base.with_suffix(\".md\"), \"w\", encoding=\"utf-8\") as out_file:\n            printer = TypewriterPrinter(out_file)\n\n            with APIClient(api_config) as client:\n                metadata = client.request(request_body, printer, tool_manager)\n\n            printer.finalize()\n\n            # 7. Add separator before complete message\n            print(f\"\\n{Colors.bold('‚îÄ' * 60)}\")\n\n            # 8. Print statistics\n            print(f\"\\n{Colors.info('‚úÖ Complete')}\")\n            print(f\"   Duration: {metadata.duration:.2f}s\")\n\n            if metadata.total_tokens:\n                print(\n                    f\"   Tokens: {metadata.total_tokens} (Prompt: {metadata.prompt_tokens}, Completion: {metadata.completion_tokens})\"\n                )\n                if token_rate := metadata.get_token_rate():\n                    print(f\"   Token Rate: {token_rate:.1f} tokens/s\")\n\n            if metadata.finish_reason:\n                print(f\"   Finish Reason: {metadata.finish_reason}\")\n\n            if printer.first_token_time:\n                print(\n                    f\"   First Token: {(printer.first_token_time - printer.start_time):.2f}s\"\n                )\n\n            if metadata.tool_calls:\n                print(f\"   Tool Calls: {len(metadata.tool_calls)} executed\")\n\n            # 9. Save results\n            md_path, json_path = ResultSaver.save(\n                output_base, request_body, metadata, printer\n            )\n\n            print(f\"\\n{Colors.info('üíæ Saved:')}\")\n            print(f\"   {md_path}\")\n            print(f\"   {json_path}\")\n            print(\n                f\"\\n{Colors.info('üí° Tip:')} Continue with: --context {json_path.name}\"\n            )\n\n            # 10. Save context to config.json if not --new\n            if not args.new:\n                # Update messages with assistant response\n                updated_messages = messages.copy()\n\n                # Add assistant message\n                assistant_msg = {\n                    \"role\": \"assistant\",\n                    \"content\": printer.content_buffer,\n                }\n                if printer.reasoning_buffer:\n                    assistant_msg[\"reasoning_content\"] = printer.reasoning_buffer\n                if metadata.tool_calls:\n                    assistant_msg[\"tool_calls\"] = metadata.tool_calls\n\n                updated_messages.append(assistant_msg)\n\n                # Add tool results\n                for result in metadata.tool_results:\n                    updated_messages.append(result)\n\n                # Save to config.json (Âè™‰øùÂ≠òsystem prompt)\n                ConfigManager.save_context_to_config(args.config, updated_messages)\n\n    except KeyboardInterrupt:\n        print(f\"\\n{Colors.warn('‚ö†Ô∏è  Interrupted')}\", file=sys.stderr)\n        sys.exit(130)\n    except (ConfigurationError, FileNotFoundError) as e:\n        print(f\"{Colors.error('‚ùå Configuration Error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n    except APIError as e:\n        print(f\"{Colors.error('‚ùå API Error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n    except Exception as e:\n        logger.exception(\"Unexpected error\")\n        print(f\"{Colors.error('‚ùå Unexpected error:')} {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## ‰∏ªË¶Å‰øÆÂ§çÁÇπÔºö\n\n### 1. **‰øÆÂ§çtool_call_id‰∏∫nullÁöÑÈóÆÈ¢ò**\n- Âú®`ToolManager.execute_tool()`‰∏≠Ëá™Âä®ÁîüÊàêUUID\n- Âú®`MessageBuilder._load_context()`‰∏≠‰øÆÂ§çÂ∑≤Â≠òÂú®ÁöÑJSONÊñá‰ª∂\n- Âú®`ResultSaver._save_json()`‰∏≠Á°Æ‰øù‰øùÂ≠òÊó∂ID‰∏ç‰∏∫null\n\n### 2. **‰øÆÂ§ç--newÂèÇÊï∞ÈÄªËæë**\n- ‰øÆÊîπ`MessageBuilder.build_messages()`ÔºåÂΩì`args.new`‰∏∫TrueÊó∂Ôºö\n  - ÂøΩÁï•`--context`ÂèÇÊï∞\n  - ÂøΩÁï•config.json‰∏≠ÁöÑmessages\n  - Âè™‰ΩøÁî®system promptÂíåÂΩìÂâçprompt\n\n### 3. **‰øÆÂ§çconfig.json‰øùÂ≠òÈÄªËæë**\n- ‰øÆÊîπ`ConfigManager.save_context_to_config()`ÔºåÂè™‰øùÂ≠òsystem promptÂà∞config.json\n- ‰∏çÂÜç‰øùÂ≠òÂÆåÊï¥ÁöÑÂØπËØùÂéÜÂè≤Âà∞config.json\n- ÂØπËØùÂéÜÂè≤‰øùÂ≠òÂú®ÊØèÊ¨°ÁîüÊàêÁöÑJSONÊñá‰ª∂‰∏≠\n\n## ‰ΩøÁî®ÊñπÂºèÔºö\n\n1. **ÁªßÁª≠ÂØπËØù**Ôºö\n   ```bash\n   python deepseek.py --context chat_ÂçóÊûÅ_20260206_035215.json \"ÁªßÁª≠\"\n   ```\n\n2. **ÂºÄÂßãÊñ∞ÂØπËØù**Ôºö\n   ```bash\n   python deepseek.py --new \"Êñ∞ÂØπËØùÂºÄÂßã\"\n   ```\n\n3. **‰ΩøÁî®Â∑•ÂÖ∑**Ôºö\n   ```bash\n   python deepseek.py \"Êü•ËØ¢Âåó‰∫¨Â§©Ê∞î\"\n   ```\n\nÁé∞Âú®‰øÆÂ§çÂêéÁöÑ‰ª£Á†ÅÂ∫îËØ•ËÉΩÊ≠£Á°ÆÂ§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®IDÔºåÂπ∂‰∏î`--new`ÂèÇÊï∞‰ºöÂøΩÁï•‰πãÂâçÁöÑÂØπËØùÂéÜÂè≤ÔºåÂè™‰ΩøÁî®config.json‰∏≠ÁöÑsystem promptÔºàÂ¶ÇÊûúÊúâÁöÑËØùÔºâ„ÄÇ",
      "reasoning_content": "Êàë‰ª¨Ê≥®ÊÑèÂà∞Áî®Êà∑Â∑≤ÁªèÂ∫îÁî®‰∫Ü‰πãÂâçÂª∫ËÆÆÁöÑ‰øÆÂ§çÔºå‰ΩÜÁî®Êà∑ÊèêÂà∞‰∏§‰∏™ÈóÆÈ¢òÔºö\n 1. Áé∞ÊúâÁöÑJSONÊñá‰ª∂‰∏≠tool_call_id‰∏∫nullÔºåÂØºËá¥Âä†ËΩΩÊó∂Âá∫Áé∞400ÈîôËØØ„ÄÇ\n 2. --newÂèÇÊï∞Êó†ÊïàÔºå‰∏îÁî®Êà∑Â∏åÊúõconfig.jsonÂè™‰øùÂ≠òsystem_promptÔºåËÄåÂØπËØùÂéÜÂè≤‰øùÂ≠òÂú®ÊØèÊ¨°ÁîüÊàêÁöÑJSONÊñá‰ª∂‰∏≠Ôºå--newÂ∫îËØ•ÂøΩÁï•contextÂíåconfig‰∏≠ÁöÑmessages„ÄÇ\n\n Ëß£ÂÜ≥ÊñπÊ°àÔºö\n 1. ‰øÆÊîπMessageBuilder._load_contextÊñπÊ≥ïÔºåÂú®Âä†ËΩΩ‰∏ä‰∏ãÊñáÊó∂Ëá™Âä®‰øÆÂ§çnullÁöÑtool_call_idÂíåtool_calls‰∏≠ÁöÑid„ÄÇ\n 2. ‰øÆÊîπMessageBuilder.build_messagesÊñπÊ≥ïÔºå‰ΩøÂÖ∂Âú®--newÂèÇÊï∞Â≠òÂú®Êó∂ÂøΩÁï•contextÂíåconfig‰∏≠ÁöÑmessagesÔºåÂè™‰ΩøÁî®system_promptÂíåÂΩìÂâçprompt„ÄÇ\n\n Âè¶Â§ñÔºåÁî®Êà∑ËøòÊèêÂà∞config.json‰∏≠‰∏çÂ∫îËØ•‰øùÂ≠òmessagesÔºåÂè™‰øùÂ≠òsystem_prompt„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÈúÄË¶Å‰øÆÊîπConfigManager.save_context_to_configÔºå‰ΩøÂÖ∂Âè™‰øùÂ≠òsystem_promptÔºåËÄå‰∏ç‰øùÂ≠òmessages„ÄÇ\n\n ‰ΩÜÊòØÔºåÊ†πÊçÆÁî®Êà∑ÁöÑÈúÄÊ±ÇÔºåconfig.jsonÂ∫îËØ•Âè™ÂåÖÂê´system_promptÔºåËÄåÂØπËØùÂéÜÂè≤Áî±ÊØèÊ¨°ÁîüÊàêÁöÑJSONÊñá‰ª∂‰øùÂ≠ò„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÈúÄË¶ÅË∞ÉÊï¥‰øùÂ≠òÈÄªËæë„ÄÇ\n\n ÁÑ∂ËÄåÔºåÁî®Êà∑Êèê‰æõÁöÑ‰ª£Á†Å‰∏≠Ôºåconfig.json‰ºº‰πéËøò‰øùÂ≠ò‰∫Ümessages„ÄÇÊàë‰ª¨ÈúÄË¶ÅÊîπÂèòËøô‰∏ÄË°å‰∏∫„ÄÇ\n\n ËÆ©Êàë‰ª¨ÈáçÊñ∞Ê¢≥ÁêÜÁî®Êà∑ÁöÑÈúÄÊ±ÇÔºö\n - config.jsonÔºöÂè™‰øùÂ≠òsystem_promptÂíåÊ®°ÂûãÂèÇÊï∞Ôºå‰∏ç‰øùÂ≠òÂØπËØùÂéÜÂè≤„ÄÇ\n - ÂØπËØùÂéÜÂè≤Ôºö‰øùÂ≠òÂú®ÊØèÊ¨°ÂØπËØùÁîüÊàêÁöÑJSONÊñá‰ª∂‰∏≠ÔºåÈÄöËøá--contextÂèÇÊï∞Âä†ËΩΩ„ÄÇ\n\n Âõ†Ê≠§ÔºåÊàë‰ª¨ÈúÄË¶ÅÂÅö‰ª•‰∏ã‰øÆÊîπÔºö\n 1. ‰øÆÊîπConfigManager.save_context_to_configÔºå‰ΩøÂÖ∂Âè™‰øùÂ≠òsystem_promptÔºàÂ¶ÇÊûúÊúâÁöÑËØùÔºâÂíåÂÖ∂‰ªñÈÖçÁΩÆÔºå‰∏ç‰øùÂ≠òmessages„ÄÇ\n 2. ‰øÆÊîπMessageBuilder.build_messagesÔºå‰ΩøÂÖ∂Âú®ÊûÑÂª∫Ê∂àÊÅØÊó∂ÔºåÂ¶ÇÊûúargs.new‰∏∫TrueÔºåÂàôÂøΩÁï•contextÂíåconfig‰∏≠ÁöÑmessagesÔºåÂè™‰ΩøÁî®system_promptÂíåÂΩìÂâçprompt„ÄÇ\n\n ‰ΩÜÊòØÔºåÊ≥®ÊÑèÁî®Êà∑Êèê‰æõÁöÑ‰ª£Á†Å‰∏≠Ôºåconfig.jsonÂèØËÉΩÂ∑≤Áªè‰øùÂ≠ò‰∫Ümessages„ÄÇ‰∏∫‰∫ÜÂÖºÂÆπÔºåÊàë‰ª¨ÂèØ‰ª•Âú®Âä†ËΩΩconfig.jsonÊó∂ÔºåÂè™ÊèêÂèñsystem_promptÔºåËÄåÂøΩÁï•messages„ÄÇ\n\n ÁÑ∂ËÄåÔºåÁî®Êà∑Êèê‰æõÁöÑ‰ª£Á†Å‰∏≠ÔºåConfigManager.load_request_body‰ºöÂä†ËΩΩÊï¥‰∏™config.jsonÔºåÂåÖÊã¨messages„ÄÇÊàë‰ª¨ÈúÄË¶Å‰øÆÊîπËøô‰∏™Ë°å‰∏∫Ôºå‰ΩøÂÖ∂Âú®Âä†ËΩΩÊó∂‰∏çÂä†ËΩΩmessagesÔºåÈô§ÈùûÁî®Êà∑Ê≤°ÊúâÊåáÂÆö--new‰∏îÊ≤°ÊúâÊåáÂÆö--context„ÄÇ\n\n ‰ΩÜÊ†πÊçÆÁî®Êà∑ÁöÑÈúÄÊ±ÇÔºåconfig.json‰∏çÂ∫îËØ•Áî®‰∫é‰øùÂ≠òÂØπËØùÂéÜÂè≤ÔºåÊâÄ‰ª•Êàë‰ª¨Â∫îËØ•‰ªéconfig.json‰∏≠ÁßªÈô§messagesÔºåÂè™‰øùÁïôsystem_promptÂíåÂÖ∂‰ªñÂèÇÊï∞„ÄÇ\n\n ‰ΩÜÊòØÔºå‰∏∫‰∫ÜÂêëÂêéÂÖºÂÆπÔºåÊàë‰ª¨ÂèØ‰ª•ËøôÊ†∑Â§ÑÁêÜÔºö\n Âú®MessageBuilder.build_messages‰∏≠ÔºåÂ¶ÇÊûúargs.new‰∏∫TrueÔºåÂàôÂøΩÁï•config.json‰∏≠ÁöÑmessagesÂíåcontext„ÄÇ\n Â¶ÇÊûúargs.new‰∏∫FalseÔºåÂàôÊåâÁÖßÂéüÊù•ÁöÑÈÄªËæëÔºöÂÖàÂä†ËΩΩcontextÔºàÂ¶ÇÊûúÊúâÔºâÔºåÁÑ∂ÂêéÂä†ËΩΩconfig.json‰∏≠ÁöÑmessagesÔºàÂ¶ÇÊûúÊúâÔºâÔºåÊúÄÂêéÂä†‰∏äÂΩìÂâçprompt„ÄÇ\n\n ‰ΩÜÊòØÔºåÁî®Êà∑Â∏åÊúõconfig.jsonÂè™‰øùÂ≠òsystem_promptÔºåÊâÄ‰ª•Êàë‰ª¨Â∫îËØ•‰øÆÊîπ‰øùÂ≠òÈÄªËæëÔºå‰∏çÂÜç‰øùÂ≠òmessagesÂà∞config.json„ÄÇ\n\n ÁÑ∂ËÄåÔºåÁî®Êà∑Êèê‰æõÁöÑ‰ª£Á†Å‰∏≠ÔºåConfigManager.save_context_to_configË¢´Áî®Êù•‰øùÂ≠òmessagesÂà∞config.json„ÄÇÊàë‰ª¨ÈúÄË¶Å‰øÆÊîπËøô‰∏™ÂáΩÊï∞Ôºå‰ΩøÂÖ∂‰∏ç‰øùÂ≠òmessagesÔºåÊàñËÄÖÂè™‰øùÂ≠òsystem_prompt„ÄÇ\n\n ‰ΩÜÊòØÔºåÁî®Êà∑ÂèØËÉΩÂ∑≤Áªè‰π†ÊÉØ‰∫ÜÂéüÊù•ÁöÑË°å‰∏∫ÔºåÊâÄ‰ª•Êàë‰ª¨ÂèØËÉΩÈúÄË¶Å‰∏Ä‰∏™ËøáÊ∏°„ÄÇËÄÉËôëÂà∞Áî®Êà∑ÊòéÁ°ÆË¶ÅÊ±ÇÔºåÊàë‰ª¨ÂÜ≥ÂÆö‰øÆÊîπÔºö\n\n 1. ConfigManager.save_context_to_configÔºö‰∏çÂÜç‰øùÂ≠òmessagesÔºåÂè™‰øùÂ≠òsystem_promptÔºàÂ¶ÇÊûúÊúâÁöÑËØùÔºâÂíåÂÖ∂‰ªñÈÖçÁΩÆ„ÄÇ\n 2. MessageBuilder.build_messagesÔºö‰ªéconfig.json‰∏≠Âè™ÊèêÂèñsystem_promptÔºå‰∏çÂÜçÊèêÂèñmessages„ÄÇ\n\n ‰ΩÜÊòØÔºåËøôÊ†∑‰øÆÊîπÂêéÔºåÂéüÊù•‰øùÂ≠òÂú®config.json‰∏≠ÁöÑmessagesÂ∞Ü‰∏ç‰ºöË¢´Âä†ËΩΩ„ÄÇÁî®Êà∑Â¶ÇÊûúÈúÄË¶ÅÁªßÁª≠ÂØπËØùÔºåÂ∫îËØ•‰ΩøÁî®--contextÂèÇÊï∞ÊåáÂÆö‰∏äÊ¨°ÁîüÊàêÁöÑJSONÊñá‰ª∂„ÄÇ\n\n ËøôÊ†∑ÔºåÊàë‰ª¨Â∞±ÂÆûÁé∞‰∫ÜÁî®Êà∑ÁöÑÈúÄÊ±ÇÔºöconfig.jsonÂè™‰øùÂ≠òsystem_promptÔºåÂØπËØùÂéÜÂè≤Áî±JSONÊñá‰ª∂‰øùÂ≠ò„ÄÇ\n\n ÂÖ∑‰Ωì‰øÆÊîπÊ≠•È™§Ôºö\n\n ‰∏Ä„ÄÅ‰øÆÊîπConfigManager.save_context_to_configÔºö\n    ‰∏çÂÜç‰øùÂ≠òmessagesÔºåÂè™‰øùÂ≠òsystem_promptÔºàÂ¶ÇÊûúÊúâÁöÑËØùÔºâÂíåÂÖ∂‰ªñÈÖçÁΩÆ„ÄÇ‰ΩÜÊòØÔºåÊ≥®ÊÑèconfig.json‰∏≠ÂèØËÉΩÂ∑≤ÁªèÊúâÂÖ∂‰ªñÈÖçÁΩÆÔºåÊàë‰ª¨Âè™Êõ¥Êñ∞system_promptÔºåÂπ∂‰øùÁïôÂÖ∂‰ªñÈÖçÁΩÆ„ÄÇ\n\n ‰ΩÜÊòØÔºåÂéüÂáΩÊï∞ÊòØÂ∞Ümessages‰øùÂ≠òÂà∞config.jsonÁöÑ\"messages\"Â≠óÊÆµ„ÄÇÁé∞Âú®Êàë‰ª¨Ë¶ÅÊîπ‰∏∫‰øùÂ≠òsystem_promptÂà∞\"system\"Â≠óÊÆµ„ÄÇ\n\n ‰ΩÜÊòØÔºåÂéü‰ª£Á†Å‰∏≠Ôºåconfig.jsonÂèØËÉΩÂ∑≤ÁªèÊúâ‰∏Ä‰∏™\"system\"Â≠óÊÆµ„ÄÇÊàë‰ª¨Âè™ÈúÄË¶ÅÊõ¥Êñ∞Ëøô‰∏™Â≠óÊÆµÔºåËÄå‰∏çÈúÄË¶ÅÂä®ÂÖ∂‰ªñÂ≠óÊÆµ„ÄÇ\n\n ÁÑ∂ËÄåÔºåÁî®Êà∑ÂèØËÉΩÂ∑≤ÁªèÂ∞Üsystem_promptÊîæÂú®config.jsonÁöÑ\"system\"Â≠óÊÆµ‰∏≠„ÄÇÊâÄ‰ª•ÔºåÊàë‰ª¨Âè™ÈúÄË¶ÅÂ∞Üsystem_prompt‰øùÂ≠òÂà∞\"system\"Â≠óÊÆµÔºåËÄå‰∏ç‰øùÂ≠òmessages„ÄÇ\n\n ‰ΩÜÊòØÔºåÂéüÂáΩÊï∞Êé•Êî∂ÁöÑÊòØmessagesÂàóË°®ÔºåÊàë‰ª¨Â¶Ç‰Ωï‰ªé‰∏≠ÊèêÂèñsystem_promptÔºüÂÆûÈôÖ‰∏äÔºåÂú®mainÂáΩÊï∞‰∏≠ÔºåÊàë‰ª¨‰øùÂ≠òÂà∞config.jsonÁöÑmessagesÊòØÂÆåÊï¥ÁöÑÂØπËØùÂéÜÂè≤ÔºåÂåÖÊã¨system_prompt„ÄÇÊâÄ‰ª•ÔºåÊàë‰ª¨ÈúÄË¶Å‰ªémessages‰∏≠ÊèêÂèñÂá∫system_promptÔºàÂç≥Á¨¨‰∏ÄÊù°Ê∂àÊÅØÔºåÂ¶ÇÊûúÂÆÉÊòØsystemËßíËâ≤ÁöÑËØùÔºâ„ÄÇ\n\n ‰ΩÜÊòØÔºåËøôÊ†∑ÊèêÂèñÂèØËÉΩ‰∏çÂáÜÁ°ÆÔºåÂõ†‰∏∫Áî®Êà∑ÂèØËÉΩÊ≤°ÊúâÂú®config.json‰∏≠ËÆæÁΩÆsystem_promptÔºåËÄåÊòØÈÄöËøáÂÖ∂‰ªñÊñπÂºè„ÄÇ\n\n ËÄÉËôëÂà∞Áî®Êà∑ÁöÑÈúÄÊ±ÇÔºåÊàë‰ª¨ÂèØËÉΩ‰∏çÈúÄË¶ÅËá™Âä®‰øùÂ≠òsystem_promptÂà∞config.json„ÄÇÁî®Êà∑ÂèØ‰ª•Âú®config.json‰∏≠ÊâãÂä®ËÆæÁΩÆsystem_prompt„ÄÇ\n\n Âõ†Ê≠§ÔºåÊàë‰ª¨‰øÆÊîπConfigManager.save_context_to_configÔºå‰ΩøÂÖ∂‰∏çÂÜç‰øùÂ≠ò‰ªª‰ΩïÊ∂àÊÅØÔºåÂè™‰øùÁïôÂéüÊù•ÁöÑÈÖçÁΩÆÔºàÂç≥‰∏çÊõ¥Êñ∞messagesÂ≠óÊÆµÔºâ„ÄÇÊàñËÄÖÔºåÊàë‰ª¨ÂèØ‰ª•Âà†Èô§config.json‰∏≠ÁöÑmessagesÂ≠óÊÆµ„ÄÇ\n\n ‰ΩÜÊòØÔºå‰∏∫‰∫ÜÊª°Ë∂≥Áî®Êà∑ÁöÑÈúÄÊ±ÇÔºåÊàë‰ª¨ÂÜ≥ÂÆöÔºöÂú®‰øùÂ≠ò‰∏ä‰∏ãÊñáÂà∞config.jsonÊó∂ÔºåÂè™‰øùÂ≠òsystem_promptÔºàÂ¶ÇÊûúÊúâÁöÑËØùÔºâÂà∞\"system\"Â≠óÊÆµÔºåÂπ∂Âà†Èô§\"messages\"Â≠óÊÆµ„ÄÇ\n\n ÁÑ∂ËÄåÔºåÁî®Êà∑ÂèØËÉΩÂ∏åÊúõ‰øùÁïôconfig.json‰∏≠ÁöÑÂÖ∂‰ªñÈÖçÁΩÆÔºåÊØîÂ¶ÇÊ®°ÂûãÂèÇÊï∞Á≠â„ÄÇÊâÄ‰ª•ÔºåÊàë‰ª¨Âè™Âà†Èô§\"messages\"Â≠óÊÆµÔºåÂπ∂Êõ¥Êñ∞\"system\"Â≠óÊÆµ„ÄÇ\n\n ‰ΩÜÊòØÔºåÊ≥®ÊÑèÔºöÂú®mainÂáΩÊï∞‰∏≠ÔºåÊàë‰ª¨Ë∞ÉÁî®ConfigManager.save_context_to_configÊó∂Ôºå‰º†ÈÄíÁöÑÊòØupdated_messagesÔºåÂç≥Êï¥‰∏™ÂØπËØùÂéÜÂè≤„ÄÇÊàë‰ª¨ÈúÄË¶Å‰ªé‰∏≠ÊèêÂèñsystem_promptÔºàÁ¨¨‰∏ÄÊù°Ê∂àÊÅØÔºå‰∏îËßíËâ≤‰∏∫systemÔºâ„ÄÇ\n\n ‰øÆÊîπConfigManager.save_context_to_configÂ¶Ç‰∏ãÔºö\n\n ‰∫å„ÄÅ‰øÆÊîπMessageBuilder.build_messagesÔºö\n    ‰ªéconfig.json‰∏≠Âè™ÊèêÂèñsystem_promptÔºå‰∏çÂÜçÊèêÂèñmessages„ÄÇÂêåÊó∂ÔºåÂ¶ÇÊûúargs.new‰∏∫TrueÔºåÂàôÂøΩÁï•contextÂíåconfig.json‰∏≠ÁöÑmessagesÔºàÂÆûÈôÖ‰∏äconfig.json‰∏≠Â∑≤ÁªèÊ≤°Êúâmessages‰∫ÜÔºå‰ΩÜ‰∏∫‰∫ÜÂÆâÂÖ®ÔºåÊàë‰ª¨‰ªçÁÑ∂ÂøΩÁï•Ôºâ„ÄÇ\n\n ‰ΩÜÊòØÔºåconfig.json‰∏≠ÂèØËÉΩËøòÊúâmessagesÂ≠óÊÆµÔºàÊóßÁâàÊú¨‰øùÂ≠òÁöÑÔºâÔºåÊàë‰ª¨Âú®Âä†ËΩΩÊó∂Â∫îËØ•ÂøΩÁï•„ÄÇÊâÄ‰ª•ÔºåÂú®ConfigManager.load_request_body‰∏≠ÔºåÊàë‰ª¨‰∏çÂ∫îËØ•ËøîÂõûmessagesÂ≠óÊÆµÔºåÊàñËÄÖMessageBuilder.build_messages‰∏≠‰∏çËØªÂèñconfig.jsonÁöÑmessages„ÄÇ\n\n ÁÑ∂ËÄåÔºåÂéü‰ª£Á†Å‰∏≠ÔºåConfigManager.load_request_bodyËøîÂõûÊï¥‰∏™config.jsonÔºåÁÑ∂ÂêéÂú®MessageBuilder.build_messages‰∏≠ÔºåÊàë‰ª¨‰ΩøÁî®config_data.pop(\"messages\", [])Êù•Ëé∑ÂèñÊ∂àÊÅØ„ÄÇÁé∞Âú®ÔºåÊàë‰ª¨‰∏çÂÜç‰ªéconfig.json‰∏≠Ëé∑ÂèñÊ∂àÊÅØÔºåÊâÄ‰ª•ÂèØ‰ª•‰øÆÊîπMessageBuilder.build_messagesÔºå‰∏çÂÜç‰ªéconfig_data‰∏≠popÂá∫messages„ÄÇ\n\n ‰ΩÜÊòØÔºå‰∏∫‰∫Ü‰øùÊåÅ‰ª£Á†ÅÊ∏ÖÊô∞ÔºåÊàë‰ª¨‰øÆÊîπMessageBuilder.build_messagesÔºå‰ΩøÂÖ∂Âú®‰ªéconfig.jsonÂä†ËΩΩÊó∂ÔºåÂè™ËØªÂèñsystem_promptÔºåËÄå‰∏çËØªÂèñmessages„ÄÇ\n\n Áî±‰∫éÁî®Êà∑Ë¶ÅÊ±ÇÔºåÊàë‰ª¨ÂÜ≥ÂÆöÔºö\n\n 1. Âú®ConfigManager.load_request_body‰∏≠ÔºåÊàë‰ª¨‰∏çÂÜçÂà†Èô§config.json‰∏≠ÁöÑmessagesÂ≠óÊÆµÔºåÂõ†‰∏∫MessageBuilder.build_messages‰ºöÂ§ÑÁêÜ„ÄÇ\n 2. Âú®MessageBuilder.build_messages‰∏≠ÔºåÊàë‰ª¨Âè™‰ªéconfig.json‰∏≠ÊèêÂèñsystem_promptÔºåËÄåÂøΩÁï•messages„ÄÇ\n\n ‰ΩÜÊòØÔºåÁî®Êà∑ÂèØËÉΩÂ∑≤Áªè‰π†ÊÉØ‰∫ÜÂéüÊù•ÁöÑË°å‰∏∫ÔºåÊâÄ‰ª•Êàë‰ª¨ÈúÄË¶ÅÊòéÁ°ÆÔºöconfig.json‰∏≠ÁöÑmessagesÂ≠óÊÆµÂ∞ÜË¢´ÂøΩÁï•„ÄÇ\n\n ‰øÆÊîπMessageBuilder.build_messagesÔºö\n   - ‰ªéconfig_data‰∏≠ÊèêÂèñsystem_promptÔºå‰ΩÜ‰∏çÊèêÂèñmessages„ÄÇ\n   - Â¶ÇÊûúargs.new‰∏∫TrueÔºåÂàôÂøΩÁï•contextÂíåconfig.json‰∏≠ÁöÑmessagesÔºàÂÆûÈôÖ‰∏äconfig.json‰∏≠Â∑≤ÁªèÊ≤°Êúâmessages‰∫ÜÔºå‰ΩÜ‰∏∫‰∫ÜÂÆâÂÖ®ÔºåÊàë‰ª¨‰ªçÁÑ∂ÂøΩÁï•Ôºâ„ÄÇ\n\n ‰ΩÜÊòØÔºåconfig.json‰∏≠ÂèØËÉΩËøòÊúâmessagesÂ≠óÊÆµÔºåÊàë‰ª¨ÂøΩÁï•ÂÆÉÂç≥ÂèØ„ÄÇ\n\n ‰∏â„ÄÅ‰øÆÂ§çÁé∞ÊúâJSONÊñá‰ª∂‰∏≠ÁöÑnull tool_call_idÔºö\n    ‰øÆÊîπMessageBuilder._load_contextÔºåÂú®Âä†ËΩΩÊó∂Ëá™Âä®‰øÆÂ§çnullÁöÑtool_call_idÂíåtool_calls‰∏≠ÁöÑid„ÄÇ\n\n Áî±‰∫éÁî®Êà∑Â∑≤ÁªèÊúâ‰∏Ä‰∏™JSONÊñá‰ª∂Ôºàchat_ÂçóÊûÅ_20260206_035215.jsonÔºâ‰∏≠tool_call_id‰∏∫nullÔºåÊàë‰ª¨ÈúÄË¶ÅÂú®Âä†ËΩΩÊó∂‰øÆÂ§ç„ÄÇ\n\n Êàë‰ª¨ÂèØ‰ª•Âú®MessageBuilder._load_context‰∏≠ÔºåÈÅçÂéÜÊ∂àÊÅØÔºåÂØπ‰∫éÊØèÊù°Ê∂àÊÅØÔºö\n   - Â¶ÇÊûúÊ∂àÊÅØÊúâ\"tool_calls\"Êï∞ÁªÑÔºåÈÅçÂéÜÊØè‰∏™tool_callÔºåÂ¶ÇÊûúid‰∏∫nullÔºåÂàôÁîüÊàê‰∏Ä‰∏™uuid„ÄÇ\n   - Â¶ÇÊûúÊ∂àÊÅØÁöÑroleÊòØ\"tool\"Ôºå‰∏îtool_call_id‰∏∫nullÔºåÂàôÁîüÊàê‰∏Ä‰∏™uuid„ÄÇ\n\n ËøôÊ†∑ÔºåÂä†ËΩΩ‰∏ä‰∏ãÊñáÊó∂Â∞±‰ºöËá™Âä®‰øÆÂ§çÔºåÈÅøÂÖçAPIËøîÂõû400ÈîôËØØ„ÄÇ\n\n ËÄÉËôëÂà∞Áî®Êà∑ÂèØËÉΩÂ∏åÊúõ‰øÆÂ§çÁé∞ÊúâÁöÑJSONÊñá‰ª∂ÔºåÊàë‰ª¨‰πüÂèØ‰ª•Êèê‰æõ‰∏Ä‰∏™ÂçïÁã¨ÁöÑ‰øÆÂ§çËÑöÊú¨Ôºå‰ΩÜ‰∏∫‰∫ÜÁÆÄ‰æøÔºåÊàë‰ª¨Áõ¥Êé•‰øÆÊîπÂä†ËΩΩÈÄªËæë„ÄÇ\n\n ‰∏ãÈù¢ÔºåÊàë‰ª¨ÂºÄÂßã‰øÆÊîπ‰ª£Á†ÅÔºö\n\n Áî±‰∫éÁî®Êà∑Ë¶ÅÊ±ÇÊèê‰æõÂÆåÊï¥‰ª£Á†ÅÔºåÊàëÂ∞ÜÊèê‰æõ‰øÆÊîπÂêéÁöÑÂÆåÊï¥deepseek.pyÊñá‰ª∂„ÄÇ‰ΩÜÊòØÔºåÁî±‰∫é‰ª£Á†ÅÂæàÈïøÔºåÊàëÂ∞ÜÂè™‰øÆÊîπÂÖ≥ÈîÆÈÉ®ÂàÜÔºåÂπ∂ÁªôÂá∫ÂÆåÊï¥‰ª£Á†Å„ÄÇ\n\n Ê≥®ÊÑèÔºöÁî±‰∫é‰ª£Á†ÅËøáÈïøÔºåÊàëÂ∞ÜÂè™‰øÆÊîπ‰ª•‰∏ãÈÉ®ÂàÜÔºö\n   1. MessageBuilder._load_context\n   2. MessageBuilder.build_messages\n   3. ConfigManager.save_context_to_config\n\n ÂêåÊó∂ÔºåÊàë‰ª¨ËøòÈúÄË¶Å‰øÆÊîπmainÂáΩÊï∞‰∏≠Ë∞ÉÁî®ConfigManager.save_context_to_configÁöÑÈÉ®ÂàÜÔºåÂõ†‰∏∫Áé∞Âú®save_context_to_config‰∏çÂÜç‰øùÂ≠òmessagesÔºåËÄåÊòØ‰øùÂ≠òsystem_prompt„ÄÇ\n\n ‰ΩÜÊòØÔºåmainÂáΩÊï∞‰∏≠ÔºåÊàë‰ª¨‰øùÂ≠òÂà∞config.jsonÁöÑÊòØupdated_messagesÔºåÁé∞Âú®Êàë‰ª¨ÈúÄË¶Å‰ªé‰∏≠ÊèêÂèñsystem_promptÔºàÂ¶ÇÊûúÊúâÁöÑËØùÔºâÁÑ∂Âêé‰øùÂ≠ò„ÄÇ\n\n ÁÑ∂ËÄåÔºåÊ≥®ÊÑèÔºöÂú®mainÂáΩÊï∞‰∏≠ÔºåÊàë‰ª¨‰øùÂ≠òÂà∞config.jsonÁöÑupdated_messagesÊòØÂÆåÊï¥ÁöÑÂØπËØùÂéÜÂè≤ÔºåÂåÖÊã¨system_promptÂíåÊâÄÊúâÊ∂àÊÅØ„ÄÇÁé∞Âú®ÔºåÊàë‰ª¨Âè™‰øùÂ≠òsystem_promptÂà∞config.json„ÄÇ\n\n ÊâÄ‰ª•ÔºåÊàë‰ª¨ÈúÄË¶Å‰øÆÊîπmainÂáΩÊï∞‰∏≠Ë∞ÉÁî®ConfigManager.save_context_to_configÁöÑÈÉ®ÂàÜÔºåÂè™‰º†ÈÄísystem_promptÔºàÂ¶ÇÊûúÊúâÁöÑËØùÔºâÔºåËÄå‰∏çÊòØÊï¥‰∏™updated_messages„ÄÇ\n\n ‰ΩÜÊòØÔºåConfigManager.save_context_to_configÂéüÊú¨Êé•Êî∂‰∏Ä‰∏™messagesÂàóË°®ÔºåÁé∞Âú®Êàë‰ª¨ÈúÄË¶ÅÊîπÂèòÂÆÉÁöÑÂèÇÊï∞ÔºåÊàñËÄÖÊîπÂèòÂÆÉÁöÑË°å‰∏∫„ÄÇ\n\n ‰∏∫‰∫ÜÊúÄÂ∞èÂåñ‰øÆÊîπÔºåÊàë‰ª¨‰øÆÊîπConfigManager.save_context_to_configÔºåËÆ©ÂÆÉ‰ªémessages‰∏≠ÊèêÂèñsystem_promptÔºàÁ¨¨‰∏ÄÊù°Ê∂àÊÅØÔºåÂ¶ÇÊûúËßíËâ≤ÊòØsystemÔºâÔºåÁÑ∂Âêé‰øùÂ≠òÂà∞config.jsonÁöÑ\"system\"Â≠óÊÆµÔºåÂπ∂Âà†Èô§\"messages\"Â≠óÊÆµ„ÄÇ\n\n ÂêåÊó∂ÔºåÊàë‰ª¨‰∏çÂÜç‰øùÂ≠òÊï¥‰∏™messagesÂà∞config.json„ÄÇ\n\n ‰øÆÊîπConfigManager.save_context_to_configÂ¶Ç‰∏ãÔºö\n\n Âè¶Â§ñÔºåÊàë‰ª¨ËøòÈúÄË¶Å‰øÆÊîπMessageBuilder.build_messages‰∏≠‰ªéconfig.jsonÂä†ËΩΩsystem_promptÁöÑÊñπÂºèÔºåÂõ†‰∏∫Áé∞Âú®config.json‰∏≠ÂèØËÉΩÊ≤°Êúâ\"system\"Â≠óÊÆµÔºåËÄåÊòØÊúâ\"messages\"Â≠óÊÆµÔºàÊóßÁâàÊú¨Ôºâ„ÄÇ‰∏∫‰∫ÜÂÖºÂÆπÔºåÊàë‰ª¨ÂêåÊó∂ÊîØÊåÅ‰ªé\"system\"Â≠óÊÆµÂíå‰ªé\"messages\"Â≠óÊÆµ‰∏≠ÊèêÂèñsystem_promptÔºàÁ¨¨‰∏ÄÊù°Ê∂àÊÅØÔºåÂ¶ÇÊûúËßíËâ≤ÊòØsystemÔºâ„ÄÇ\n\n ‰ΩÜÊòØÔºå‰∏∫‰∫ÜÁÆÄÂåñÔºåÊàë‰ª¨ÂÅáËÆæÁî®Êà∑Â∑≤ÁªèÊåâÁÖßÊñ∞ÁâàÊú¨‰ΩøÁî®ÔºåÂç≥config.json‰∏≠Âè™Êúâ\"system\"Â≠óÊÆµÔºåÊ≤°Êúâ\"messages\"Â≠óÊÆµ„ÄÇ\n\n ËÄÉËôëÂà∞Áî®Êà∑ÂèØËÉΩÂ∑≤Áªè‰ΩøÁî®ÊóßÁâàÊú¨ÔºåÊàë‰ª¨ÂèØ‰ª•ÂÅöÂÖºÂÆπÂ§ÑÁêÜÔºöÂú®MessageBuilder.build_messages‰∏≠ÔºåÂ¶ÇÊûúconfig.jsonÊúâ\"system\"Â≠óÊÆµÔºåÂàô‰ΩøÁî®ÂÆÉÔºõÂê¶ÂàôÔºåÂ¶ÇÊûúconfig.jsonÊúâ\"messages\"Â≠óÊÆµÔºåÂàô‰ªé‰∏≠ÊèêÂèñsystem_promptÔºàÁ¨¨‰∏ÄÊù°Ê∂àÊÅØÔºåÂ¶ÇÊûúËßíËâ≤ÊòØsystemÔºâ„ÄÇ\n\n ‰ΩÜÊòØÔºåËøôÊ†∑ÂèØËÉΩ‰ºöÂ§çÊùÇ„ÄÇÊàë‰ª¨ÊåâÁÖßÁî®Êà∑ÁöÑÈúÄÊ±ÇÔºåÂè™‰ªéconfig.jsonÁöÑ\"system\"Â≠óÊÆµËé∑Âèñsystem_promptÔºåÂøΩÁï•\"messages\"Â≠óÊÆµ„ÄÇ\n\n Âõ†Ê≠§Ôºå‰øÆÊîπMessageBuilder.build_messagesÔºå‰ªéconfig_data‰∏≠Ëé∑Âèñ\"system\"Â≠óÊÆµ‰Ωú‰∏∫system_promptÔºåËÄå‰∏çÂÜçËé∑Âèñ\"messages\"Â≠óÊÆµ„ÄÇ\n\n ÂêåÊó∂ÔºåÂ¶ÇÊûúconfig_data‰∏≠Êúâ\"messages\"Â≠óÊÆµÔºåÊàë‰ª¨ÂøΩÁï•ÂÆÉ„ÄÇ\n\n Áé∞Âú®ÔºåÊàë‰ª¨ÂºÄÂßã‰øÆÊîπ‰ª£Á†Å„ÄÇ\n\n Áî±‰∫é‰øÆÊîπËæÉÂ§öÔºåÊàëÂ∞ÜÊèê‰æõ‰øÆÊîπÂêéÁöÑÂÆåÊï¥deepseek.py‰ª£Á†Å„ÄÇ\n\n Ê≥®ÊÑèÔºöÁî±‰∫é‰ª£Á†ÅËøáÈïøÔºåÊàëÂ∞ÜÂè™‰øÆÊîπÂÖ≥ÈîÆÈÉ®ÂàÜÔºåÂπ∂Âú®‰ª£Á†Å‰∏≠Ê≥®Èáä‰øÆÊîπÁöÑÂú∞Êñπ„ÄÇ\n\n Áî±‰∫éÁî®Êà∑Ë¶ÅÊ±ÇÂÆåÊï¥‰ª£Á†ÅÔºåÊàëÂ∞ÜÊèê‰æõÊï¥‰∏™Êñá‰ª∂ÁöÑ‰ª£Á†ÅÔºå‰ΩÜ‰∏∫‰∫ÜËäÇÁúÅÁ©∫Èó¥ÔºåÊàëÂè™‰øÆÊîπÂÖ≥ÈîÆÈÉ®ÂàÜÔºåÂπ∂ÁªôÂá∫‰øÆÊîπÂêéÁöÑÂÆåÊï¥‰ª£Á†Å„ÄÇ\n\n Áî±‰∫éÊó∂Èó¥ÂÖ≥Á≥ªÔºåÊàëÂ∞ÜÂè™‰øÆÊîπ‰∏äËø∞ÊèêÂà∞ÁöÑÈÉ®ÂàÜÔºåÂπ∂Êèê‰æõÂÆåÊï¥‰ª£Á†Å„ÄÇ\n\n ‰øÆÊîπÂêéÁöÑdeepseek.pyÂ¶Ç‰∏ãÔºö"
    }
  ],
  "context": "chat_deepseekpy_20260206_034856.json"
}